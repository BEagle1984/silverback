{
  "about.html": {
    "href": "about.html",
    "title": "About | Silverback",
    "keywords": "About Author Silverback is an open-source project maintained by Sergio Aquilini (BEagle1984 on GitHub). GitHub LinkedIn Personal E-Mail Project E-Mail License The code is licensed under MIT license (see LICENSE file for details). Credits Silverback uses the following libraries under the hood: Rx.Net Json.NET Confluent's .NET Client for Apache Kafka MQTTNet RabbitMQ .NET Client Special Thanks A very big thank you to all the contributors and especially to my friends and colleagues: Fabio for the help with Kafka Laurent for constantly challenging, pushing and bringing new ideas and feedbacks Marc for his contributions and the valuable constant feedbacks and ideas"
  },
  "concepts/background-services.html": {
    "href": "concepts/background-services.html",
    "title": "Distributed Background Services | Silverback",
    "keywords": "Distributed Background Services To implement the <xref:Silverback.Messaging.Outbound.TransactionalOutbox.OutboxWorkerService> we had to create a database based locking mechanism, to ensure that only a single instance of our worker was running. You can take advantage of this implementation to build your IHostedService . DistributedBackgroundService Two base classes are available in Silverback.Core : <xref:Silverback.Background.DistributedBackgroundService> implements the basic locking mechanism, while <xref:Silverback.Background.RecurringDistributedBackgroundService> adds on top of it the ability to run a task as specified intervals. using Silverback.Background; namespace Sample { public class MyBackroundService : RecurringDistributedBackgroundService { private readonly IMyService _myService; public MyBackroundService( IMyService _myService, IDistributedLockManager distributedLockManager, ILogger<OutboundQueueWorkerService> logger) : base( TimeSpan.FromMinutes(5), // interval distributedLockManager, logger) { } protected override Task ExecuteRecurringAsync( CancellationToken stoppingToken) => _myService.DoWork(stoppingToken); } } Note A <xref:Silverback.Background.DistributedLockSettings> object can be passed to the constructor of the base class to customize lock timeout, heartbeat interval, etc. Lock Manager To enable the distributed locks an <xref:Silverback.Background.IDistributedLockManager> implementation (probably a <xref:Silverback.Background.DbDistributedLockManager>) must be registered for dependency injection as shown in the next code snippet. Note The Silverback.Core.EntityFrameworkCore package is also required and the DbContext must configure a DbSet<Lock> . See also the Sample DbContext (EF Core) . public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .UseDbContext<MyDbContext>() .AddDbDistributedLockManager(); } }"
  },
  "concepts/broker/behaviors.html": {
    "href": "concepts/broker/behaviors.html",
    "title": "Broker behaviors pipeline | Silverback",
    "keywords": "Broker behaviors pipeline Silverback is built to be modular and most of its feature are plugged into the consumers and producers via some so-called behaviors. The inbound and outbound messages flow through this pipeline and each behavior take care of a specific task such as serialization, encryption, chunking, logging, etc. The <xref:Silverback.Messaging.Broker.Behaviors.IProducerBehavior> and <xref:Silverback.Messaging.Broker.Behaviors.IConsumerBehavior> are the interfaces used to build such behaviors. Note <xref:Silverback.Messaging.Broker.Behaviors.IProducerBehavior> and <xref:Silverback.Messaging.Broker.Behaviors.IConsumerBehavior> inherit the <xref:Silverback.ISorted> interface. It is therefore mandatory to specify the exact sort index of each behavior. Built-in producer behaviors This behaviors build the producer pipeline and contain the actual logic to properly serialize the messages according to the applied configuration. Name Index Description <xref:Silverback.Messaging.Diagnostics.ActivityProducerBehavior> 100 Starts an Activity and adds the tracing information to the message headers. <xref:Silverback.Messaging.Headers.HeadersWriterProducerBehavior> 200 Maps the properties decorated with the <xref:Silverback.Messaging.Messages.HeaderAttribute> to the message headers. <xref:Silverback.Messaging.Outbound.Enrichers.MessageEnricherProducerBehavior> 250 Invokes all the <xref:Silverback.Messaging.Outbound.Enrichers.IOutboundMessageEnricher> configured for to the endpoint. <xref:Silverback.Messaging.Broker.Behaviors.MessageIdInitializerProducerBehavior> 300 It ensures that an x-message-id header is always produced. BrokerKeyHeaderInitializer 400 Provided by the message broker implementation (e.g. <xref:Silverback.Messaging.Outbound.KafkaMessageKeyInitializerProducerBehavior> or <xref:Silverback.Messaging.Outbound.RabbitRoutingKeyInitializerProducerBehavior>), sets the message key header that will be used by the <xref:Silverback.Messaging.Broker.IProducer> implementation to set the actual message key. <xref:Silverback.Messaging.BinaryFiles.BinaryFileHandlerProducerBehavior> 500 Switches to the <xref:Silverback.Messaging.BinaryFiles.BinaryFileMessageSerializer> if the message being produced implements the <xref:Silverback.Messaging.Messages.IBinaryFileMessage> interface. <xref:Silverback.Messaging.Serialization.SerializerProducerBehavior> 600 Serializes the message being produced using the configured <xref:Silverback.Messaging.Serialization.IMessageSerializer>. <xref:Silverback.Messaging.Encryption.EncryptorProducerBehavior> 700 Encrypts the message according to the <xref:Silverback.Messaging.Encryption.EncryptionSettings>. <xref:Silverback.Messaging.Sequences.SequencerProducerBehavior> 800 Uses the available implementations of <xref:Silverback.Messaging.Sequences.ISequenceWriter> (e.g. <xref:Silverback.Messaging.Sequences.Chunking.ChunkSequenceWriter>) to set the proper headers and split the published message or messages set to create the sequences. <xref:Silverback.Messaging.Outbound.Routing.EndpointNameResolverProducerBehavior> 900 Resolves the actual target endpoint name for the message being published. <xref:Silverback.Messaging.Outbound.Routing.KafkaPartitionResolverProducerBehavior> 901 Resolves the actual target endpoint name for the message being published. <xref:Silverback.Messaging.Headers.CustomHeadersMapperProducerBehavior> 1000 Applies the custom header name mappings. Built-in consumer behaviors This behaviors are the foundation of the consumer pipeline and contain the actual logic to deserialize the incoming messages. Name Index Description <xref:Silverback.Messaging.Diagnostics.ActivityConsumerBehavior> 100 Starts an Activity with the tracing information from the message headers. <xref:Silverback.Messaging.Diagnostics.FatalExceptionLoggerConsumerBehavior> 200 Logs the unhandled exceptions thrown while processing the message. These exceptions are fatal since they will usually cause the consumer to stop. <xref:Silverback.Messaging.Headers.CustomHeadersMapperConsumerBehavior> 300 Applies the custom header name mappings. <xref:Silverback.Messaging.Inbound.Transaction.TransactionHandlerConsumerBehavior> 400 Handles the consumer transaction and applies the error policies. <xref:Silverback.Messaging.Sequences.RawSequencerConsumerBehavior> 500 Uses the available implementations of <xref:Silverback.Messaging.Sequences.ISequenceReader> (e.g. <xref:Silverback.Messaging.Sequences.Chunking.ChunkSequenceReader>) to assign the incoming message to the right sequence. <xref:Silverback.Messaging.Inbound.ExactlyOnce.ExactlyOnceGuardConsumerBehavior> 600 Uses the configured implementation of <xref:Silverback.Messaging.Inbound.ExactlyOnce.IExactlyOnceStrategy> to ensure that the message is processed only once. <xref:Silverback.Messaging.Encryption.DecryptorConsumerBehavior> 700 Decrypts the message according to the <xref:Silverback.Messaging.Encryption.EncryptionSettings>. <xref:Silverback.Messaging.BinaryFiles.BinaryFileHandlerProducerBehavior> 800 Switches to the <xref:Silverback.Messaging.BinaryFiles.BinaryFileMessageSerializer> if the message being consumed is a binary message (according to the x-message-type header. <xref:Silverback.Messaging.Serialization.DeserializerConsumerBehavior> 900 Deserializes the messages being consumed using the configured <xref:Silverback.Messaging.Serialization.IMessageSerializer>. <xref:Silverback.Messaging.Headers.HeadersReaderConsumerBehavior> 1000 Maps the headers with the properties decorated with the <xref:Silverback.Messaging.Messages.HeaderAttribute>. <xref:Silverback.Messaging.Sequences.SequencerConsumerBehavior> 1100 Uses the available implementations of <xref:Silverback.Messaging.Sequences.ISequenceReader> (e.g. <xref:Silverback.Messaging.Sequences.Batch.BatchSequenceReader>) to assign the incoming message to the right sequence. <xref:Silverback.Messaging.Inbound.PublisherConsumerBehavior> 2000 Publishes the consumed messages to the internal bus. Custom behaviors The behaviors can be used to implement cross-cutting concerns or add new features to Silverback. Custom IProducerBehavior example The following example demonstrate how to set a custom message header on each outbound message. Note The <xref:Silverback.Messaging.Broker.Behaviors.ProducerPipelineContext> and <xref:Silverback.Messaging.Broker.Behaviors.ConsumerPipelineContext> hold a reference to the IServiceProvider and can be used to resolve the needed services. The IServiceProvider in the <xref:Silverback.Messaging.Broker.Behaviors.ConsumerPipelineContext> can be either the root service provider or the scoped service provider for the processing of the consumed message (depending on the position of the behavior in the pipeline). Note The broker behaviors can be registered either as singleton or transient services. When registered as transient a new instance will be created per each producer or consumer. ProducerBehavior Startup public class CustomHeadersProducerBehavior : IProducerBehavior { public int SortIndex => 1000; public async Task HandleAsync( ProducerPipelineContext context, ProducerBehaviorHandler next) { context.Envelope.Headers.Add(\"generated-by\", \"silverback\"); await next(context); } } public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka()) .AddSingletonBrokerBehavior<CustomHeadersBehavior>(); } } Custom IConsumerBehavior example The following example demonstrate how to log the headers received with each inbound message. ConsumerBehavior Startup public class LogHeadersConsumerBehavior : IConsumerBehavior { private readonly ILogger<LogHeadersBehavior> _logger; public LogHeadersBehavior(ILogger<LogHeadersBehavior> logger) { _logger = logger; } public int SortIndex => 1000; public async Task HandleAsync( ConsumerPipelineContext context, ConsumerBehaviorHandler next) { foreach (var header in context.Envelope.Headers) { _logger.LogTrace( \"{Name}={Value}\", header.Name, header.Value); } await next(context); } } public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka()) .AddSingletonBrokerBehavior<LogHeadersBehavior>(); } } See also Behaviors"
  },
  "concepts/broker/binary-files.html": {
    "href": "concepts/broker/binary-files.html",
    "title": "Binary Files | Silverback",
    "keywords": "Binary Files Serializing a binary file (a stream or a byte array) using the regular <xref:Silverback.Messaging.Serialization.JsonMessageSerializer> would mean to encode it in base64 and convert it to a UTF-8 encoded byte array. Beside not being very elegant this approach may cause you some trouble when integrating with other systems expecting the raw file content. This procedure would also result in the transferred byte array to be approximately a 30% bigger than the file itself. In this page it's shown how to use an <xref:Silverback.Messaging.Messages.IBinaryFileMessage> to more efficiently transfer raw binary files. Producer configuration The <xref:Silverback.Messaging.Messages.IBinaryFileMessage> interface is meant to transfer files over the message broker and is natively supported by Silverback. This means that the raw file content will be transferred in its original form. For convenience the <xref:Silverback.Messaging.Messages.BinaryFileMessage> class already implements the <xref:Silverback.Messaging.Messages.IBinaryFileMessage> interface. This class exposes a ContentType property as well, resulting in the content-type header to be produced. EndpointsConfigurator (fluent) EndpointsConfigurator (legacy) Publisher public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<IBinaryFileMessage>(endpoint => endpoint .ProduceTo(\"raw-files\"))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<IBinaryFileMessage>( new KafkaProducerEndpoint(\"inventory-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" } }); } public class FileTransferService { private readonly IPublisher _publisher; public FileTransferService(IPublisher publisher) { _publisher = publisher; } public async Task TransferFile(byte[] content, string contentType) { await _publihser.PublishAsync( new BinaryFileMessage(content, contentType)); } } Otherwise you can implement the interface yourself or extend the <xref:Silverback.Messaging.Messages.BinaryFileMessage> (e.g. to add some additional headers, as explained in the Message Headers section). EndpointsConfigurator (fluent) EndpointsConfigurator (legacy) Message Publisher public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<IBinaryFileMessage>(endpoint => endpoint .ProduceTo(\"raw-files\"))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<IBinaryFileMessage>( new KafkaProducerEndpoint(\"raw-files\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" } }); } public class MyBinaryFileMessage : BinaryFileMessage { [Header(\"x-user-id\")] public Guid UserId { get; set; } } public class FileTransferService { private readonly IPublisher _publisher; public FileTransferService(IPublisher publisher) { _publisher = publisher; } public async Task TransferFile( byte[] content, string contentType, Guid userId) { await _publihser.PublishAsync( new MyBinaryFileMessage { Content = content, ContentType = contentType, UserId = userId }); } } Consumer configuration You don't need to do anything special to consume a binary file, if all necessary headers are in place (ensured by Silverback, if it was used to produce the message). The message will be wrapped again in a <xref:Silverback.Messaging.Messages.BinaryFileMessage> that can be subscribed like any other message. EndpointsConfigurator (fluent) EndpointsConfigurator (legacy) Subscriber public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"raw-files\") .Configure(config => { config.GroupId = \"my-consumer\" })); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new KafkaConsumerEndpoint(\"raw-files\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" } }); } public class FileSubscriberService { public async Task OnFileReceived(IBinaryFileMessage message) { // ...your file handling logic... } } If the message wasn't produced by Silverback chances are that the message type header is not there. In that case you need to explicitly configure the <xref:Silverback.Messaging.BinaryFiles.BinaryFileMessageSerializer> in the inbound endpoint. EndpointsConfigurator (fluent) EndpointsConfigurator (legacy) Subscriber public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"raw-files\") .ConsumeBinaryFiles() .Configure(config => { config.GroupId = \"my-consumer\" })); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new KafkaConsumerEndpoint(\"raw-files\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" } , Serializer = BinaryFileMessageSerializer.Default }); } public class FileSubscriberService { public async Task OnFileReceived(IBinaryFileMessage message) { // ...your file handling logic... } } If you need to read additional headers you can either extend the <xref:Silverback.Messaging.Messages.BinaryFileMessage> (suggested approach) or subscribe to an <xref:Silverback.Messaging.Messages.IInboundEnvelope`1> . The following snippet assumes that the files aren't being streamed by a Silverback producer, otherwise it wouldn't be necessary to explicitly set the serializer and the type would be inferred from the x-message-type header. EndpointsConfigurator (fluent) EndpointsConfigurator (legacy) Message Subscriber public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"raw-files\") .ConsumeBinaryFiles(serializer => serializer.UseModel<MyBinaryFileMessage>())); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<IBinaryFileMessage>( new KafkaProducerEndpoint(\"raw-files\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, Serializer = new BinaryFileMessageSerializer<MyBinaryFileMessage>() }); } public class MyBinaryFileMessage : BinaryFileMessage { [Header(\"x-user-id\")] public Guid UserId { get; set; } } public class FileSubscriberService { public async Task OnFileReceived(MyBinaryFileMessage message) { // ...your file handling logic... } } Samples Kafka - Files Streaming"
  },
  "concepts/broker/callbacks.html": {
    "href": "concepts/broker/callbacks.html",
    "title": "Broker Callbacks | Silverback",
    "keywords": "Broker Callbacks The callbacks are used to notify some events happening during the lifecycle of a message broker client. An interface has to be implemented by the callback handler that is then registered via the Add*BrokerCallbacksHandler methods. The only generic callback, invoked for any of the actual broker implementation is: <xref:Silverback.Messaging.Broker.Callbacks.IEndpointsConfiguredCallback> Some broker specific callbacks may be added by the specific broker implementation (see Kafka Events and MQTT Events ). Example In the following example an handler for the <xref:Silverback.Messaging.Broker.Callbacks.IEndpointsConfiguredCallback> is being registered. Startup EndpointsConfiguredCallbackHandler public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka()) .AddSingletonBrokerCallbacksHandler<EndpointsConfiguredCallbackHandler>(); } } public class EndpointsConfiguredCallbackHandler : IKafkaPartitionsAssignedCallback { public Task OnEndpointsConfiguredAsync() { // Perform some initialization logic, // e.g. create the missing topics } }"
  },
  "concepts/broker/chunking.html": {
    "href": "concepts/broker/chunking.html",
    "title": "Chunking | Silverback",
    "keywords": "Chunking Some message brokers like Apache Kafka are very efficient at handling huge amount of relatively small messages. In order to make the most out of it you may want to split your largest messages (e.g. containing binary data) into smaller chunks. Silverback can handle such scenario transparently, reassembling the message automatically in the consumer before pushing it to the internal bus. The messages are being split into small chunks. Producer configuration The producer endpoint can be configured to split the message into chunks by specifying their maximum size (in bytes). Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<IIntegrationEvent>(endpoint => endpoint .ProduceTo(\"order-events\") .EnableChunking(500000) .ProduceToOutbox()); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<IIntegrationEvent>( new KafkaProducerEndpoint(\"order-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, Chunk = new ChunkSettings { Size = 500000 }, Strategy = new OutboxProduceStrategy() }); } Important The chunks belonging to the same message must be contiguous. It is therefore recommended to have a single producer per endpoint or partition. If using Kafka see also Kafka Partitioning and Message Key . Consumer configuration No particular configuration is needed in the consumer side. Silverback will automatically recognize the chunks sequence by its headers and transparently reassemble the message. With Silverback 3.0.0 the consumer pipeline has been rewritten completely to handle this scenario in a streaming way, processing each chunk directly and applying the behaviors (such as the deserializer) on the fly. The entire original message is never stored anywhere, therefore this approach is suitable also for very large payloads. Important The chunks belonging to the same message must be contiguous. It is therefore recommended to have a single producer per endpoint or partition. If using Kafka see also Kafka Partitioning and Message Key . Incomplete sequences Some chunks sequences may be incomplete because either the producer failed to publish all chunks or the consumer started consuming from the middle of a sequence. In both cases Silverback will silently ignore the incomplete sequences and log a warning. Limitations As mentioned already, the chunks have to be written to the same partition and have to be contiguous. This is by design. Another limitation is that the <xref:Silverback.Messaging.Inbound.ErrorHandling.MoveMessageErrorPolicy> is currently unable to move a sequence and is therefore unusable with chunked messages. This may be fixed in a future release. Please open an issue on GitHub if this is important for your use case. Headers Some headers are used to describe the chunks sequence. See Message Headers for details. Samples Kafka - Files Streaming"
  },
  "concepts/broker/connecting.html": {
    "href": "concepts/broker/connecting.html",
    "title": "Connecting to a Message Broker | Silverback",
    "keywords": "Connecting to a Message Broker To connect Silverback to a message broker we need a reference to Silverback.Integration , plus the concrete implementation ( Silverback.Integration.Kafka , Silverback.Integration.MQTT , Silverback.Integration.RabbitMQ , etc.). We can then add the broker to the DI and configure the connected endpoints. Sample configuration The following example is very basic and there are of course many more configurations and possibilities. Some more details are given in the dedicated Outbound Endpoint and Inbound Endpoint sections. The basic concepts: WithConnectionToMessageBroker registers the services necessary to connect to a message broker AddKafka , AddMqtt , AddRabbit , etc. register the message broker implementation(s) AddEndpointsConfigurator is used to outsource the endpoints configuration into a separate class implementing the <xref:Silverback.Messaging.Configuration.IEndpointsConfigurator> interface (of course multiple configurators can be registered) AddInbound is used to automatically relay the incoming messages to the internal bus and they can therefore be subscribed as seen in the previous chapters AddOutbound works the other way around and subscribes to the internal bus to forward the integration messages to the message broker More complex and complete samples can be found in the Samples section. Basic configuration The following sample demonstrates how to setup some inbound and outbound endpoints against the built-in message brokers (Apache Kafka, MQTT or RabbitMQ). Apache Kafka Startup EndpointsConfigurator public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka()) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"basket-events\") .Configure(config => { config.GroupId = \"order-service\"; })) .AddInbound(endpoint => endpoint .ConsumeFrom(\"payment-events\") .Configure(config => { confing.GroupId = \"order-service\" })) .AddOutbound<IIntegrationEvent>(endpoint => endpoint .ProduceTo(\"order-events\"))); } MQTT Startup EndpointsConfigurator public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddMqtt()) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddMqttEndpoints(endpoints => endpoints .Configure( config => config .WithClientId(\"order-service\") .ConnectViaTcp(\"localhost\") .SendLastWillMessage( lastWill => lastWill .Message(new TestamentMessage()) .ProduceTo(\"testaments\"))) .AddInbound(endpoint => endpoint .ConsumeFrom(\"basket-events\") .WithQualityOfServiceLevel( MqttQualityOfServiceLevel.AtLeastOnce)) .AddInbound(endpoint => endpoint .ConsumeFrom(\"payment-events\") .WithQualityOfServiceLevel( MqttQualityOfServiceLevel.AtLeastOnce)) .AddOutbound<IIntegrationEvent>(endpoint => endpoint .ProduceTo(\"order-events\") .WithQualityOfServiceLevel( MqttQualityOfServiceLevel.AtLeastOnce) .Retain())); } Important Silverback uses by default the v5 of the MQTT protocol, since it supports the user properties (headers). You can of course configure the client to use an older version but some Silverback functionalities (relying on message headers) might not work. RabbitMQ Startup EndpointsConfigurator public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddRabbit()) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddInbound( new RabbitExchangeConsumerEndpoint(\"basket-events\") { Connection = new RabbitConnectionConfig { HostName = \"localhost\", UserName = \"guest\", Password = \"guest\", }, Exchange = new RabbitExchangeConfig { IsDurable = true, IsAutoDeleteEnabled = false, ExchangeType = ExchangeType.Fanout }, QueueName = \"basket-events-order-service-queue\", Queue = new RabbitQueueConfig { IsDurable = true, IsExclusive = true, IsAutoDeleteEnabled = false } }) .AddInbound( new RabbitExchangeConsumerEndpoint(\"payment-events\") { Connection = new RabbitConnectionConfig { HostName = \"localhost\", UserName = \"guest\", Password = \"guest\", }, Exchange = new RabbitExchangeConfig { IsDurable = true, IsAutoDeleteEnabled = false, ExchangeType = ExchangeType.Fanout }, QueueName = \"payment-events-order-service-queue\", Queue = new RabbitQueueConfig { IsDurable = true, IsExclusive = true, IsAutoDeleteEnabled = false } }) .AddOutbound<IIntegrationEvent>( new RabbitExchangeProducerEndpoint(\"order-events\") { Connection = new RabbitConnectionConfig { HostName = \"localhost\", UserName = \"guest\", Password = \"guest\" }, Exchange = new RabbitExchangeConfig { IsDurable = true, IsAutoDeleteEnabled = false, ExchangeType = ExchangeType.Fanout } }); } } Tip All <xref:Silverback.Messaging.Configuration.IEndpointsConfigurator> implementations are registered as scoped services. Multiple implementations can be registered to split the configuration and of course dependencies (such as IOption or a DbContext ) can be injected to load the configuration variables. Important Starting from version 3.0.0 the broker(s) will be connected and all consumers started automatically at startup, unless explicitly disabled (see the Connection modes chapter for details). Inline endpoints configuration The preferred and suggested way to configure the message broker endpoints is using the <xref:Silverback.Messaging.Configuration.IEndpointsConfigurator> but you can use AddEndpoints (or AddKafkaEndpoints etc.) directly and configure everything inline. public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka()) .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(...) .AddOutbound<IIntegrationEvent>(...)); } } Multiple brokers It is possible to use multiple message broker implementation together in the same application. The following sample demonstrates how to consume from both Apache Kafka and RabbitMQ. Startup EndpointsConfigurator public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka() .AddRabbit()) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(...) .AddOutbound<IIntegrationEvent>(...)) .AddInbound( new RabbitExchangeConsumerEndpoint(\"rabbit-events\") { ... }); } } Connection modes You may not want to connect your broker immediately. In the following example is shown how to postpone the automatic connection after the application startup. public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka() .WithConnectionOptions(new BrokerConnectionOptions { Mode = BrokerConnectionMode.AfterStartup, RetryInterval = TimeSpan.FromMinutes(5) })) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } But it's also possible to completely disable the automatic connection and manually perform it. Startup Service public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka() .WithConnectionOptions(new BrokerConnectionOptions { Mode = BrokerConnectionMode.Manual })) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } public class BrokerConnectionService { private readonly IBroker _broker; public BrokerConnectionService(IBroker broker) { _broker = broker; } public async ConnectAsync() { broker.ConnectAsync(); } } Tip See the <xref:Silverback.Messaging.Configuration.BrokerConnectionOptions> documentation for details about the different options. Note Use <xref:Silverback.Messaging.Broker.IBrokerCollection> instead of <xref:Silverback.Messaging.Broker.IBroker> when multiple broker implementations are used. Important If your application is not running using an IHost ( GenericHost or WebHost , like in a normal ASP.NET Core application) you always need to manually connect it as shown in the second example above. Graceful shutdown It is important to properly close the consumers using the DisconnectAsync method before exiting. The offsets have to be committed and the broker has to be notified (it will then proceed to reassign the partitions as needed). Starting from version 3.0.0 this is done automatically (if your application is running using an IHost ( GenericHost or WebHost , like in a normal ASP.NET Core application). Health Monitoring The Silverback.Integration.HealthChecks package contains some extensions for Microsoft.Extensions.Diagnostics.HealthChecks that can be used to monitor the connection to the message broker. Currently, two checks exists: AddOutboundEndpointsCheck : Adds an health check that sends a ping message to all the outbound endpoints. AddOutboxCheck : Adds an health check that monitors the outbound queue (outbox table), verifying that the messages are being processed. AddConsumersCheck : Adds a health check that verifies that all consumers are connected. The usage is very simple, you just need to configure the checks in the Startup.cs, as shown in the following example. public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddHealthChecks() .AddOutboundEndpointsCheck() .AddOutboundQueueCheck() .AddConsumersCheck(); } public void Configure(IApplicationBuilder app) { app.UseHealthChecks(\"/health\"); } } Consumer management API The consumer exposes some information and statistics that can be used to programmatically check the consumer status (see <xref:Silverback.Messaging.Broker.IConsumer#Silverback_Messaging_Broker_IConsumer_StatusInfo>). A consumer can also be connected, started, stopped and disconnected at will. The following example shows a sample service that is used to monitor the total number of consumed message and restart the faulted consumers (the consumers get disconnected when an unhandled exception is thrown while processing the consumed message). public class ConsumerManagementService { private readonly IBrokerCollection _brokers; public ConsumerManagementService(IBrokerCollection brokers) { _brokers = brokers; } public int GetTotalConsumedMessages() { int totalCount = 0; foreach (var broker in _brokers) { foreach (var consumer in broker.Consumers) { totalCount += consumer.StatusInfo.ConsumedMessagesCount; } } } public void RestartDisconnectedConsumers() { foreach (var broker in _brokers) { if (!broker.IsConnected) continue; foreach (var consumer in broker.Consumers) { if (consumer.StatusInfo.Status == ConsumerStatus.Disconnected) { consumer.Connect(); } } } } } Samples All"
  },
  "concepts/broker/encryption.html": {
    "href": "concepts/broker/encryption.html",
    "title": "Encryption | Silverback",
    "keywords": "Encryption The end-to-end message encryption in Silverback is handled transparently in the producer and consumer and works independently from the used serializer or other features like chunking . The messages are transparently encrypted and decrypted. Symmetric encryption Enabling the end-to-end encryption using a symmetric algorithm just require an extra configuration in the endpoint. Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<InventoryEvent>(endpoint => endpoint .ProduceTo(\"inventory-events\") .EncryptUsingAes(encryptionKey)) .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .DecryptUsingAes(encryptionKey))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<InventoryEvent>( new KafkaProducerEndpoint(\"inventory-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, Encryption = new SymmetricEncryptionSettings { AlgorithmName = \"AES\", Key = encryptionKey } }) .AddInbound( new KafkaConsumerEndpoint(\"order-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" }, Encryption = new SymmetricEncryptionSettings { AlgorithmName = \"AES\", Key = encryptionKey } }); } The <xref:Silverback.Messaging.Encryption.SymmetricEncryptionSettings> class encapsulates all common settings of a symmetric algorithm (block size, initialization vector, ...). The AlgorithmName is used to load the algorithm implementation using the SymmetricAlgorithm.Create(string) method. Refer to the SymmetricAlgorithm class documentation to see which implementations are available in .net core are. Silverback uses Aes by default. Random initialization vector If no static initialization vector is provided, a random one is automatically generated per each message and prepended to the actual encrypted message. The consumer will automatically extract and use it. It is recommended to stick to this default behavior, for increased security. Ket rotation You can smoothly rotate the key being used to encrypt the messages. In the outbound endpoint you can specify the current key identifier to be submitted as header, while in the inbound endpoint a custom function can be used to provide the correct key, depending on the value in the header. This simple mechanism allows to consume messages that were encrypted using different keys, enabling key rotation and supporting a rolling update of the producers. Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<InventoryEvent>(endpoint => endpoint .ProduceTo(\"inventory-events\") .EncryptUsingAes(encryptionKey, \"key1\")) .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .DecryptUsingAes(keyIdentifier => { switch (keyIdentifier) { case \"key1\": return encryptionKey1; default: return encryptionKey2; } }))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<InventoryEvent>( new KafkaProducerEndpoint(\"inventory-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, Encryption = new SymmetricEncryptionSettings { AlgorithmName = \"AES\", KeyProvider = encryptionKey, KeyIdentifier = \"key1\" } }) .AddInbound( new KafkaConsumerEndpoint(\"order-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" }, Encryption = new SymmetricEncryptionSettings { AlgorithmName = \"AES\", KeyProvider = keyIdentifier => { switch (keyIdentifier) { case \"key1\": return encryptionKey1; default: return encryptionKey2; } } } }); }"
  },
  "concepts/broker/headers.html": {
    "href": "concepts/broker/headers.html",
    "title": "Message Headers | Silverback",
    "keywords": "Message Headers Custom headers There are multiple ways to add custom headers to an outbound message: adding an enricher to the <xref:Silverback.Messaging.IProducerEndpoint> annotating some properties with the <xref:Silverback.Messaging.Messages.HeaderAttribute>, as shown in the next chapter. using a custom <xref:Silverback.Messaging.Publishing.IBehavior> or <xref:Silverback.Messaging.Broker.Behaviors.IProducerBehavior> can be implemented, as shown in the Behaviors and Broker behaviors pipeline sections. Warning Some message broker implementations might not support headers and Silverback doesn't currently provide any workaround, thus the headers will simply be ignored. Using enrichers Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<InventoryEvent>(endpoint => endpoint .ProduceTo(\"inventory-events\") .AddHeader( \"x-my-header\", \"static value\") .AddHeader<InventoryEvent>( \"x-product-id\", envelope => envelope.Message?.ProductId))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<InventoryEvent>( new KafkaProducerEndpoint(\"inventory-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, MessageEnrichers = new List<IOutboundMessageEnricher> { new GenericOutboundHeadersEnricher( \"x-my-header\", \"static value\"), new GenericOutboundHeadersEnricher<InventoryEvent>( \"x-product-id\", envelope => envelope.Message?.ProductId) } }); } Using HeaderAttribute The <xref:Silverback.Messaging.Messages.HeaderAttribute> usage is very simple: you just have to decorate the properties you want to publish as headers and specify a name for the header. The headers value will also automatically be mapped back to the property upon consuming if the property declares a setter. using Silverback.Messaging.Messages; namespace Sample { public class OrderCreatedEvent { public List<LineItems> Items { get; set; } [Header(\"x-order-type\", PublishDefaultValue = true)] [JsonIgnore] public OrderType OrderType { get; set; } [Header(\"x-books-order\")] public bool ContainsBooks => Items.Any(item => item.Type == \"book\") [Header(\"x-dvd-order\")] public bool ContainsDvd => Items.Any(item => item.Type == \"dvd\") } } Note The PublishDefaultValue boolean property defines whether the header has to be published even if the property is set to the default value for its data type. The default is false . Note that the JsonIgnoreAttribute can be used to prevent the same properties to be serialized in the JSON body, when using the <xref:Silverback.Messaging.Serialization.JsonMessageSerializer>. Important Only the message type will be scanned, therefore the properties decorated with the <xref:Silverback.Messaging.Messages.HeaderAttribute> must be in the root of the message object. Default headers Silverback will add some headers to the produced messages. They may vary depending on the scenario. Here is the list of the default headers that may be sent. Header Key Description x-message-id The message identifier . x-message-type The assembly qualified name of the message type. Used by the default <xref:Silverback.Messaging.Serialization.JsonMessageSerializer>. x-failed-attempts If an exception if thrown the failed attempts will be incremented and stored as header. This is necessary for the error policies to work. x-source-endpoint This will be set by the Move error policy and will contain the name of the endpoint the failed message is being moved from. x-chunk-index The message chunk index, used when chunking is enabled. x-chunk-count The total number of chunks the message was split into, used when chunking is enabled. x-chunk-last A boolean value indicating whether the message is the last one of a chunks sequence, used when chunking is enabled. x-first-chunk-offset The <xref:Silverback.Messaging.Broker.IBrokerMessageOffset> value of the first chunk of the same message, used when chunking is enabled. traceparent Used for distributed tracing. It is set by the <xref:Silverback.Messaging.Broker.IProducer> using the current Activity.Id . The <xref:Silverback.Messaging.Broker.IConsumer> uses it's value to set the Activity.ParentId . Note that an Activity is automatically started by the default <xref:Silverback.Messaging.Broker.IProducer> implementation. The header is implemented according to the W3C Trace Context proposal . tracestate Used for distributed tracing. It corresponds to the Activity.TraceStateString . The header is implemented according to the W3C Trace Context proposal . tracebaggage Used for distributed tracing. It corresponds to the string representation of the Activity.Baggage dictionary. This is not part of the w3c standard. content-type The content type of the binary file , used when producing or consuming an <xref:Silverback.Messaging.Messages.IBinaryFileMessage>. x-failure-reason The header that will be set by the <xref:Silverback.Messaging.Inbound.ErrorHandling.MoveMessageErrorPolicy> and will contain the reason why the message failed to be processed. Kafka specific Header Key Description x-kafka-message-key The header that will be filled with the key of the message consumed from Kafka. x-kafka-message-timestamp The header that will be filled with the timestamp of the message consumed from Kafka. x-source-consumer-group-id The header that will be set by the <xref:Silverback.Messaging.Inbound.ErrorHandling.MoveMessageErrorPolicy> and will contain the GroupId the consumer that consumed the message that failed to be processed. x-source-topic The header that will be set by the <xref:Silverback.Messaging.Inbound.ErrorHandling.MoveMessageErrorPolicy> and will contain the source topic of the message that failed to be processed. x-source-partition The header that will be set by the <xref:Silverback.Messaging.Inbound.ErrorHandling.MoveMessageErrorPolicy> and will contain the source partition of the message that failed to be processed. x-source-offset The header that will be set by the <xref:Silverback.Messaging.Inbound.ErrorHandling.MoveMessageErrorPolicy> and will contain the offset of the message that failed to be processed. x-source-timestamp The header that will be set by the <xref:Silverback.Messaging.Inbound.ErrorHandling.MoveMessageErrorPolicy> and will contain the timestamp of the message that failed to be processed. The static classes <xref:Silverback.Messaging.Messages.DefaultMessageHeaders> and <xref:Silverback.Messaging.Messages.KafkaMessageHeaders> contain all default header names constants. Customizing header names The default header names can be overridden using the WithCustomHeaderName configuration method. public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka()) .AddEndpointsConfigurator<MyEndpointsConfigurator>() .WithCustomHeaderName(DefaultMessageHeaders.ChunkId, \"x-ch-id\") .WithCustomHeaderName(DefaultMessageHeaders.ChunksCount, \"x-ch-cnt\")); } }"
  },
  "concepts/broker/inbound.html": {
    "href": "concepts/broker/inbound.html",
    "title": "Inbound Endpoint | Silverback",
    "keywords": "Inbound Endpoint An inbound endpoint is used to configure Silverback to automatically consume a topic/queue and relay the messages to the internal bus. If no exception is thrown by the subscribers, the message is acknowledged and the next one is consumed. The endpoint object identifies the topic/queue that is being connected and the client configuration, such the connection options. The endpoint object is therefore very specific and every broker type will define it's own implementation of IConsumerEndpoint . The options in the endpoint object are also used to tweak the Silverback behavior (e.g. the deserialization ) and to enable additional features such as batch processing , decryption , etc. Note Silverback abstracts the message broker completely and the messages are automatically acknowledged if the subscribers complete without throwing an exception. Apache Kafka The <xref:Silverback.Messaging.KafkaConsumerEndpoint> is defined by Silverback.Integration.Kafka and is used to declare an inbound endpoint connected to Apache Kafka. Fluent (preferred) Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\", \"inventory-events\") .Configure(config => { config.GroupId = \"my-consumer\"; config.AutoOffsetReset = AutoOffsetResetType.Earliest; } .OnError(policy => policy.Retry(5)))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new KafkaConsumerEndpoint( \"order-events\", \"inventory-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\", AutoOffsetReset = AutoOffsetResetType.Earliest }, ErrorPolicy = new RetryErrorPolicy().MaxFailedAttempts(5) }); } Note You can decide whether to use one consumer per topic or subscribe multiple topics with the same consumer (passing multiple topic names in the endpoint constructor, as shown in the example above). There are advantages and disadvantages of both solutions and the best choice really depends on your specific requirements, the amount of messages being produced, etc. Anyway the main difference is that when subscribing multiple topics you will still consume one message after the other but they will simply be interleaved (this may or may not be an issue, it depends) and on the other hand each consumer will use some resources, so creating multiple consumers will result in a bigger overhead. Note For a more in-depth documentation about the Kafka client configuration refer also to the confluent-kafka-dotnet documentation . MQTT The <xref:Silverback.Messaging.MqttConsumerEndpoint> is defined by Silverback.Integration.MQTT and is used to declare an inbound endpoint connected to an MQTT broker. Fluent (preferred) Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddMqttEndpoints(endpoints => endpoints .Configure( config => config .WithClientId(\"order-service\") .ConnectViaTcp(\"localhost\") .SendLastWillMessage( lastWill => lastWill .Message(new TestamentMessage()) .ProduceTo(\"testaments\"))) .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\", \"inventory-events\") .WithQualityOfServiceLevel( MqttQualityOfServiceLevel.AtLeastOnce) .OnError(policy => policy.Retry(5)))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new MqttConsumerEndpoint( \"order-events\", \"inventory-events\") { Configuration = { ClientId = \"order-service\", ChannelOptions = new MqttClientTcpOptions { Server = \"localhost\" }, WillMessage = new MqttApplicationMessage() { ... } }, QualityOfServiceLevel = MqttQualityOfServiceLevel.AtLeastOnce ErrorPolicy = new RetryErrorPolicy().MaxFailedAttempts(5) }); } Note It doesn't matter how you configure the inbound and outbound endpoints, a single client will be created as long as all endpoints match the exact same configuration. (Using a slightly different configuration for the same client it will cause an exception to be thrown when validating the endpoints configuration.) Note For a more in-depth documentation about the MQTT client configuration refer also to the MQTTNet documentation . RabbitMQ Silverback.Integration.RabbitMQ is a bit more intricate and uses 2 different classes to specify an endpoint that connects to a queue (<xref:Silverback.Messaging.RabbitQueueConsumerEndpoint>) or directly to an exchange (<xref:Silverback.Messaging.RabbitExchangeConsumerEndpoint>). public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new RabbitQueueConsumerEndpoint(\"inventory-commands-queue\") { Connection = new RabbitConnectionConfig { HostName = \"localhost\", UserName = \"guest\", Password = \"guest\" }, Queue = new RabbitQueueConfig { IsDurable = true, IsExclusive = false, IsAutoDeleteEnabled = false } }) .AddInbound( new RabbitExchangeConsumerEndpoint(\"order-events\") { Connection = new RabbitConnectionConfig { HostName = \"localhost\", UserName = \"guest\", Password = \"guest\" }, Exchange = new RabbitExchangeConfig { IsDurable = true, IsAutoDeleteEnabled = false, ExchangeType = ExchangeType.Fanout }, QueueName = \"my-consumer-group\", Queue = new RabbitQueueConfig { IsDurable = true, IsExclusive = false, IsAutoDeleteEnabled = false } }); } Note For a more in-depth documentation about the RabbitMQ configuration refer to the RabbitMQ tutorials and documentation . Error handling If an exceptions is thrown by the methods consuming the incoming messages (subscribers) the consumer will stop, unless some error policies are defined. The built-in policies are: <xref:Silverback.Messaging.Inbound.ErrorHandling.StopConsumerErrorPolicy> (default) <xref:Silverback.Messaging.Inbound.ErrorHandling.SkipMessageErrorPolicy> <xref:Silverback.Messaging.Inbound.ErrorHandling.RetryErrorPolicy> <xref:Silverback.Messaging.Inbound.ErrorHandling.MoveMessageErrorPolicy> <xref:Silverback.Messaging.Inbound.ErrorHandling.ErrorPolicyChain> Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\", \"inventory-events\") .Configure(config => { config.GroupId = \"my-consumer\"; config.AutoOffsetReset = AutoOffsetResetType.Earliest; }) .OnError(policy => policy .Retry(3, TimeSpan.FromSeconds(1)) .ThenSkip()))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new KafkaConsumerEndpoint( \"order-events\", \"inventory-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\", AutoOffsetReset = AutoOffsetResetType.Earliest }, ErrorPolicy = new ErrorPolicyChain( new RetryErrorPolicy().MaxFailedAttempts(5), new SkipErrorPolicy()) }); } Important If the processing still fails after the last policy is applied the exception will be returned to the consumer, causing it to stop. Important The number of attempts are tracked according to the message id header . A message id must be provided in order for the MaxFailedAttempts mechanism to work. This is ensured by the Silverback producer but might not be the case when consuming messages coming from other sources. Some message broker implementations might transparently cope with the missing message id header and derive it from other identifiers (e.g. the kafka message key) but it's not automatically guaranteed that they will always be unique. You should carefully check that before relying on this feature. Important The <xref:Silverback.Messaging.Inbound.ErrorHandling.RetryErrorPolicy> will prevent the message broker to be polled for the duration of the configured delay, which could lead to a timeout. With Kafka you should for example set the max.poll.interval.ms settings to an higher value. Apply rules Use ApplyTo and Exclude methods to decide which exceptions must be handled by the error policy or take advantage of ApplyWhen to specify a custom apply rule. .OnError(policy => policy .MoveToKafkaTopic( moveEndpoint => moveEndpoint.ProduceTo(\"some-other-topic\"), movePolicy => movePolicy .ApplyTo<MyException>() .ApplyWhen((msg, ex) => msg.Xy == myValue)) .ThenSkip()); public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\", \"inventory-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .OnError(policy => policy .MoveToKafkaTopic( moveEndpoint => moveEndpoint.ProduceTo(\"some-other-topic\"), movePolicy => movePolicy .ApplyTo<MyException>() .ApplyWhen((msg, ex) => msg.Xy == myValue)) .ThenSkip()))); } Publishing events Messages can be published when a policy is applied, in order to execute custom code. EndpointsConfigurator Event Handler public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\", \"inventory-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .OnError(policy => policy .Retry(3, TimeSpan.FromSeconds(1)) .ThenSkip(skipPolicy => skipPolicy .Publish(msg => new ProcessingFailedEvent(msg)))))); } public void OnProcessingFailed(ProcessingFailedEvent @event) { _processingStatusService.SetFailed(@event.Message.Id); _mailService.SendNotification(\"Failed to process message!\"); } Batch processing In some scenario, when having to deal with huge amounts of messages, processing each one of them on its own isn't the most efficient approach. Batch processing allow to process an arbitrary number of unrelated messages as a single unit of work. The messages are processed in batches. Refer to the <xref:Silverback.Messaging.Sequences.Batch.BatchSettings> documentation for details about the configuration. The batch can be subscribed either as IEnumerable , IAsyncEnumerable or <xref:Silverback.Messaging.Messages.IMessageStreamEnumerable`1>. See also Streaming for details. EndpointsConfigurator (fluent) EndpointsConfigurator (legacy) Subscriber public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"inventory-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .EnableBatchProcessing(100, TimeSpan.FromSeconds(5)))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new KafkaConsumerEndpoint(\"inventory-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" }, Batch = new Messaging.Batch.BatchSettings { Size = 100, MaxWaitTime = TimeSpan.FromSeconds(5) } }); } public class InventoryService { private DbContext _db; public InventoryService(MyDbContext db) { _db = db; } public async Task OnBatchReceived(IAsyncEnumerable<InventoryUpdateEvent> messages) { async foreach (var message in messages) { // Process each message } // Commit all changes in a single transaction await _db.SaveChangesAsync(); } } Parallelism The consumer processes the messages sequentially, this is by design. The <xref:Silverback.Messaging.Broker.KafkaConsumer> is a bit special and actually processes each assigned partition independently and concurrently. This feature can be toggled using the ProcessAllPartitionsTogether and ProcessPartitionsIndependently methods of the <xref:Silverback.Messaging.Configuration.Kafka.IKafkaConsumerEndpointBuilder> (or the KafkaConsumerEndpoint.ProcessPartitionsIndependently property), while the LimitParallelism method (or the KafkaConsumerEndpoint.MaxDegreeOfParallelism property) can be used to limit the number of messages being actually processed concurrently. Exactly-once processing Silverback is able to keep track of the messages that have been consumed in order to guarantee that each message is processed exactly once. Offset storage The <xref:Silverback.Messaging.Inbound.ExactlyOnce.OffsetStoreExactlyOnceStrategy> will store the offset of the latest processed message (of each topic/partition) into a database table. The offsets are being stored to prevent the very same message to be consumed twice. Note The Silverback.Core.EntityFrameworkCore package is also required and the DbContext must include a DbSet of <xref:Silverback.Database.Model.StoredOffset>. See also the Sample DbContext (EF Core) . Startup EndpointsConfigurator (fluent) EndpointsConfigurator (legacy) public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .UseDbContext<MyDbContext>() .WithConnectionToMessageBroker(options => options .AddKafka() .AddOffsetStoreDatabaseTable()) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"inventory-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .EnsureExactlyOnce(strategy => strategy.StoreOffsets()))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new KafkaConsumerEndpoint(\"inventory-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" }, ExactlyOnceStrategy = new OffsetStoreExactlyOnceStrategy() }); } Inbound log The <xref:Silverback.Messaging.Inbound.ExactlyOnce.LogExactlyOnceStrategy> will store the identifiers of all processed messages into a database table. The inbound messages are logged to prevent two messages with the same key to be consumed. Note The Silverback.Core.EntityFrameworkCore package is also required and the DbContext must include a DbSet of <xref:Silverback.Database.Model.InboundLogEntry>. See also the Sample DbContext (EF Core) . Startup EndpointsConfigurator (fluent) EndpointsConfigurator (legacy) public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .UseDbContext<MyDbContext>() .WithConnectionToMessageBroker(options => options .AddKafka() .AddInboundLogDatabaseTable()) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"inventory-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .EnsureExactlyOnce(strategy => strategy.LogMessages()))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new KafkaConsumerEndpoint(\"inventory-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" }, ExactlyOnceStrategy = new LogExactlyOnceStrategy() }); } Custom store At the moment only a database accessed using Entity Framework is supported as offset or log storage, but a custom storage can be used implementing <xref:Silverback.Messaging.Inbound.ExactlyOnce.Repositories.IOffsetStore> or <xref:Silverback.Messaging.Inbound.ExactlyOnce.Repositories.IInboundLog>. public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .UseDbContext<MyDbContext>() .WithConnectionToMessageBroker(options => options .AddKafka() .AddOffsetStore<MyCustomOffsetStore>()) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } Samples All"
  },
  "concepts/broker/inboundenvelope.html": {
    "href": "concepts/broker/inboundenvelope.html",
    "title": "IInboundEnvelope | Silverback",
    "keywords": "IInboundEnvelope When a message is consumed Silverback wraps it into an <xref:Silverback.Messaging.Messages.IInboundEnvelope`1> and pushes it to the message bus. Both the <xref:Silverback.Messaging.Messages.IInboundEnvelope`1> or the contained message in its pure form can be subscribed. You can take advantage of this mechanism to gain access to the transport information of the message, since the <xref:Silverback.Messaging.Messages.IInboundEnvelope`1> holds all the information like endpoint, offset and headers data. Subscribing to the <xref:Silverback.Messaging.Messages.IInboundEnvelope`1> works exactly the same as subscribing to any other message. public class SubscribingService { public async Task OnWrappedMessageReceived(IInboundEnvelope<SampleMessage> envelope) { // ...your message handling logic... } public async Task OnPureMessageReceived(SampleMessage message) { // ...your message handling logic... } } Note Subscribing to the non-generic <xref:Silverback.Messaging.Messages.IInboundEnvelope> or <xref:Silverback.Messaging.Messages.IRawInboundEnvelope> it is possible to subscribe even the messages with an empty body."
  },
  "concepts/broker/kafka/kafka-events.html": {
    "href": "concepts/broker/kafka/kafka-events.html",
    "title": "Kafka Events | Silverback",
    "keywords": "Kafka Events The underlying library ( Confluent.Kafka ) uses some events to let you catch important information, interact with the partitions assignment process, etc. Silverback proxies those events to give you full access to those features. Consumer events These callbacks are available: <xref:Silverback.Messaging.Broker.Callbacks.IKafkaPartitionsAssignedCallback> <xref:Silverback.Messaging.Broker.Callbacks.IKafkaPartitionsRevokedCallback> <xref:Silverback.Messaging.Broker.Callbacks.IKafkaOffsetCommittedCallback> <xref:Silverback.Messaging.Broker.Callbacks.IKafkaConsumerErrorCallback> <xref:Silverback.Messaging.Broker.Callbacks.IKafkaConsumerStatisticsCallback> <xref:Silverback.Messaging.Broker.Callbacks.IKafkaConsumerLogCallback> <xref:Silverback.Messaging.Broker.Callbacks.IKafkaPartitionEofCallback> Offset reset example In the following example the partitions assigned event is subscribed in order to reset the start offsets and replay the past messages. Startup ResetOffsetPartitionsAssignedCallbackHandler public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka()) .AddSingletonBrokerCallbacksHandler<ResetOffsetPartitionsAssignedCallbackHandler>(); } } public class ResetOffsetPartitionsAssignedCallbackHandler : IKafkaPartitionsAssignedCallback { public IEnumerable<TopicPartitionOffset> OnPartitionsAssigned( IReadOnlyCollection<TopicPartition> topicPartitions, KafkaConsumer consumer) => topicPartitions.Select( topicPartition => new TopicPartitionOffset(topicPartition, Offset.Beginning)); } Producer events These callbacks are available: <xref:Silverback.Messaging.Broker.Callbacks.IKafkaProducerStatisticsCallback> <xref:Silverback.Messaging.Broker.Callbacks.IKafkaProducerLogCallback> See also Broker Callbacks"
  },
  "concepts/broker/kafka/kafka-partitioning.html": {
    "href": "concepts/broker/kafka/kafka-partitioning.html",
    "title": "Kafka Partitioning and Message Key | Silverback",
    "keywords": "Kafka Partitioning and Message Key Producer Destination partition If the destination topic contains multiple partitions, the destination partition is picked according to the hash of the message key . If no explicit message key was set, a random one is generated, resulting in the messages being randomly spread across the partitions. You can override this default behavior explicitly setting the target partition in the endpoint. The endpoint can be statically defined like in the following snippet or resolved via dynamic routing . Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<IIntegrationEvent>(endpoint => endpoint .ProduceTo(\"order-events\", 2))); // <- partition 2 } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<IIntegrationEvent>( new KafkaProducerEndpoint(\"order-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, Partition = 2 }); } Producing to a fixed partition may be required in the case you have multiple producers to the same topic and you have to prevent the messages from the different clients to be interleaved (e.g. because you are relying on sequences, like chunking ). Message key Apache Kafka require a message key for different purposes, such as: Partitioning : Kafka can guarantee ordering only inside the same partition and it is therefore important to be able to route correlated messages into the same partition. To do so you need to specify a key for each message and Kafka will put all messages with the same key in the same partition. Compacting topics : A topic can be configured with cleanup.policy=compact to instruct Kafka to keep only the latest message related to a certain object, identified by the message key. In other words Kafka will retain only 1 message per each key value. The messages with the same key are guaranteed to be written to the same partition. Silverback will always generate a message key (same value as the x-message-id header ) but you can also generate your own key, either adding an enricher to the <xref:Silverback.Messaging.IProducerEndpoint> or decorating the properties that must be part of the key with <xref:Silverback.Messaging.Messages.KafkaKeyMemberAttribute>. Using enricher Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<InventoryEvent>(endpoint => endpoint .ProduceTo(\"inventory-events\") .WithKafkaKey<InventoryEvent>( envelope => envelope.Message?.ProductId))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<InventoryEvent>( new KafkaProducerEndpoint(\"inventory-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, MessageEnrichers = new List<IOutboundMessageEnricher> { new OutboundMessageKafkaKeyEnricher<InventoryEvent>( envelope => envelope.Message?.ProductId) } }); } Using KafkaKeyMemberAttribute public class MultipleKeyMembersMessage : IIntegrationMessage { public Guid Id { get; set; } [KafkaKeyMember] public string One { get; set; } [KafkaKeyMember] public string Two { get; set; } public string Three { get; set; } } Note The message key will also be received as header (see Message Headers for details). Consumer Partitions processing While using a single poll loop, Silverback processes the messages consumed from each Kafka partition independently and concurrently. By default up to 10 messages/partitions are processed concurrently (per topic). This value can be tweaked in the endpoint configuration or disabled completely. Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\") .LimitParallelism(2) .Configure(config => { config.GroupId = \"my-consumer\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"inventory-events\") .ProcessAllPartitionsTogether() .Configure(config => { config.GroupId = \"my-consumer\"; }))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new KafkaConsumerEndpoint(\"order-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\", }, MaxDegreeOfParallelism = 2 }) .AddInbound( new KafkaConsumerEndpoint(\"inventory-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\", }, ProcessPartitionsIndependently = false }); } Manual partitions assignment In some cases you don't want to let the broker randomly distribute the partitions among the consumers. This might also be useful when dealing with large sequences (e.g. large messages/files being chunked or when batch processing ), to prevent that a rebalance occurs in the middle of a sequence, forcing the consumer to abort and restart from the beginning. The assignment can either be completely static or dynamic using a resolver function that will receive all available partitions as input (see <xref:Silverback.Messaging.Configuration.Kafka.IKafkaConsumerEndpointBuilder> and <xref:Silverback.Messaging.KafkaConsumerEndpoint> for details). Fluent (static) Fluent (dynamic) Legacy (static) Legacy (dynamic) public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom( new TopicPartition(\"order-events\", 0), new TopicPartition(\"order-events\", 1)) .Configure(config => { config.GroupId = \"my-consumer\"; }))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom( \"order-events\", partitions => partitions .Where(partition => partition.Partition % 2 == 0)) .Configure(config => { config.GroupId = \"my-consumer\"; }))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new KafkaConsumerEndpoint( new TopicPartition(\"order-events\", 0), new TopicPartition(\"order-events\", 1)) { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" } }); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new KafkaConsumerEndpoint( \"order-events\", partitions => partitions .Where(partition => partition.Partition % 2 == 0)) { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" } }); } Samples Kafka - Files Streaming"
  },
  "concepts/broker/kafka/multiple-consumer-groups.html": {
    "href": "concepts/broker/kafka/multiple-consumer-groups.html",
    "title": "Multiple Consumer Groups (in same process) | Silverback",
    "keywords": "Multiple Consumer Groups (in same process) In some cases you may want to subscribe multiple times the same consumed message, to perform independent tasks. Having multiple subscribers handling the very same message is not a good idea since a failure in one of them will cause the message to be consumed again and thus reprocessed by all subscribers. A much safer approach is to bind multiple consumers to the same topic, using a different consumer group id. This will cause the message to be consumed multiple times (once per consumer group) and being committed independently. The <xref:Silverback.Messaging.Subscribers.KafkaGroupIdFilterAttribute> can be used to execute a subscribed method according to the group id. EndpointsConfigurator (fluent) EndpointsConfigurator (legacy) Subscriber public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"document-events\") .Configure(config => { config.GroupId = \"group1\"; })) .AddInbound(endpoint => endpoint .ConsumeFrom(\"document-events\") .Configure(config => { config.GroupId = \"group2\"; }))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new KafkaConsumerEndpoint(\"document-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"group1\" } }) .AddInbound( new KafkaConsumerEndpoint(\"document-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"group2\" } }); } public class MySubscriber { [KafkaGroupIdFilter(\"group1\")] public void PerformTask1(MyEvent @event) => ... [KafkaGroupIdFilter(\"group2\")] public void PerformTask2(MyEvent @event) => ... } Note The filters can be added dynamically using the overloads of AddSubscriber accepting a <xref:Silverback.Messaging.Subscribers.Subscriptions.SubscriptionOptions> or <xref:Silverback.Messaging.Subscribers.Subscriptions.TypeSubscriptionOptions> and this allows you to use a variable for the group id. .AddSingletonSubscriber<MySubscriber>( new TypeSubscriptionOptions { Filters = new[] { new KafkaGroupIdFilterAttribute(\"consumer1\") } }) Using the <xref:Silverback.Messaging.Subscribers.KafkaGroupIdFilterAttribute> is the cleanest and easiest approach but alternatively you can always subscribe to the <xref:Silverback.Messaging.Messages.IInboundEnvelope`1> and perform different tasks according to the GroupId value. public class MySubscriber { public void OnMessageReceived(IInboundEnvelope<MyEvent> envelope) { switch (((KafkaConsumerEndpoint)envelope.Endpoint).Configuration.GroupId) { case \"group1\": PerformTask1(envelope.Message); break; case \"group2\": PerformTask2(envelope.Message); break; } } private void PerformTask1(MyEvent @event) => ... private void PerformTask2(MyEvent @event) => ... }"
  },
  "concepts/broker/message-id.html": {
    "href": "concepts/broker/message-id.html",
    "title": "Message Identifier | Silverback",
    "keywords": "Message Identifier Silverback will ensure that an x-message-id header is always set with each message. This header is used mostly for tracing purpose and it's value is always printed in the logs generated by Silverback. The producer will automatically generate a random Guid to be used as message identifier. (This value will be used also as Kafka key by default, see Kafka Partitioning and Message Key .) In the consumer side the message identifier may be used also to rebuild the chunks sequence , implement exactly once processing and similar. If the header is not present, the consumer may artificially set if with another identifier such as the Kafka key (see Kafka Partitioning and Message Key ). Custom value It is of course possible to use a customized message identifier instead of a random Guid , simply overriding the x-message-id header as shown in the following snippet. More information about the message headers can be found in the Message Headers section. using Silverback.Messaging.Messages; namespace Sample { public class OrderSubmittedEvent { [Header(DefaultMessageHeaders.MessageId)] public string UniqueOrderNumber { get; set; } } } Note This example assumes that only one message per each order is published to the same endpoint, because the message id should be unique in order for the various features relying on it to work properly."
  },
  "concepts/broker/mqtt/mqtt-events.html": {
    "href": "concepts/broker/mqtt/mqtt-events.html",
    "title": "MQTT Events | Silverback",
    "keywords": "MQTT Events Some lifetime events are fired by the <xref:Silverback.Messaging.Broker.MqttBroker> and can be handled using the following callbacks: <xref:Silverback.Messaging.Broker.Callbacks.IMqttClientConnectedCallback> <xref:Silverback.Messaging.Broker.Callbacks.IMqttClientDisconnectingCallback> Example In the following example a message is sent as soon as the client is connected. Startup ConnectionCallbackHandler public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddMqtt()) .AddSingletonBrokerCallbacksHandler<ConnectionCallbackHandler>(); } } public class ConnectionCallbackHandler : IMqttClientConnectedCallback { private readonly IPublisher _publisher; public ConnectionCallbackHandler(IPublisher publisher) { _publisher = publisher; } public Task OnClientConnectedAsync(MqttClientConfig config) => _publisher.PublishAsync(new ClientConnectedMessage()); } See also Broker Callbacks"
  },
  "concepts/broker/mqtt/multiple-clients.html": {
    "href": "concepts/broker/mqtt/multiple-clients.html",
    "title": "Multiple Clients (in same process) | Silverback",
    "keywords": "Multiple Clients (in same process) In some cases you may want to subscribe multiple times the same consumed message, to perform independent tasks. Having multiple subscribers handling the very same message is not a good idea since a failure in one of them will cause the message to be consumed again and thus reprocessed by all subscribers. A much safer approach is to bind multiple consumers to the same topic, using a different client id. This will cause the message to be consumed multiple times (once per client) and being committed independently. The <xref:Silverback.Messaging.Subscribers.MqttClientIdFilterAttribute> can be used to execute a subscribed method according to the client id. EndpointsConfigurator (fluent) EndpointsConfigurator (legacy) Subscriber public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddMqttEndpoints(endpoints => endpoints .Configure( config => config .ConnectViaTcp(\"localhost\")) .AddInbound(endpoint => endpoint .Configure(config => config.WithClientId(\"client1\")) .ConsumeFrom(\"document-events\")) .AddInbound(endpoint => endpoint .Configure(config => config.WithClientId(\"client2\")) .ConsumeFrom(\"document-events\"))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddInbound( new MqttConsumerEndpoint(\"document-events\") { Configuration = { ClientId = \"client1\", ChannelOptions = new MqttClientTcpOptions { Server = \"localhost\" } } }) .AddInbound( new MqttConsumerEndpoint(\"document-events\") { Configuration = { ClientId = \"client2\", ChannelOptions = new MqttClientTcpOptions { Server = \"localhost\" } } }); } public class MySubscriber { [MqttClientIdFilter(\"client1\")] public void PerformTask1(MyEvent @event) => ... [MqttClientIdFilter(\"client2\")] public void PerformTask2(MyEvent @event) => ... } Note The filters can be added dynamically using the overloads of AddSubscriber accepting a <xref:Silverback.Messaging.Subscribers.Subscriptions.SubscriptionOptions> or <xref:Silverback.Messaging.Subscribers.Subscriptions.TypeSubscriptionOptions> and this allows you to use a variable for the client id. .AddSingletonSubscriber<MySubscriber>( new TypeSubscriptionOptions { Filters = new[] { new MqttClientIdFilterAttribute(\"client1\") } }) Using the <xref:Silverback.Messaging.Subscribers.MqttClientIdFilterAttribute> is the cleanest and easiest approach but alternatively you can always subscribe to the <xref:Silverback.Messaging.Messages.IInboundEnvelope`1> and perform different tasks according to the ClientId value. public class MySubscriber { public void OnMessageReceived(IInboundEnvelope<MyEvent> envelope) { switch (((MqttConsumerEndpoint)envelope.Endpoint).Configuration.ClientId) { case \"client1\": PerformTask1(envelope.Message); break; case \"client2\": PerformTask2(envelope.Message); break; } } private void PerformTask1(MyEvent @event) => ... private void PerformTask2(MyEvent @event) => ... }"
  },
  "concepts/broker/outbound.html": {
    "href": "concepts/broker/outbound.html",
    "title": "Outbound Endpoint | Silverback",
    "keywords": "Outbound Endpoint An outbound endpoint is used to configure silverback to automatically relay the integration messages that ate published to the internal bus to the message broker. Multiple outbound endpoints can be configured and Silverback will route the messages according to their type or a custom routing logic. The endpoint object identifies the topic/queue that is being connected and the client configuration, such the connection options. The endpoint object is therefore very specific and every broker type will define it's own implementation of IProducerEndpoint . The options in the endpoint object are also used to tweak the Silverback behavior (e.g. the serialization ) and to enable additional features such as chunking , encryption , etc. Apache Kafka The <xref:Silverback.Messaging.KafkaProducerEndpoint> is defined by Silverback.Integration.Kafka and is used to declare an outbound endpoint connected to Apache Kafka. Fluent (preferred) Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<IIntegrationEvent>(endpoint => endpoint .ProduceTo(\"order-events\") .EnableChunking(500000) .ProduceToOutbox())); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<IIntegrationEvent>( new KafkaProducerEndpoint(\"order-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, Chunk = new ChunkSettings { Size = 500000 }, Strategy = new OutboxProduceStrategy() }); } Note For a more in-depth documentation about the Kafka client configuration refer also to the confluent-kafka-dotnet documentation . MQTT The <xref:Silverback.Messaging.MqttProducerEndpoint> is defined by Silverback.Integration.MQTT and is used to declare an outbound endpoint connected to an MQTT broker. Fluent (preferred) Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddMqttEndpoints(endpoints => endpoints .Configure( config => config .WithClientId(\"order-service\") .ConnectViaTcp(\"localhost\") .SendLastWillMessage( lastWill => lastWill .Message(new TestamentMessage()) .ProduceTo(\"testaments\"))) .AddOutbound<IIntegrationEvent>(endpoint => endpoint .ProduceTo(\"order-events\") .WithQualityOfServiceLevel( MqttQualityOfServiceLevel.AtLeastOnce) .Retain())); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<IIntegrationEvent>( new MqttProducerEndpoint(\"order-events\") { Configuration = { ClientId = \"order-service\", ChannelOptions = new MqttClientTcpOptions { Server = \"localhost\" }, WillMessage = new MqttApplicationMessage() { ... } }, QualityOfServiceLevel = MqttQualityOfServiceLevel.AtLeastOnce, Retain = true }); } Note For a more in-depth documentation about the MQTT client configuration refer also to the MQTTNet documentation . RabbitMQ Silverback.Integration.RabbitMQ is a bit more intricate and uses 2 different classes to specify an endpoint that connects to a queue (<xref:Silverback.Messaging.RabbitQueueProducerEndpoint>) or directly to an exchange (<xref:Silverback.Messaging.RabbitExchangeProducerEndpoint>). public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<IIntegrationEvent>( new RabbitQueueProducerEndpoint(\"inventory-commands-queue\") { Connection = new RabbitConnectionConfig { HostName = \"localhost\", UserName = \"guest\", Password = \"guest\" }, Queue = new RabbitQueueConfig { IsDurable = true, IsExclusive = false, IsAutoDeleteEnabled = false } }) .AddOutbound<IIntegrationEvent>( new RabbitExchangeProducerEndpoint(\"order-events\") { Connection = new RabbitConnectionConfig { HostName = \"localhost\", UserName = \"guest\", Password = \"guest\" }, Exchange = new RabbitExchangeConfig { IsDurable = true, IsAutoDeleteEnabled = false, ExchangeType = ExchangeType.Fanout } }); } Note For a more in-depth documentation about the RabbitMQ configuration refer to the RabbitMQ tutorials and documentation . Transactional outbox strategy The transactional outbox pattern purpose is to reliably update the database and publish the messages in the same atomic transaction. This is achieved storing the outbound messages into a temporary outbox table, whose changes are committed together with the other changes to the rest of the data. Messages 1, 2 and 3 are stored in the outbox table and produced by a separate thread or process. When using entity framework the outbound messages are stored into a DbSet and are therefore implicitly saved in the same transaction used to save all other changes. Note The Silverback.Core.EntityFrameworkCore package is also required and the DbContext must include a DbSet of <xref:Silverback.Database.Model.OutboxMessage>. See also the Sample DbContext (EF Core) . Important The current <xref:Silverback.Messaging.Outbound.TransactionalOutbox.OutboxWorker> cannot scale horizontally and starting multiple instances will cause the messages to be produced multiple times. In the following example a distributed lock (stored in the database) is used to ensure that only one instance is running and another one will immediately take over when it stops (the DbContext must include a DbSet of <xref:Silverback.Database.Model.Lock> as well, see also the Sample DbContext (EF Core) ). Startup EndpointsConfigurator (fluent) EndpointsConfigurator (legacy) Publisher public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .UseDbContext<MyDbContext>() // Setup the lock manager using the database // to handle the distributed locks. // If this line is omitted the OutboundWorker will still // work without locking. .AddDbDistributedLockManager() .WithConnectionToMessageBroker(options => options .AddKafka() .AddOutboxDatabaseTable() .AddOutboxWorker()) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<IIntegrationEvent>( endpoint => endpoint .ProduceTo(\"order-events\") .ProduceToOutbox())); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<IIntegrationEvent>( new KafkaProducerEndpoint(\"order-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, Strategy = new OutboxProduceStrategy() }); } private readonly IPublisher _publisher; private readonly SampleDbContext _dbContext; public async Task CancelOrder(int orderId) { // You can use _dbContext to update/insert entities here await _publisher.PublishAsync(new OrderCancelledEvent { OrderId = orderId }); // No messages will be published unless you call SaveChangesAsync! await _dbContext.SaveChangesAsync(); } Custom outbox You can easily use another kind of storage as outbox, simply creating your own <xref:Silverback.Messaging.Outbound.TransactionalOutbox.Repositories.IOutboxWriter> and <xref:Silverback.Messaging.Outbound.TransactionalOutbox.Repositories.IOutboxReader> implementations. At the moment only a database table accessed using Entity Framework is supported as outbox, but a custom storage can be used implementing <xref:Silverback.Messaging.Outbound.TransactionalOutbox.Repositories.IOutboxWriter> and <xref:Silverback.Messaging.Outbound.TransactionalOutbox.Repositories.IOutboxReader>. public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .UseDbContext<MyDbContext>() .AddDbDistributedLockManager() .WithConnectionToMessageBroker(options => options .AddKafka() .AddOutbox<MyCustomOutboxWriter, MyCustomOutboxReader() .AddOutboxWorker()) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } Subscribing locally The published messages that are routed to an outbound endpoint cannot be subscribed locally (within the same process), unless explicitly desired. public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .AddDbDistributedLockManager() .WithConnectionToMessageBroker(options => options .AddKafka()) .AddEndpointsConfigurator<MyEndpointsConfigurator>() .PublishOutboundMessagesToInternalBus(); } } Note What said above is only partially true, as you can subscribe to the wrapped message (<xref:Silverback.Messaging.Messages.IOutboundEnvelope`1>) even without calling PublishOutboundMessagesToInternalBus . Producing the same message to multiple endpoints An outbound route can point to multiple endpoints resulting in a broadcast to all endpoints. Messages 1, 2 and 3 are published to both topics simultaneously. public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddOutbound<IIntegrationCommand>( new KafkaProducerEndpoint(\"topic-1\") { ... }, new KafkaProducerEndpoint(\"topic-2\") { ... })); } } A message will also be routed to all outbound endpoint mapped to a type compatible with the message type. In the example below an OrderCreatedMessage (that inherits from OrderMessage ) would be sent to both endpoints. public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddOutbound<OrderMessage>( new KafkaProducerEndpoint(\"topic-1\") { ... }) .AddOutbound<OrderCreatedMessage>( new KafkaProducerEndpoint(\"topic-2\") { ... })); } } Dynamic custom routing By default Silverback routes the messages according to their type and the static configuration defined at startup. In some cases you may need more flexibility, being able to apply your own routing rules. More information in the dedicated Outbound Messages Routing chapter. Samples All"
  },
  "concepts/broker/outbound-routing.html": {
    "href": "concepts/broker/outbound-routing.html",
    "title": "Outbound Messages Routing | Silverback",
    "keywords": "Outbound Messages Routing By default Silverback routes the messages according to their type and the static configuration defined at startup. In some cases you may need more flexibility, being able to apply your own routing rules. In such cases it is possible to either take advantage of the simple endpoint name resolvers or even implement a fully customized router. The messages are dynamically routed to the appropriate endpoint. Endpoint name resolver Using an endpoint name resolver is fairly simple and just requires a slightly different configuration in the <xref:Silverback.Messaging.IProducerEndpoint>. Here below a few examples of custom routing. Please refer to the <xref:Silverback.Messaging.KafkaProducerEndpoint>/<xref:Silverback.Messaging.Configuration.Kafka.IKafkaProducerEndpointBuilder> or <xref:Silverback.Messaging.MqttProducerEndpoint>/<xref:Silverback.Messaging.Configuration.Mqtt.IMqttProducerEndpointBuilder> API documentation for further information about all the possibilities. Fluent Legacy ProducerEndpointNameResolver public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) // Using a resolver function .AddOutbound<OrderCreatedEvent>(endpoint => endpoint .ProduceTo<OrderCreatedEvent>(envelope => { if (envelope.Message.IsPriority) return \"priority-orders\"; else return \"normal-orders\"; })) // Using format string and arguments function .AddOutbound<OrderCreatedEvent>(endpoint => endpoint .ProduceTo<OrderCreatedEvent>( \"orders-{0}\", envelope => { if (envelope.Message.IsPriority) return new[] { \"priority\" }; else return new[] { \"normal\" }; })) // Using a resolver class .AddOutbound<OrderCreatedEvent>(endpoint => endpoint .UseEndpointNameResolver<MyEndpointNameResolver>()) // Kafka only: using a partition resolver function .AddOutbound<InventoryUpdateMessage>(endpoint => endpoint .ProduceTo<InventoryUpdateMessage>( _ => \"topic1\", envelope => { switch (envelope.Message.Supplier) { case \"foo\": return 0; case \"bar\": return 1; case \"baz\": return 2; } })))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder // Using a resolver function .AddOutbound<OrderCreatedEvent>( new KafkaProducerEndpoint(envelope => { var message = (OrderCreatedEvent) envelope.Message; if (message.IsPriority) return \"priority-orders\"; else return \"normal-orders\"; }) { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" } }) // Using format string and arguments function .AddOutbound<OrderCreatedEvent>( new KafkaProducerEndpoint( \"orders-{0}\", envelope => { var message = (OrderCreatedEvent) envelope.Message; if (message.IsPriority) return new[] { \"priority\" }; else return new[] { \"normal\" }; }) { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" } }) // Using a resolver class .AddOutbound<OrderCreatedEvent>( new KafkaProducerEndpoint(typeof(MyEndpointNameResolver)) { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" } }) // Kafka only: using a partition resolver function .AddOutbound<InventoryUpdateMessage>( new KafkaProducerEndpoint( _ => \"topic1\", envelope => { var message = (InventoryUpdateMessage) envelope.Message; switch (message.Supplier) { case \"foo\": return 0; case \"bar\": return 1; case \"baz\": return 2; } }) { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" } }); } public class MyEndpointNameResolver : ProducerEndpointNameResolver<TestEventOne> { private readonly IMyService _service; public MyEndpointNameResolver(IMyService service) { _service = service; } protected override string GetName(IOutboundEnvelope<TestEventOne> envelope) { if (_service.IsPriorityOrder(envelope.Message.OrderNumber)) return \"priority-orders\"; else return \"normal-orders\"; } } Custom router In the following example a custom router is used to route the messages according to their priority (a copy is also sent to a catch-all topic). Startup EndpointsConfigurator public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka()) .AddEndpointsConfigurator<MyEndpointsConfigurator>(); } } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<IPrioritizedCommand>( (message, _, endpointsDictionary) => new [] { endpointsDictionary[message.Priority.ToString()], endpointsDictionary[\"all\"] }, new Dictionary<string, Action<IKafkaProducerEndpointBuilder>> { { \"low\", endpoint => endpoint.ProduceTo(\"low-priority\") }, { \"normal\", endpoint => endpoint.ProduceTo(\"normal-priority\") }, { \"high\", endpoint => endpoint.ProduceTo(\"high-priority\") }, { \"all\", endpoint => endpoint.ProduceTo(\"all\") } }); } Alternatively, an actual router class can also be created to encapsulate the routing logic. Startup EndpointsConfigurator Router public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka()) .AddEndpointsConfigurator<MyEndpointsConfigurator>() .AddSingletonOutboundRouter<PrioritizedRouter>(); } } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder.AddOutbound<IPrioritizedCommand, PrioritizedRouter>(); } public class PrioritizedRouter : OutboundRouter<IPrioritizedCommand> { private static readonly IProducerEndpoint HighPriorityEndpoint = new KafkaProducerEndpoint(\"high-priority\") { ... }; private static readonly IProducerEndpoint NormalPriorityEndpoint = new KafkaProducerEndpoint(\"normal-priority\") { ... }; private static readonly IProducerEndpoint LowPriorityEndpoint = new KafkaProducerEndpoint(\"low-priority\") { ... }; private static readonly IProducerEndpoint AllMessagesEndpoint = new KafkaProducerEndpoint(\"all\") { ... }; public override IEnumerable<IProducerEndpoint> Endpoints { get { yield return AllMessagesEndpoint; yield return LowPriorityEndpoint; yield return NormalPriorityEndpoint; yield return HighPriorityEndpoint; } } public override IEnumerable<IProducerEndpoint> GetDestinationEndpoints( IPrioritizedCommand message, MessageHeaderCollection headers) { yield return AllMessagesEndpoint; switch (message.Priority) { case MessagePriority.Low: yield return LowPriorityEndpoint; break; case MessagePriority.High: yield return HighPriorityEndpoint; break; default: yield return NormalPriorityEndpoint; break; } } }"
  },
  "concepts/broker/producer.html": {
    "href": "concepts/broker/producer.html",
    "title": "Producer | Silverback",
    "keywords": "Producer In some cases when high throughput is important you might want to skip the <xref:Silverback.Messaging.Publishing.IPublisher> and take advantage of the several options offered by the <xref:Silverback.Messaging.Broker.IProducer> interface. Producing pre serialized messages A pre-serialized message can be produced via the normal Produce / ProduceAsync or the RawProduce / RawProduceAsync methods. The difference is that the latter skip the entire Silverback behaviors pipeline (note that it means that no chunking or other features will kick in). Non-blocking overloads These are especially important for Kafka, since the underlying library is able to batch the outgoing messages for efficiency and that improves the throughput a lot. They will complete as soon as the message has been enqueued and invoke a callback when it is successfully produced (or when it fails / times out). These overloads exist for Produce , ProduceAsync , RawProduce and RawProduceAsync . (Note that ) public class ProducerService { private readonly IProducer _producer; private readonly ILogger _logger; public ProducerService( IBroker broker, ILogger<ProducerService> logger) { _producer = broker.GetProducer(\"some-topic\"); _logger = logger; } public async Task Produce(byte[] rawMessage) { for (int i = 0; i < 100000; i++) { _producer.RawProduce( rawMessage, null, () => _logger.LogInformation($\"Produced {i}\"), ex => _logger.LogError(ex, $\"Failed to produce {i}\"); } } } Note The non-async overload with callback functions is generally the fastest option with Kafka."
  },
  "concepts/broker/rabbit/routing-key.html": {
    "href": "concepts/broker/rabbit/routing-key.html",
    "title": "Routing Key | Silverback",
    "keywords": "Routing Key With RabbitMQ a routing key can be used to route the messages to a specific queue or filter the messages in a topic. See also the routing and topics tutorials on the official RabbitMQ web site. The messages are routed according to the routing key. Silverback offers a convenient way to specify the routing key, using the <xref:Silverback.Messaging.Messages.RabbitRoutingKeyAttribute>. public class MyMessage : IIntegrationMessage { [RabbitRoutingKey] public string Key { get; set; } ... }"
  },
  "concepts/broker/serialization.html": {
    "href": "concepts/broker/serialization.html",
    "title": "Serialization | Silverback",
    "keywords": "Serialization Being flexible when serializing and deserializing the messages sent over the message broker is crucial for interoperability and these mechanisms are therefore completely customizable. Default JSON serialization The default <xref:Silverback.Messaging.Serialization.JsonMessageSerializer> internally uses System.Text.Json to serialize the messages as JSON and encode them in UTF-8. A few headers are added to the message, in particular x-message-type is used by the <xref:Silverback.Messaging.Serialization.JsonMessageSerializer> to know the message type when deserializing it in the consumer, thus allowing messages of different types being sent over the same topic or queue. Warning The <xref:Silverback.Messaging.Serialization.JsonMessageSerializer> will obviously try to map the message to a type with the exact assembly qualified name found in the x-message-type header. It is therefore a good practice to share the message models among the services, maybe through a shared project or a nuget package. This is the suggested serialization strategy when both producer and consumer are based on Silverback but may not be ideal for interoperability. Have a look at the Message Headers section for an overview on the headers that are appended to the messages. Fixed-type JSON for interoperability If you are consuming a message coming from another system (not based on Silverback), chances are that the message type name is not being delivered as header. In that case you can resort to the typed <xref:Silverback.Messaging.Serialization.JsonMessageSerializer`1>. This serializer works like the default one seen in the previous chapter, but the message type is hard-coded, instead of being resolved according to the message header. Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<InventoryEvent>(endpoint => endpoint .ProduceTo(\"inventory-events\") .SerializeAsJson(serializer => serializer .UseFixedType<InventoryEvent>())) // Specifying the message type will automatically // switch to the JsonMessageSerializer<TMessage> // and deserialize the specified type without // needing the type header .AddInbound<OrderEvent>(endpoint => endpoint .ConsumeFrom(\"order-events\") .Configure(config => { config.GroupId = \"my-consumer\"; })) // The following configurations is equivalent to the // previous one, but more verbose .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .DeserializeJson(serializer => serializer .UseFixedType<OrderEvent>()))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<InventoryEvent>( new KafkaProducerEndpoint(\"inventory-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, Serializer = new JsonMessageSerializer<InventoryEvent>() }) .AddInbound( new KafkaConsumerEndpoint(\"order-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" }, Serializer = new JsonMessageSerializer<OrderEvent>() }); } JSON using Newtonsoft.Json Prior to release 3.0.0 the default <xref:Silverback.Messaging.Serialization.JsonMessageSerializer> was based on Newtonsoft.Json instead of System.Text.Json . For backward compatibility reasons and since System.Text.Json may not support all use cases covered by Newtonsoft.Json , the old serializers have been renamed to <xref:Silverback.Messaging.Serialization.NewtonsoftJsonMessageSerializer> and <xref:Silverback.Messaging.Serialization.NewtonsoftJsonMessageSerializer`1> and moved into the dedicated Silverback.Integration.Newtonsoft package. Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<InventoryEvent>(endpoint => endpoint .ProduceTo(\"inventory-events\") .SerializeAsJsonUsingNewtonsoft()) .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .DeserializeJsonUsingNewtonsoft()) // Specifying the message type will automatically // switch to the NewtonsoftJsonMessageSerializer<TMessage> .AddInbound<DeliveryNotification>(endpoint => endpoint .ConsumeFrom(\"delivery-notification-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .DeserializeJsonUsingNewtonsoft()) ); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<InventoryEvent>( new KafkaProducerEndpoint(\"inventory-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, Serializer = new NewtonsoftJsonMessageSerializer() }) .AddInbound( new KafkaConsumerEndpoint(\"order-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" }, Serializer = new NewtonsoftJsonMessageSerializer() }); } Apache Avro The <xref:Silverback.Messaging.Serialization.AvroMessageSerializer`1> contained in the Silverback.Integration.Kafka.SchemaRegistry package can be used to connect with a schema registry and exchange messages in Apache Avro format. Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<InventoryEvent>(endpoint => endpoint .ProduceTo(\"inventory-events\") .SerializeAsAvro(serializer => serializer .UseType<InventoryEvent>() .Configure( schemaRegistryConfig => { schemaRegistryConfig.Url = \"localhost:8081\"; }, serializerConfig => { serializerConfig.AutoRegisterSchemas = true; }))) .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .DeserializeAvro(serializer => serializer .UseType<OrderEvent>() .Configure( schemaRegistryConfig => { schemaRegistryConfig.Url = \"localhost:8081\"; }, serializerConfig => { serializerConfig.AutoRegisterSchemas = true; })))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<InventoryEvent>( new KafkaProducerEndpoint(\"inventory-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, Serializer = new AvroMessageSerializer<InventoryEvent> { SchemaRegistryConfig = new SchemaRegistryConfig { Url = \"localhost:8081\" }, AvroSerializerConfig = new AvroSerializerConfig { AutoRegisterSchemas = true } } }) .AddInbound( new KafkaConsumerEndpoint(\"order-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" }, Serializer = new AvroMessageSerializer<OrderEvent> { SchemaRegistryConfig = new SchemaRegistryConfig { Url = \"localhost:8081\" }, AvroSerializerConfig = new AvroSerializerConfig { AutoRegisterSchemas = true } } }); } Note The C# message models can be generated from an Avro schema using AvroGen . Note This serializer is built for Kafka but it could work with other brokers, as long as a schema registry is available. Custom serializer In some cases you may want to build your very own custom serializer implementing <xref:Silverback.Messaging.Serialization.IMessageSerializer> directly. Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<InventoryEvent>(endpoint => endpoint .ProduceTo(\"inventory-events\") .SerializeUsing(new MyCustomSerializer())) .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .DeserializeUsing(new MyCustomSerializer()))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<InventoryEvent>( new KafkaProducerEndpoint(\"inventory-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, Serializer = new MyCustomSerialzer() }) .AddInbound( new KafkaConsumerEndpoint(\"order-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" }, Serializer = new MyCustomSerialzer() }); } Note You may need to implement IKafkaMessageSerializer if you want to have full control over the serialization of the Kafka key as well. Binary Files Please refer to the Binary Files page if you need to produce or consume raw binary files."
  },
  "concepts/broker/streaming.html": {
    "href": "concepts/broker/streaming.html",
    "title": "Streaming | Silverback",
    "keywords": "Streaming The <xref:Silverback.Messaging.Messages.IMessageStreamEnumerable`1> can be used to consume an endpoint in a streaming fashion and it is the only way to consume sequences (see for example batch processing ). This stream will be forwarded to the subscribed method as soon as the first message is consumed and it is then asynchronously pushed with the next messages. <xref:Silverback.Messaging.Messages.IMessageStreamEnumerable`1> implements both IEnumerable and IAsyncEnumerable and the subscriber method can either declare an <xref:Silverback.Messaging.Messages.IMessageStreamEnumerable`1>, an IEnumerable or an IAsyncEnumerable as argument. Since the asynchronous and I/O bound nature of this stream it is recommended to take advantage of the IAsyncEnumerable capabilities to asynchronously loop through the messages. public class StreamSubscriber { public async Task OnOrderStreamReceived( IAsyncEnumerable<OrderEvent> eventsStream) { await foreach(var orderEvent in eventsStream) { // ...process the event... } } } A single instance of <xref:Silverback.Messaging.Messages.IMessageStreamEnumerable`1> is created and published per each queue/topic/partition and the messages are acknowledged (committed) after a single iteration completes, unless sequencing (e.g. batch processing ) is configured or a sequence is automatically recognized by Silverback (e.g. a dataset). In that case an instance is published per each sequence and the entire sequence is atomically committed. Rx (Observable) The Silverback.Core.Rx package adds the <xref:Silverback.Messaging.Messages.IMessageStreamObservable`1> that works like the <xref:Silverback.Messaging.Messages.IMessageStreamEnumerable`1> but implements IObservable enabling the usage of Rx.NET . Startup Subscriber public class Startup { public void ConfigureServices(IServiceCollection services) { services.AddSilverback().AsObservable(); } } public class StreamSubscriber { public async Task OnOrderStreamReceived( IObservable<OrderEvent> eventsStream) { stream.Subscribe(...); } } Notes, suggestions and insights The stream will be pushed with messages as they are read from the message broker. Since the I/O bound nature of the operation you should obviously prefer to subscribe to an IAsyncEnumerable instead of an IEnumerable and in any case loop asynchronously ( await foreach or similar approach). If the sequence is interrupted because the application is disconnecting or an error occurred in another subscriber, the IEnumerator will throw an OperationCancelledException . Handle it if you need to gracefully abort or cleanup. Throwing an exception while enumerating a sequence (e.g. a BatchSequence ) will cause it to be aborted and handled according to the defined error policies . If you just break the iteration and the subscriber return, the operation will be considered successful instead and the sequence will be committed."
  },
  "concepts/broker/testing.html": {
    "href": "concepts/broker/testing.html",
    "title": "Testing | Silverback",
    "keywords": "Testing Silverback ships a mocked version of the message broker implementations on a different nuget package: Silverback.Integration.Kafka.Testing (coming soon) Silverback.Integration.RabbitMQ.Testing These packages allow to perform end-to-end tests without having to integrate with a real message broker. Unit Tests Here an example of an xUnit test built using Silverback.Integration.Kafka.Testing . public class KafkaTests { private readonly IServiceProvider _serviceProvider; // Configure DI during setup public InMemoryBrokerTests() { var services = new ServiceCollection(); // Loggers are a prerequisite services.AddSingleton<ILoggerFactory, NullLoggerFactory>(); services.AddSingleton(typeof(ILogger<>), typeof(NullLogger<>)); services // Register Silverback as usual .AddSilverback() // Register the mocked KafkaBroker .WithConnectionToMessageBroker(config => config.AddMockedKafka()) // Configure inbound and outbound endpoints .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://tests\"; }) .AddOutbound<InventoryEvent>(endpoint => endpoint .ProduceTo(\"test-topic\")) .AddInbound(endpoint => endpoint .ConsumeFrom(\"test-topic\") .Configure(config => { config.GroupId = \"my-test-consumer\"; }))) // Register the subscriber under test .AddScopedSubscriber<MySubscriber>(); // ...register all other types you need... _serviceProvider = services.BuildServiceProvider(); } [Fact] public async Task SampleTest() { // Arrange // Connect the broker await _serviceProvider.GetRequiredService<IBroker>().ConnectAsync(); // Create a producer to push to test-topic var producer = _serviceProvider .GetRequiredService<IBroker>() .GetProducer(new KafkaProducerEndpoint(\"test-topic\")); // Act await producer.ProduceAsync(new TestMessage { Content = \"hello!\" }); await producer.ProduceAsync(new TestMessage { Content = \"hello 2!\" }); // Assert // ...your assertions... } } Integration Tests Mocking the message broker is especially interesting for the integration tests, where you probably leverage the ASP.NET Core integration tests to perform a full test based on the real configuration applied in the application's startup class. The following code shows the simplest integration test possible, in which an object is published to the broker and e.g. a subscriber is called. public class IntegrationTests : IClassFixture<WebApplicationFactory<Startup>> { private readonly WebApplicationFactory<Startup> _factory; public IntegrationTests(WebApplicationFactory<Startup> factory) { _factory = factory.WithWebHostBuilder(builder => { builder.ConfigureTestServices(services => { // Replace the usual broker (KafkaBroker) // with the mocked version services.UseMockedKafka(); }); }); } [Fact] public async Task SampleTest() { // Arrange // Resolve a producer to push to test-topic var producer = _factory.Server.Host.Services .GetRequiredService<IBroker>() .GetProducer(new KafkaProducerEndpoint(\"tst-topic\")); // Act await producer.ProduceAsync(new TestMessage { Content = \"abc\" }); // Assert // ...your assertions... } } Testing helper The testing helpers (such has <xref:Silverback.Testing.IKafkaTestingHelper>) contain some methods that simplify testing with the message broker, given it's asynchronous nature. public class IntegrationTests : IClassFixture<WebApplicationFactory<Startup>> { private readonly WebApplicationFactory<Startup> _factory; public IntegrationTests(WebApplicationFactory<Startup> factory) { _factory = factory.WithWebHostBuilder(builder => { builder.ConfigureTestServices(services => { services.UseMockedKafka(); }); }); } [Fact] public async Task SampleTest() { // Arrange // Resolve the IKafkaTestingHelper (used below) var testingHelper = _factory.Server.Host.Services .GetRequiredService<IKafkaTestingHelper>(); // Resolve a producer to push to test-topic var producer = testingHelper.Broker .GetProducer(new KafkaProducerEndpoint(\"tst-topic\")); // Act await producer.ProduceAsync(new TestMessage { Content = \"abc\" }); // Wait until all messages have been consumed and // committed before asserting await testingHelper.WaitUntilAllMessagesAreConsumedAsync(); // Assert // ...your assertions... } } IntegrationSpy The <xref:Silverback.Testing.IIntegrationSpy> ships with the Silverback.Integration.Testing package (referenced by the other Integration.Testing.* packages) and can be used to inspect all outbound and inbound messages. public class IntegrationTests : IClassFixture<WebApplicationFactory<Startup>> { private readonly WebApplicationFactory<Startup> _factory; public IntegrationTests(WebApplicationFactory<Startup> factory) { _factory = factory.WithWebHostBuilder(builder => { builder.ConfigureTestServices(services => { services .ConfigureSilverback() .UseMockedKafka() .AddIntegrationSpy(); }); }); } [Fact] public async Task SampleTest() { // Arrange var testingHelper = _factory.Server.Host.Services .GetRequiredService<IKafkaTestingHelper>(); var producer = testingHelper.Broker .GetProducer(new KafkaProducerEndpoint(\"tst-topic\")); // Act await producer.ProduceAsync(new TestMessage { Content = \"abc\" }); // Wait until all messages have been consumed and // committed before asserting await testingHelper.WaitUntilAllMessagesAreConsumedAsync(); // Assert testingHelper.Spy.OutboundEnvelopes.Should().HaveCount(1); testingHelper.Spy.InboundEnvelopes.Should().HaveCount(1); testingHelper.Spy.InboundEnvelopes[0].Message.As<TestMessage> .Content.Should().Be(\"abc\"); } } Mocked Kafka Many aspects of the Kafka broker have been mocked to replicated as much as possible the behavior you have when connected with the real broker. This new implementation supports commits, kafka events, offset reset, partitioning, rebalance, etc. The mocked topics can be retrieved and inspected via the GetTopic method of the <xref:Silverback.Testing.IKafkaTestingHelper>. Partitioning By default 5 partitions will be created per each topic being mocked. This number can be configured as shown in the following snippet. The setting is per broker and there's currently no way to configure each topic independently. public class IntegrationTests : IClassFixture<WebApplicationFactory<Startup>> { private readonly WebApplicationFactory<Startup> _factory; public IntegrationTests(WebApplicationFactory<Startup> factory) { _factory = factory.WithWebHostBuilder(builder => { builder.ConfigureTestServices(services => { services .UseMockedKafka(options => options .WithDefaultPartitionsCount(10)); }); }); } }"
  },
  "concepts/broker/tombstone.html": {
    "href": "concepts/broker/tombstone.html",
    "title": "Tombstone Message | Silverback",
    "keywords": "Tombstone Message A tombstone message is a message with a null body, used to indicate that the record has been deleted. This technique is for example used with Kafka topics compaction, to get rid of obsolete records. Consumer Silverback maps by default the messages with a null body to a <xref:Silverback.Messaging.Messages.Tombstone> or <xref:Silverback.Messaging.Messages.Tombstone`1>. This behavior can be changed setting using the SkipNullMessages or UseLegacyNullMessageHandling of the <xref:Silverback.Messaging.Configuration.IConsumerEndpointBuilder`1>, or setting the NullMessageHandlingStrategy property of the <xref:Silverback.Messaging.ConsumerEndpoint>). The <xref:Silverback.Messaging.Messages.Tombstone>/<xref:Silverback.Messaging.Messages.Tombstone`1> message exposes a single property containing the message identifier . EndpointConfigurator Subscriber public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddInbound(endpoint => endpoint .ConsumeFrom(\"catalog-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .DeserializeJson(serializer => serializer .UseFixedType<Product>()))); } public class MySubscriber { public async Task OnProductDeleted(Tombstone<Product> tombstone) { // TODO: use tombstone.MessageId to remove the product // from the local database } } Important In order to create a typed <xref:Silverback.Messaging.Messages.Tombstone`1> it is required that either the consumed message declares the x-message-type header or a fixed type deserializer is used (as shown in the example above). Otherwise the null message will be mapped to a simple <xref:Silverback.Messaging.Messages.Tombstone>. Producer A <xref:Silverback.Messaging.Messages.Tombstone`1> (or <xref:Silverback.Messaging.Messages.Tombstone>) can also be used to produce a null message. EndpointConfigurator Publisher public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<Product>(endpoint => endpoint .ProduceTo(\"catalog-events\"))); } public class MyService { private readonly IPublisher _publisher; public MyService(IPublisher publisher) { _publisher = publisher; } public async Task DeleteProduct(string productId) { ... await _publisher.PublishAsync(new Tombstone<Product>(productId)); } } Note The <xref:Silverback.Messaging.Messages.Tombstone`1> messages are routed according to the type parameter TMessage . This means that they will be published to the outbound endpoints papped to the same TMessage ( Product in the above example), as well as to the outbound endpoints explicitly mapping <xref:Silverback.Messaging.Messages.Tombstone>."
  },
  "concepts/broker/translating-messages.html": {
    "href": "concepts/broker/translating-messages.html",
    "title": "Translating Messages | Silverback",
    "keywords": "Translating Messages It is not uncommon to be willing to slightly transform the internal message and maybe publish only a subset of the information to the message broker (e.g. you may not want to export the full entity related to the domain event). You can easily achieve this with a subscriber that just maps/translates the messages. public class MapperService { public IMessage MapCheckoutEvent(CheckoutDomainEvent message) => new CheckoutIntegrationEvent { UserId = message.Source.UserId, Total = mesage.Source.Total, ... }; } As explained in the Publishing section, when the subscriber returns one or more messages those are automatically republished to the bus. The message is then relayed to the message broker, if its type matches with an outbound endpoint declaration."
  },
  "concepts/broker/validation.html": {
    "href": "concepts/broker/validation.html",
    "title": "Message Validation | Silverback",
    "keywords": "Message Validation Both the consumed and produced messages are being validated using the same mechanism implemented in the asp.net controllers. You can either decorate the message model with the System.ComponentModel.DataAnnotations.ValidationAttribute standard implementations, create your own attributes (extending ValidationAttribute ) or otherwise you can implement the IValidatableObject interface in your message model. Configuration The <xref:Silverback.Messaging.Validation.MessageValidationMode> can be configured per endpoint and has 3 values: LogWarning (default): a warning is logged if the message is not valid ThrowException : an exception is thrown if the message is not valid None : the validation is completely disabled Note If an invalid message is produced, the <xref:Silverback.Messaging.Validation.MessageValidationException> will be rethrown by the Produce / Publish method. In the consumer it will instead be handled like any other exception, according to the configured policies, or leading to the consumer being stopped. Warning The validation might have a - relatively speaking - big impact on the performance, depending on the object size, the number of validations to be performed and their complexity. You might want to consider disabling the validation, if performance is a critical concern in your use case. Fluent Legacy public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddKafkaEndpoints(endpoints => endpoints .Configure(config => { config.BootstrapServers = \"PLAINTEXT://kafka:9092\"; }) .AddOutbound<InventoryEvent>(endpoint => endpoint .ProduceTo(\"inventory-events\") .DisableMessageValidation() .AddInbound(endpoint => endpoint .ConsumeFrom(\"order-events\") .Configure(config => { config.GroupId = \"my-consumer\"; }) .ValidateMessage(throwException: true))); } public class MyEndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) => builder .AddOutbound<InventoryEvent>( new KafkaProducerEndpoint(\"inventory-events\") { Configuration = new KafkaProducerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\" }, MessageValidationMode = MessageValidationMode.None }) .AddInbound( new KafkaConsumerEndpoint(\"order-events\") { Configuration = new KafkaConsumerConfig { BootstrapServers = \"PLAINTEXT://kafka:9092\", GroupId = \"my-consumer\" }, MessageValidationMode = MessageValidationMode.ThrowException }); } Validated models examples Using annotations public class CreateUserCommand { [Required] [MinLength(3)] [MaxLength(100)] public string Username { get; set; } } Implementing IValidatableObject public class CreateUserCommand : IValidatableObject { public string Username { get; set; } public IEnumerable<ValidationResult> Validate( ValidationContext validationContext) { if (Username == null || Username.Length < 3 || Username.Length > 100) { yield return new ValidationResult( \"Invalid username.\", new[] { nameof(Username) }); } } }"
  },
  "concepts/bus/behaviors.html": {
    "href": "concepts/bus/behaviors.html",
    "title": "Behaviors | Silverback",
    "keywords": "Behaviors The behaviors can be used to build a custom pipeline (similar to the asp.net pipeline), easily adding your cross-cutting concerns such as logging, validation, etc. The behaviors are implementations of the <xref:Silverback.Messaging.Publishing.IBehavior> interface and will be invoked by the <xref:Silverback.Messaging.Publishing.IPublisher> every time a message is published to the internal bus (this includes the inbound/outbound messages, but they will be wrapped into an <xref:Silverback.Messaging.Messages.IInboundEnvelope`1> or <xref:Silverback.Messaging.Messages.IOutboundEnvelope`1>). The HandleAsync method of each registered behavior is called every time a message (or a batch of messages) is published to the internal bus, passing in the collection of messages and the delegate to the next step in the pipeline. This gives you the flexibility to execute any sort of code before and after the messages have been actually published (before or after calling the next step). You can for example modify the messages before publishing them, validate them (like in the above example), add some logging / tracing, etc. The <xref:Silverback.Messaging.Publishing.IBehavior> implementation have simply to be registered for DI. IBehavior example The following example demonstrates how to use a behavior to trace the messages. Behavior Startup public class TracingBehavior : IBehavior { private readonly ITracer _tracer; public TracingBehavior(ITracer tracer) { _tracer = tracer; } public async Task<IReadOnlyCollection<object?>> HandleAsync( object message, MessageHandler next) { tracer.TraceProcessing(message); var result = await next(message); tracer.TraceProcessed(message); return result; } } public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .AddScopedBehavior<TracingBehavior>(); } } Note <xref:Silverback.Messaging.Messages.IInboundEnvelope> and <xref:Silverback.Messaging.Messages.IOutboundEnvelope> are internally used by Silverback to wrap the messages being sent to or received from the message broker and will be received by the <xref:Silverback.Messaging.Broker.IBroker>. Those interfaces contains the message plus the additional data like endpoint, headers, offset, etc. Sorting The order in which the behaviors are executed might matter and it is possible to precisely define it implementing the <xref:Silverback.ISorted> interface. public class SortedBehavior : IBehavior, ISorted { public int SortIndex => 120; public Task<IReadOnlyCollection<object?>> HandleAsync( object message, MessageHandler next) { // ...your logic... return next(message); } } See also Broker behaviors pipeline"
  },
  "concepts/bus/enabling.html": {
    "href": "concepts/bus/enabling.html",
    "title": "Enabling Silverback | Silverback",
    "keywords": "Enabling Silverback Enabling the bus Silverback's main component is the internal in-memory message bus and pretty much all other features are built on top of that. The first mandatory step to start using Silverback is to register the core services (internal bus) with the .net core dependency injection. public class Startup { public void ConfigureServices(IServiceCollection services) { services.AddSilverback(); } } Configuring Silverback The AddSilverback method highlighted in the previous chapter returns an <xref:Silverback.Messaging.Configuration.ISilverbackBuilder> that exposes all the methods needed to configure Silverback and wire everything up. The several configuration options will are exhaustively presented in each dedicated section of this documentation but here is a basic sample startup. public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka()) .AddEndpointsConfigurator<OrdersEndpointsConfigurator>() .AddEndpointsConfigurator<ProductsEndpointsConfigurator>() .AddScopedSubscriber<OrderEventsSubscriber>() .AddScopedSubscriber<ProductEventsSubscriber>(); } } Note that AddSilverback should be called only once but you can use the ConfigureSilverback extension method on the IServiceCollection to retrieve the <xref:Silverback.Messaging.Configuration.ISilverbackBuilder> instance once again. Startup Orders Feature Products Feature public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithConnectionToMessageBroker(options => options .AddKafka()); services .AddOrdersFeature() .AddProductsFeature(); } } public static class OrdersFeatureConfigurator { public static void AddOrdersFeature(this IServiceCollection services) { services .ConfigureSilverback() .AddEndpointsConfigurator<OrdersEndpointsConfigurator>() .AddScopedSubscriber<OrderEventsSubscriber>(); } } public static class ProductsFeatureConfigurator { public static void AddProductsFeature(this IServiceCollection services) { services .ConfigureSilverback() .AddEndpointsConfigurator<ProductsEndpointsConfigurator>() .AddScopedSubscriber<ProductEventsSubscriber>(); } }"
  },
  "concepts/bus/model.html": {
    "href": "concepts/bus/model.html",
    "title": "Creating the Message model | Silverback",
    "keywords": "Creating the Message model Basics First of all we need to create a message class. The message class can be any POCO class, it just need to be serializable. public class SampleMessage { public string Content { get; set; } } It is very much suggested to consider using the Silverback.Core.Model package (documented in the next chapter) to better organize your message and write better and more readable code. Silverback.Core.Model A hierarchy of interfaces is available in Silverback.Core.Model to help specify the meaning of each message and produce in better, cleaner and more readable code. The internal messages are being sent through the internal in-memory bus and don't leave the service scope, while the integration messages are those messages exchanged between different microservices, through a message broker like Apache Kafka or RabbitMQ. Event though strongly suggested, it's not mandatory to use the proposed hierarchy from Silverback.Core.Model and everything can be achieved using POCO classes as messages and using the generic <xref:Silverback.Messaging.Publishing.IPublisher> to publish them. In the following chapters you will find an overview of the different message types and their meaning but first of all we need to reference the Silverback.Core.Model package and register it with the dependency injection. public class Startup { public void ConfigureServices(IServiceCollection services) { services.AddSilverback().UseModel(); } } Internal Messages This messages can be used internally to the microservice bus but cannot be relayed to the message broker. See Translating Messages for a convenient way to map the internal message to an <xref:Silverback.Messaging.Messages.IIntegrationMessage>. Events <xref:Silverback.Messaging.Messages.IEvent> is to be used to notify thing that happened inside a service and may be of some interest for one or more other service. The events are a fire-and-forget message type and no response is expected. The <xref:Silverback.Domain.IDomainEvent> and <xref:Silverback.Domain.IDomainEvent`1> extend <xref:Silverback.Messaging.Messages.IEvent> and the types implementing this interface are usually published only from within the domain entities (actually adding them to the internal collection and letting them be published during the save changes transaction). See also DDD and Domain Events . Commands <xref:Silverback.Messaging.Messages.ICommand> or <xref:Silverback.Messaging.Messages.ICommand`1> are used to trigger an action in another service or component and are therefore very specific and usually consumed by one single subscriber. This messages can return a value (of type TResult). Queries <xref:Silverback.Messaging.Messages.IQuery`1> works exactly like <xref:Silverback.Messaging.Messages.ICommand`1>. This messages are obviously always returning something since they represent a request for data (query). Integration messages The <xref:Silverback.Messaging.Messages.IIntegrationMessage> interface identifies those messages that are either published to the message broker or received through it. Note that <xref:Silverback.Messaging.Messages.IIntegrationMessage> implements <xref:Silverback.Messaging.Messages.IMessage>, obviously. Integration Event <xref:Silverback.Messaging.Messages.IIntegrationEvent> can be used to export events to other microservices or, more generally, other applications. <xref:Silverback.Messaging.Publishing.IEventPublisher> can be used to publish these events and they will automatically be routed to the message broker if an outbound connector was properly configured. See Connecting to a Message Broker for details. Integration Command <xref:Silverback.Messaging.Messages.IIntegrationCommand> is used to trigger an action on another microservices (or application). <xref:Silverback.Messaging.Publishing.ICommandPublisher> can be used to publish these messages and they will automatically be routed to the message broker if an outbound connector was properly configured. See Connecting to a Message Broker for details."
  },
  "concepts/bus/publish.html": {
    "href": "concepts/bus/publish.html",
    "title": "Publishing | Silverback",
    "keywords": "Publishing Basic Publishing To publish the message you just need an instance of <xref:Silverback.Messaging.Publishing.IPublisher> (or derived interfaces if using Silverback.Core.Model , as shown later on). using Silverback.Messaging.Publishing; public class PublishingService { private readonly IPublisher _publisher; public PublishingService(IPublisher publisher) { _publisher = publisher; } public async Task PublishSomething() { await _publisher.PublishAsync(new SampleMessage { Content = \"whatever\" }); } } The publisher always exposes a synchronous and an asynchronous version of each method. The second option is of course to be preferred to take advantage of non-blocking async/await. Return values In some cases you will of course return a response after having processed the message. public async Task<Report> PublishSomething() { var result = await _publisher.PublishAsync(new ReportQuery() { ... }); return result.Single(); } Important Please note the required call to Single() , because Silverback allows you to have multiple subscribers for the same message and therefore collect multiple return values. This is not needed if using <xref:Silverback.Messaging.Publishing.IQueryPublisher> or <xref:Silverback.Messaging.Publishing.ICommandPublisher> described in the Creating the Message model page. Silverback.Core.Model Silverback.Core.Model has been introduced in the previous page Creating the Message model . Each message type (<xref:Silverback.Messaging.Messages.IEvent>, <xref:Silverback.Messaging.Messages.ICommand>/<xref:Silverback.Messaging.Messages.ICommand`1> and <xref:Silverback.Messaging.Messages.IQuery`1>) also comes with its specialized <xref:Silverback.Messaging.Publishing.IPublisher> as quickly shown in the following sub-chapters. Events The messages implementing <xref:Silverback.Messaging.Messages.IEvent>, <xref:Silverback.Domain.IDomainEvent> or <xref:Silverback.Messaging.Messages.IIntegrationEvent> can be published using an <xref:Silverback.Messaging.Publishing.IEventPublisher>. using Silverback.Messaging.Publishing; public class PublishingService { private readonly IEventPublisher _publisher; public PublishingService(IEventPublisher publisher) { _publisher = publisher; } public async Task PublishEvent() { var myEvent = new MyEvent() { ... }; await _publisher.PublishAsync(myEvent); } } Commands The messages that implement <xref:Silverback.Messaging.Messages.ICommand>, <xref:Silverback.Messaging.Messages.ICommand`1> or <xref:Silverback.Messaging.Messages.IIntegrationCommand> can be published using an <xref:Silverback.Messaging.Publishing.ICommandPublisher>. Without result With result using Silverback.Messaging.Publishing; public class PublishingService { private readonly ICommandPublisher _publisher; public PublishingService(ICommandPublisher publisher) { _publisher = publisher; } public async Task ExecuteCommand() { var command = new MyCommand() { ... }; await _publisher.ExecuteAsync(command); } } using Silverback.Messaging.Publishing; public class PublishingService { private readonly ICommandPublisher _publisher; public PublishingService(ICommandPublisher publisher) { _publisher = publisher; } public async Task<MyResult> ExecuteCommand() { var command = new MyCommand() { ... }; var result = await _publisher.ExecuteAsync(command); return result; } } Queries The <xref:Silverback.Messaging.Publishing.IQueryPublisher> ca be used to publish the messages implementing the <xref:Silverback.Messaging.Messages.IQuery`1> interface. using Silverback.Messaging.Publishing; public class PublishingService { private readonly IQueryPublisher _publisher; public PublishingService(IQueryPublisher publisher) { _publisher = publisher; } public async Task<MyResult> GetResults() { var query = new MyQuery() { ... }; var result = await _publisher.ExecuteAsync(myQuery); return result; } }"
  },
  "concepts/bus/subscribe.html": {
    "href": "concepts/bus/subscribe.html",
    "title": "Subscribing | Silverback",
    "keywords": "Subscribing Now all is left to do is write a subscriber method to process the published messages. Introduction The subscription in the Silverback internal bus is based on the message type. This means that when a message is published Silverback will simply evaluate the signature of the subscribed methods and invoke the ones that accept a message of that specific type, a base type or an implemented interface. For example, given the following message structure: public abstract class OrderEvent : IEvent { ... } public class OrderCreatedEvent : OrderEvent { ... } All these subscriber methods will be invoked to handle an instance of OrderCreatedEvent : void Handle(OrderCreatedEvent message) void Handle(OrderMessage message) void Handle(IEvent message) Note It is perfectly fine to have multiple subscribers handling the same message but you have to be aware that all them will share the same DI scope (and thus the same DbContext instance). Subscriber class The default and usually preferred way to subscribe is to implement the message handling logic into a subscriber class. Such class can declare one or more public message handler methods that are automatically subscribed. All subscribers must be registered with the service provider as shown in the following example. Subscriber Startup public class SubscribingService { public async Task OnMessageReceived(SampleMessage message) { // TODO: Process message } public async Task OnOtherMessageReceived(OtherSampleMessage message) { // TODO: Process message } } public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .AddScopedSubscriber<SubscribingService>(); } } Subscription options All public methods are automatically subscribed by default but the <xref:Silverback.Messaging.Subscribers.SubscribeAttribute> can be used to decorate the non-public methods and subscribe them as well. The <xref:Silverback.Messaging.Subscribers.SubscribeAttribute> can also be used to customize the subscription options, see the attribute properties for details. It is also possible to disable the automatic subscription of the public methods. public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .AddScopedSubscriber<SubscribingService>( autoSubscribeAllPublicMethods: false) } } Delegate based subscription In some cases you may prefer to subscribe a method delegate (or an inline lambda) directly using the AddDelegateSubscriber method. public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .AddDelegateSubscriber((SampleMessage message) => { // TODO: Process messages }); } } Supported methods and parameters The subscribed method can either be synchronous or asynchronous (returning a Task ). The first parameter must be the message and the parameter type can be the specific message, a base class or an implemented interface. Furthermore, when consuming from a message broker, it is possible to subscribe to the message stream and asynchronously enumerate through the messages, as shown in the Streaming chapter. The method can have other parameters that will be resolved using the service provider. Most useful to integrate existing code subscribing via a delegate. Delegate Subscriber public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .AddDelegateSubscriber( (BasketCheckoutMessage message, CheckoutService service) => { service.Checkout(message.BaksetId, message.UserId) }); } } public class SubscribingService { public async Task OnMessageReceived(BasketCheckoutMessage message, CheckoutService service) { service.Checkout(message.BaksetId, message.UserId) } } Return values A subscriber can also have a return value that can be collected by the publisher. public class SubscribingService { public async Task<SampleResult> OnMessageReceived(SampleMessage message) { ... return new SampleResult(...); } } Return new messages (republishing) A subscribed method can also optionally return a message or a collection of messages (either IEnumerable , IReadOnlyCollection or IObservable , if using Silverback.Core.Rx ) that will be automatically republished to the internal bus. Single Multiple public class SubscribingService { public async Task<OtherSampleMessage> OnMessageReceived(SampleMessage message) { ... return new OtherSampleMessage { ... }; } } public class SubscribingService { public IEnumerable<IMessage> OnMessageReceived(IEnumerable<SampleMessage> messages) => messages.SelectMany(message => { yield return new OtherSampleMessage1 { ... }; yield return new OtherSampleMessage2 { ... }; }); } Silverback recognizes per default only the messages implementing <xref:Silverback.Messaging.Messages.IMessage> but you can register your own types (you can register base types and interfaces as well). Startup Subscriber Message Model public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .HandleMessagesOfType<ICustomMessage>(); } } public class SubscribingService { public async Task<CustomSampleMessage> OnMessageReceived(SampleMessage message) { ... return new CustomSampleMessage { ... }; } } public class CustomSampleMessage : ICustomMessage { public string SomeProperty { get; set; } }"
  },
  "concepts/dbcontext.html": {
    "href": "concepts/dbcontext.html",
    "title": "Sample DbContext (EF Core) | Silverback",
    "keywords": "Sample DbContext (EF Core) Default Tables Some features rely on data being stored in a persistent storage such as a database. This chapter highlights the DbSet 's that have to be added to your DbContext when using Silverback in combination with EF Core (via the Silverback.Core.EntityFrameworkCore ). Here a breakdown of the use cases that require a DbSet : Using an outbox table (see Outbound Endpoint ) will require a DbSet<OutboundMessage> and possibly a DbSet<Lock> , to enable horizontal scaling. Either a DbSet<StoredOffset> or a DbSet<InboundMessage> is necessary to ensure exactly-once processing (see Inbound Endpoint ). When consuming chunked messages (see Chunking ), you may want to temporary store the received chunks into a database table, until all chunks are received and the full message can be rebuilt and processed and you therefore need a DbSet<TemporaryMessageChunk> to be configured. This is what a DbContext built to support all the aforementioned features will look like. using Microsoft.EntityFrameworkCore; using Silverback.Database.Model; using Silverback.EntityFrameworkCore; namespace Sample { public class SampleDbContext : DbContext { public SampleDbContext(DbContextOptions options) : base(options) { } public DbSet<OutboxMessage> Outbox { get; set; } = null!; public DbSet<InboundLogEntry> InboundMessages { get; set; } = null!; public DbSet<StoredOffset> StoredOffsets { get; set; } = null!; public DbSet<Lock> Locks { get; set; } = null!; protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<InboundLogEntry>() .HasKey(t => new { t.MessageId, t.ConsumerGroupName }); } } } Warning <xref:Silverback.Database.Model.InboundLogEntry> declare a composite primary key via annotation, thing that isn't supported yet by EF Core. It is therefore mandatory to explicitly redeclare their primary key via the HasKey fluent API. DDD and Transactional Messages Some additional changes are required in order for the events generated by the domain entities to be fired as part of the SaveChanges transaction. More details on this topic can be found in the DDD and Domain Events section. using Microsoft.EntityFrameworkCore; using Silverback.EntityFrameworkCore; using Silverback.Messaging.Publishing; namespace Sample { public class SampleDbContext : DbContext { private readonly DbContextEventsPublisher _eventsPublisher; public SampleDbContext(IPublisher publisher) { _eventsPublisher = new DbContextEventsPublisher(publisher, this); } public SampleDbContext(DbContextOptions options, IPublisher publisher) : base(options) { _eventsPublisher = new DbContextEventsPublisher(publisher, this); } // ...DbSet properties and OnModelCreating... public override int SaveChanges() => SaveChanges(true); public override int SaveChanges(bool acceptAllChangesOnSuccess) => _eventsPublisher.ExecuteSaveTransaction(() => base.SaveChanges(acceptAllChangesOnSuccess)); public override Task<int> SaveChangesAsync(CancellationToken cancellationToken = default) => SaveChangesAsync(true, cancellationToken); public override Task<int> SaveChangesAsync( bool acceptAllChangesOnSuccess, CancellationToken cancellationToken = default) => _eventsPublisher.ExecuteSaveTransactionAsync(() => base.SaveChangesAsync(acceptAllChangesOnSuccess, cancellationToken)); } }"
  },
  "concepts/domain-events.html": {
    "href": "concepts/domain-events.html",
    "title": "DDD and Domain Events | Silverback",
    "keywords": "DDD and Domain Events One of the core features of Silverback is the ability to publish the domain events as part of the DbContext save changes transaction in order to guarantee consistency. The Silverback.Core.Model package contains a sample implementation of a <xref:Silverback.Domain.DomainEntity> but you can also implement you own type. Important In case of a custom implementation the only constraint is that you must implement the <xref:Silverback.Messaging.Messages.IMessagesSource> interface in order for Silverback to be able to access the associated events. The DomainEntity.AddEvent method adds the domain event to the events collection, to be published when the entity is saved. To enable this mechanism we just need to override the various SaveChanges methods to plug-in the <xref:Silverback.EntityFrameworkCore.DbContextEventsPublisher> contained in the Silverback.Core.EntityFrameworkCore package. Sample Entity DbContext using Silverback.Domain; namespace Sample { public class Basket : DomainEntity, IAggregateRoot { private readonly List<BasketItem> _items = new List<BasketItem>(); private Basket() { } public Basket(Guid userId) { UserId = userId; Created = DateTime.UtcNow; } [Key] public int Id { get; private set; } public IEnumerable<BasketItem> Items => _items.AsReadOnly(); public Guid UserId { get; private set; } public DateTime Created { get; private set; } public DateTime? CheckoutDate { get; private set; } public void Checkout() { CheckoutDate = DateTime.UtcNow; AddEvent<BasketCheckoutEvent>(); } } } using Microsoft.EntityFrameworkCore; using Silverback.EntityFrameworkCore; using Silverback.Messaging.Publishing; namespace Sample { public class SampleDbContext : DbContext { private readonly DbContextEventsPublisher _eventsPublisher; public SampleDbContext(IPublisher publisher) { _eventsPublisher = new DbContextEventsPublisher(publisher, this); } public SampleDbContext(DbContextOptions options, IPublisher publisher) : base(options) { _eventsPublisher = new DbContextEventsPublisher(publisher, this); } public override int SaveChanges() => SaveChanges(true); public override int SaveChanges(bool acceptAllChangesOnSuccess) => _eventsPublisher.ExecuteSaveTransaction(() => base.SaveChanges(acceptAllChangesOnSuccess)); public override Task<int> SaveChangesAsync(CancellationToken cancellationToken = default) => SaveChangesAsync(true, cancellationToken); public override Task<int> SaveChangesAsync( bool acceptAllChangesOnSuccess, CancellationToken cancellationToken = default) => _eventsPublisher.ExecuteSaveTransactionAsync(() => base.SaveChangesAsync(acceptAllChangesOnSuccess, cancellationToken)); } }"
  },
  "concepts/event-sourcing.html": {
    "href": "concepts/event-sourcing.html",
    "title": "Event Sourcing | Silverback",
    "keywords": "Event Sourcing Silverback.EventSourcing is a basic implementation of an event store that perfectly integrates within the Silverback ecosystem. At the moment only a version using Entity Framework Core is implemented, allowing to store the events in a database but other implementations may be added in the future. Configuration The only needed configuration is the call to UseDbContext<TDbContext> when initializing Silverback. public class Startup { public void ConfigureServices(IServiceCollection services) { services.AddSilverback().UseDbContext<MyDbContext>() } } Creating the Event Store Creating an event store is very straightforward and requires basically just 3 components: a domain entity model, the event store model and a repository. Domain Entity model The domain entity have to extend EventSourcingDomainEntity (or a custom class implementing IEventSourcingDomainEntity ). The two generic type parameters refer to the type of the key (entity unique identifier) and the base type for the domain events (can be omited if you don't need domain events). public class Person : EventSourcingDomainEntity<int, PersonDomainEvent> { public Person() { } public Person(IReadOnlyCollection<IEntityEvent> events) : base(events) { } public string Name { get; private set; } public string SocialSecurityNumber { get; private set; } public int Age { get; private set; } public string PhoneNumber { get; private set; } } Important The domain entity must have a constructor able to rebuild the entity state from the stored events. The AddAndApplyEvent protected method must be used to add new events. public class Person : EventSourcingDomainEntity<int, PersonDomainEvent> { public void ChangeName(string newName) => AddAndApplyEvent(new NameChangedEvent { NewName = newName }); public void ChangeAge(int newAge) => AddAndApplyEvent(new AgeChangedEvent { NewAge = newAge }); public void ChangePhoneNumber(string newPhoneNumber) => AddAndApplyEvent(new PhoneNumberChangedEvent { NewPhoneNumber = newPhoneNumber }); } An Apply method is needed for each event type to modify the entity current state according to the described mutation. public class Person : EventSourcingDomainEntity<int, PersonDomainEvent> { private void Apply(NameChangedEvent @event) => Name = @event.NewName; private void Apply(AgeChangedEvent @event) => Age = @event.NewAge; private void Apply(PhoneNumberChangedEvent @event, bool isReplaying) { PhoneNumber = @event.NewPhoneNumber; // Fire domain event only if the event is new if (!isReplaying) AddEvent<PhoneNumberChangedDomainEvent>(); } } Note The apply method can be private but it must have a specific signature: its name must begin with \"Apply\" and have a parameter of the specific event type (or base type). It can also receive an additional boolean parameter ( isReplaying ) that will let you differentiate between new events and events that are being reapplied because loaded from the store. The events are just models inheriting from EntityEvent (or another custom class implementing IEntityEvent ). public class NameChangedEvent : EntityEvent { public string NewName { get; set; } } public class AgeChangedEvent : EntityEvent { public int NewAge { get; set; } } public class PhoneNumberChangedEvent : EntityEvent { public string NewPhoneNumber { get; set; } } Event Store model The event store basically consists of an EventStore entity and related event (they either inherit from EventStoreEntity and EventEntity or implement the interfaces IEventStoreEntity and IEventEntity respectively). public class PersonEventStore : EventStoreEntity<PersonEvent> { [Key] public int Id { get; set; } public string SocialSecurityNumber { get; set; } } public class PersonEvent : EventEntity { [Key] public int Id { get; private set; } } Note The event store record can be extended with extra fields (see SocialSecurityNumber in the example above) and those will be automatically set with the value of the matching propertyi in the domain entity (unless the mapping method is overridden in the repository implementing a custom logic). Important It is advised to add some indexes and a concurrency token, to ensure proper performance and consistency. A DbSet must also be mapped to the defined event store entity and that's it. public class MyDbContext : DbContext { public MyDbContext(DbContextOptions options) : base(options) { } public DbSet<PersonEventStore> Persons { get; set; } } EventStore repository The repository is the component that is storing the domain entity in form of single events, being able to rebuild it afterwards. The repository must inherit from DbContextEventStoreRepository and the 4 generic type parameters refer respectively to: the domain entity its unique key the event store entity its related event entity public class PersonEventStoreRepository : DbContextEventStoreRepository<Person, int, PersonEventStore, PersonEvent> { public PersonEventStoreRepository(DbContext dbContext) : base(dbContext) { } } Storing and retrieving entities Using the EventStoreRepository to store and retrieve domain entities is fairly simple. Have a look at the following code snippet to get an idea. public class PersonService { private readonly MyDbContext _dbContext; private readonly PersonEventStoreRepository _repository = new PersonEventStoreRepository(_dbContext); public async Task<Person> CreatePerson(string name, int age) { var person = new Person(); person.ChangeName(\"Sergio\"); person.ChangeAge(35); person = await _repository.StoreAsync(person); await _dbContext.SaveChangesAsync(); return person; } public async Task<Person> ChangePhoneNumber( int personId, string newPhoneNumber) { var person = _repository.Get(p => p.Id == personId); person.ChangePhoneNumber(newPhoneNumber); person = await _repository.StoreAsync(person); await _dbContext.SaveChangesAsync(); return person; } } Merging events / handling conflicts You may need to merge events coming from different sources and/or being received with a certain latency. In the example below the Apply method checks whether another (newer) conflicting event was added already in the meantime. private void Apply(NameChangedEvent @event, bool isReplaying) { // Skip if a newer event exists if (!isReplaying && Events.Any(e => e is NameChangedEvent && e.Timestamp > @event.Timestamp)) { return; } Name = @event.NewName; }"
  },
  "concepts/introduction.html": {
    "href": "concepts/introduction.html",
    "title": "Introduction | Silverback",
    "keywords": "Introduction What's Silverback? Silverback is essentially a bus that can be either used internally to an application or connected to a message broker to integrate different applications or microservices. Silverback is used to exchange message inside an application and/or connect to a message broker to integrate with other applications or microservices. Samples and examples This documentation is filled with examples and code snippets, plus an entire section is dedicated to fully functional ready-to-run code samples . The plan is to improve the samples section over time, adding real-world scenarios that demonstrates how to take advantage of the various Silverback features in your projects. A few notes about the code: Even though it will compile and work just fine (most of the time), it is intended for demo purpose only and don't automatically imply the best practices are being in place nor it is by any mean production ready. Sometimes some shortcuts may have been taken to keep the code compact, readable and focused on the aspect that is being highlighted. Not all examples are ported to each message broker implementation, but most of them can be adapted to work with either Kafka, MQTT or Rabbit with minimal effort (unless some broker-specific features are being used, of course). Packages Silverback is modular and delivered in multiple packages, available through nuget.org . Core Silverback.Core It implements a very simple, yet very effective, publish/subscribe in-memory bus that can be used to decouple the software parts and easily implement a Domain Driven Design approach. Silverback.Core.Model It contains some interfaces that will help organize the messages and write cleaner code, adding some semantic. It also includes a sample implementation of a base class for your domain entities. Silverback.Core.EntityFrameworkCore It contains the storage implementation to integrate Silverback with Entity Framework Core. It is needed to use a DbContext as storage for (temporary) data and to fire the domain events as part of the SaveChanges transaction. Silverback.Core.Rx Adds the possibility to create an Rx Observable over the internal bus. Integration Silverback.Integration Contains the message broker and connectors abstraction. Inbound and outbound connectors can be attached to a message broker to either export some events/commands/messages to other microservices or react to the messages fired by other microservices in the same way as internal messages are handled. Silverback.Integration.Testing Includes some utilities to help writing automated tests involving Silverback.Integration. Silverback.Integration.Kafka An implementation of Silverback.Integration for the popular Apache Kafka message broker. Silverback.Integration.Kafka.SchemaRegistry Adds the support for Apache Avro and the schema registry on top of Silverback.Integration.Kafka . Silverback.Integration.Kafka.Testing Includes a mock for the Kafka message broker to be used for in-memory testing. Silverback.Integration.MQTT An implementation of Silverback.Integration for MQTT. Silverback.Integration.MQTT.Testing Includes a mock for the MQTT message broker to be used for in-memory testing. Silverback.Integration.RabbitMQ An implementation of Silverback.Integration for the popular RabbitMQ message broker. Silverback.Integration.RabbitMQ.Testing (coming soon) Includes a mock for the RabbitMQ message broker to be used for in-memory testing. Silverback.Integration.HealthChecks Contains the extensions for Microsoft.Extensions.Diagnostics.HealthChecks to monitor the connection to the message broker. Silverback.Integration.Newtonsoft Contains the legacy implementations of <xref:Silverback.Messaging.Serialization.IMessageSerializer>, based on Newtonsoft.Json. Event Sourcing Silverback.EventSourcing Contains an implementation of an event store that perfectly integrates within the Silverback ecosystem. Glossary The following list serves as introduction to the terminology and types used in Silverback. Publisher An object that can be used to publish messages to the internal in-memory bus. It is represented by the <xref:Silverback.Messaging.Publishing.IPublisher> or (better) the more specific <xref:Silverback.Messaging.Publishing.IEventPublisher> and <xref:Silverback.Messaging.Publishing.ICommandPublisher> interfaces, that can be resolved via dependency injection. Subscriber A method (or delegate) that is subscribed to the bus and will process some (or all) of the messages that will be published or consumed from a message broker (since those messages are automatically pushed to the internal bus). Broker A message broker, like Apache Kafka or RabbitMQ. It is represented by the <xref:Silverback.Messaging.Broker.IBroker> interface and is used internally by Silverback to bind the internal bus with a message broker. It can be resolved and used directly but that shouldn't be necessary for most of the use cases. Producer An object used to publish messages to the broker. It is represented by the <xref:Silverback.Messaging.Broker.IProducer> interface. Consumer An object used to receive messages from the broker. It is represented by the <xref:Silverback.Messaging.Broker.IConsumer> interface. Endpoint Identifies a specific topic or queue. It also contains all the settings to bind to that endpoint and is therefore specific to the message broker implementation. It is represented by an implementation of the <xref:Silverback.Messaging.IEndpoint> interface. Inbound Endpoint / Consumer Endpoint An endpoint that is consumed and whose messages are relayed into the internal bus, where they can be consumed by one or more subscribers. It is represented by an implementation of the <xref:Silverback.Messaging.IConsumerEndpoint> interface such as the <xref:Silverback.Messaging.KafkaConsumerEndpoint>. Outbound Endpoint / Producer Endpoint Silverback can be configured to automatically publish some messages to the message broker, observing the internal bus and relaying the messages matching with the configure type. The outbound/producer endpoint specifies the topic or queue where those message have to be produced. It is represented by an implementation of the <xref:Silverback.Messaging.IProducerEndpoint> interface such as the <xref:Silverback.Messaging.KafkaProducerEndpoint>. Behavior Multiple behaviors are chained to build a sort of pipeline to process the messages transiting across the internal bus, the consumer or the producer. They are used to implement cross-cutting concerns, isolate responsibilities and allow for greater flexibility. Some built-in behaviors are responsible for serialization, error policies enforcement, batching, chunking, encryption, etc."
  },
  "concepts/logging.html": {
    "href": "concepts/logging.html",
    "title": "Logging | Silverback",
    "keywords": "Logging Silverback logs quite a few events that may be very useful for troubleshooting. It is recommended to set the minum log level to Information for the Silverback namespace, in order to have the important logs while avoiding too much noise. Customizing log levels The WithLogLevels configuration method can be used to tweak the log levels of each event. public class Startup { public void ConfigureServices(IServiceCollection services) { services .AddSilverback() .WithLogLevels(configurator => configurator .SetLogLevel(IntegrationLogEvents.MessageSkipped, LogLevel.Critical) .SetLogLevel(IntegrationLogEvents.ErrorProcessingInboundMessage, LogLevel.Error)); } } Each package (that writes any log) has a static class declaring each log event (see next chapter). Logged events Here is a list of all events that are being logged and their default log level. See also: <xref:Silverback.Diagnostics.CoreLogEvents> <xref:Silverback.Diagnostics.IntegrationLogEvents> <xref:Silverback.Diagnostics.KafkaLogEvents> <xref:Silverback.Diagnostics.RabbitLogEvents> Core Id Level Message Reference 11 Debug Discarding result of type {type} because it doesn't match the expected return type {expectedType}. SubscriberResultDiscarded 21 Information Trying to acquire lock {lockName} ({lockUniqueId})... AcquiringDistributedLock 22 Information Acquired lock {lockName} ({lockUniqueId}). DistributedLockAcquired 23 Debug Failed to acquire lock {lockName} ({lockUniqueId}). FailedToAcquireDistributedLock 24 Information Released lock {lockName} ({lockUniqueId}). DistributedLockReleased 25 Warning Failed to release lock {lockName} ({lockUniqueId}). FailedToReleaseDistributedLock 26 Error Failed to check lock {lockName} ({lockUniqueId}). FailedToCheckDistributedLock 27 Error Failed to send heartbeat for lock {lockName} ({lockUniqueId}). FailedToSendDistributedLockHeartbeat 41 Information Starting background service {backgroundService}... BackgroundServiceStarting 42 Information Lock acquired, executing background service {backgroundService}. BackgroundServiceLockAcquired 43 Error Background service {backgroundService} execution failed. BackgroundServiceException 51 Information Background service {backgroundService} stopped. RecurringBackgroundServiceStopped 52 Debug Background service {backgroundService} sleeping for {delay} milliseconds. RecurringBackgroundServiceSleeping 53 Warning Background service {backgroundService} execution failed. RecurringBackgroundServiceException Integration Id Level Message Reference 1001 Information Processing inbound message. ProcessingInboundMessage 1002 Error Error occurred processing the inbound message. ErrorProcessingInboundMessage 1003 Debug Message '{messageId}' added to {sequenceType} '{sequenceId}'. | length: {sequenceLength} MessageAddedToSequence 1004 Debug Started new {sequenceType} '{sequenceId}'. SequenceStarted 1005 Debug {sequenceType} '{sequenceId}' completed. | length: {sequenceLength} SequenceCompleted 1006 Debug The {sequenceType} '{sequenceId}' processing has been aborted. | length: {sequenceLength}, reason: {reason} SequenceProcessingAborted 1007 Error Error occurred processing the {sequenceType} '{sequenceId}'. | length: {sequenceLength} ErrorProcessingInboundSequence 1008 Warning The incomplete {sequenceType} '{sequenceId}' is aborted. | length: {sequenceLength} IncompleteSequenceAborted 1009 Warning Skipping the incomplete sequence '{sequenceId}'. The first message is missing. SkippingIncompleteSequence 1110 Warning Error occurred aborting the {sequenceType} '{sequenceId}'. ErrorAbortingInboundSequence 1011 Debug {broker} connecting to message broker... BrokerConnecting 1012 Information {broker} connected to message broker. BrokerConnected 1013 Debug {broker} disconnecting from message broker... BrokerDisconnecting 1014 Information {broker} disconnected from message broker. BrokerDisconnected 1015 Information Creating new consumer for endpoint '{endpointName}'. CreatingNewConsumer 1016 Information Creating new producer for endpoint '{endpointName}'. CreatingNewProducer 1017 Error Error occurred connecting to the message broker(s). BrokerConnectionError 1021 Debug Connected consumer to endpoint. ConsumerConnected 1022 Debug Disconnected consumer from endpoint. ConsumerDisconnected 1023 Critical Fatal error occurred processing the consumed message. The consumer will be stopped. ConsumerFatalError 1024 Warning Error occurred while disposing the consumer. ConsumerDisposingError 1025 Error Commit failed. ConsumerCommitError 1026 Error Rollback failed. ConsumerRollbackError 1127 Error Error occurred while connecting the consumer. ConsumerConnectError 1128 Error Error occurred while disconnecting the consumer. ConsumerDisconnectError 1129 Error Error occurred while (re)starting the consumer. ConsumerStartError 1130 Error Error occurred while stopping the consumer. ConsumerStopError 1131 Warning Failed to reconnect the consumer. Will retry in {retryDelay} milliseconds. ErrorReconnectingConsumer 1027 Debug Connected producer to endpoint. ProducerConnected 1028 Debug Disconnected producer from endpoint. ProducerDisconnected 1031 Information Message produced. MessageProduced 1032 Warning Error occurred producing the message. ErrorProducingMessage 1041 Trace The {policyType} will be skipped because the current failed attempts ({failedAttempts}) exceeds the configured maximum attempts ({maxFailedAttempts}). PolicyMaxFailedAttemptsExceeded 1042 Trace The {policyType} will be skipped because the {exceptionType} is not in the list of handled exceptions. PolicyExceptionNotIncluded 1043 Trace The {policyType} will be skipped because the {exceptionType} is in the list of excluded exceptions. PolicyExceptionExcluded 1044 Trace The {policyType} will be skipped because the apply rule evaluated to false. PolicyApplyRuleReturnedFalse 1045 Trace All policies have been applied but the message(s) couldn't be successfully processed. The consumer will be stopped. PolicyChainCompleted 1046 Information The message(s) will be processed again. RetryMessageProcessing 1047 Trace Waiting {delay} milliseconds before retrying to process the message(s). RetryDelayed 1048 Information The message will be moved to the endpoint '{targetEndpointName}'. MessageMoved 1049 Information The message(s) will be skipped. MessageSkipped 1050 Warning The message belongs to a {sequenceType} and cannot be moved. CannotMoveSequences 1051 Warning Error occurred while rolling back, the retry error policy cannot be applied. The consumer will be reconnected. RollbackToRetryFailed 1052 Warning Error occurred while rolling back or committing, the skip message error policy cannot be applied. The consumer will be reconnected. RollbackToSkipFailed 1061 Warning Not used anymore. ErrorInitializingActivity 1062 Information The null message will be skipped. NullMessageSkipped 1072 Information Message is being skipped since it was already processed. MessageAlreadyProcessed 1073 Debug Writing the outbound message to the transactional outbox. MessageWrittenToOutbox 1074 Trace Reading a batch of {readBatchSize} messages from the outbox queue... ReadingMessagesFromOutbox 1075 Trace The outbox is empty. OutboxEmpty 1076 Debug Processing outbox message {currentMessageIndex} of {totalMessages}. ProcessingOutboxStoredMessage 1077 Error Failed to produce the message stored in the outbox. ErrorProducingOutboxStoredMessage 1078 Error Error occurred processing the outbox. ErrorProcessingOutbox 1079 Warning An invalid message has been produced. | validation errors:{validationErrors} InvalidMessageProduced 1080 Warning An invalid message has been processed. | validation errors:{validationErrors} InvalidMessageProcessed 1101 Critical Invalid configuration for endpoint '{endpointName}'. InvalidEndpointConfiguration 1102 Critical Error occurred configuring the endpoints. | configurator: {endpointsConfiguratorName} EndpointConfiguratorError 1103 Error Error occurred invoking the callback handler(s). CallbackHandlerError 1999 Trace The actual message will vary. LowLevelTracing Kafka Id Level Message Reference 2011 Debug Consuming message: {topic}[{partition}]@{offset}. ConsumingMessage 2012 Information Partition EOF reached: {topic}[{partition}]@{offset}. EndOfPartition 2013 Warning An error occurred while trying to pull the next message. The consumer will try to recover. KafkaExceptionAutoRecovery 2014 Error An error occurred while trying to pull the next message. The consumer will be stopped. Enable auto recovery to allow Silverback to automatically try to recover (EnableAutoRecovery=true in the consumer configuration). KafkaExceptionNoAutoRecovery 2015 Warning Not used anymore. ErrorRecoveringFromKafkaException 2016 Trace Consuming canceled. ConsumingCanceled 2021 Debug Creating Confluent.Kafka.Producer... CreatingConfluentProducer 2022 Warning The message was transmitted to broker, but no acknowledgement was received. ProduceNotAcknowledged 2031 Information Assigned partition {topic}[{partition}]. PartitionAssigned 2032 Debug {topic}[{partition}] offset will be reset to {offset}. PartitionOffsetReset 2033 Information Revoked partition {topic}[{partition}] (offset was {offset}). PartitionRevoked 2034 Debug Successfully committed offset {topic}[{partition}]@{offset}. OffsetCommitted 2035 Error Error occurred committing the offset {topic}[{partition}]@{offset}: '{errorReason}' ({errorCode}). OffsetCommitError 2036 Error Fatal error in Kafka consumer: '{errorReason}' ({errorCode}). ConfluentConsumerFatalError 2037 Error Error in Kafka error handler. KafkaErrorHandlerError 2038 Debug Kafka consumer statistics received: {statistics} ConsumerStatisticsReceived 2039 Debug Kafka producer statistics received: {statistics} ProducerStatisticsReceived 2040 Error The received statistics JSON couldn't be deserialized. StatisticsDeserializationError 2041 Information Assigned partition {topic}[{partition}]@{offset}. PartitionManuallyAssigned 2042 Warning Error in Kafka consumer: '{errorReason}' ({errorCode}). ConfluentConsumerError 2043 Error Error in Kafka log handler. KafkaLogHandlerError 2050 Warning An error occurred while disconnecting the consumer. ConfluentConsumerDisconnectError 2060 Warning {sysLogLevel} event from Confluent.Kafka consumer: '{logMessage}'. -> The consumer will try to recover. PollTimeoutAutoRecovery 2061 Error {sysLogLevel} event from Confluent.Kafka consumer: '{logMessage}'. -> Enable auto recovery to allow Silverback to automatically try to recover (EnableAutoRecovery=true in the consumer configuration). PollTimeoutNoAutoRecovery 2201 Critical {sysLogLevel} event from Confluent.Kafka producer: '{logMessage}'. ConfluentProducerLogCritical 2202 Error {sysLogLevel} event from Confluent.Kafka producer: '{logMessage}'. ConfluentProducerLogError 2203 Warning {sysLogLevel} event from Confluent.Kafka producer: '{logMessage}'. ConfluentProducerLogWarning 2204 Information {sysLogLevel} event from Confluent.Kafka producer: '{logMessage}'. ConfluentProducerLogInformation 2205 Debug {sysLogLevel} event from Confluent.Kafka producer: '{logMessage}'. ConfluentProducerLogDebug 2211 Critical {sysLogLevel} event from Confluent.Kafka consumer: '{logMessage}'. ConfluentConsumerLogCritical 2212 Error {sysLogLevel} event from Confluent.Kafka consumer: '{logMessage}'. ConfluentConsumerLogError 2213 Warning {sysLogLevel} event from Confluent.Kafka consumer: '{logMessage}'. ConfluentConsumerLogWarning 2214 Information {sysLogLevel} event from Confluent.Kafka consumer: '{logMessage}'. ConfluentConsumerLogInformation 2215 Debug {sysLogLevel} event from Confluent.Kafka consumer: '{logMessage}'. ConfluentConsumerLogDebug MQTT Id Level Message Reference 4011 Debug Consuming message '{messageId}' from topic '{topic}'. ConsumingMessage 4021 Warning Error occurred connecting to the MQTT broker. | clientId: {clientId} ConnectError 4022 Debug Error occurred retrying to connect to the MQTT broker. | clientId: {clientId} ConnectRetryError 4023 Warning Connection with the MQTT broker lost. The client will try to reconnect. | clientId: {clientId} ConnectionLost 4031 Debug Producer queue processing was canceled. ProducerQueueProcessingCanceled 4101 Error Error from MqttClient ({source}): '{logMessage}'. MqttClientLogError 4102 Warning Warning from MqttClient ({source}): '{logMessage}'. MqttClientLogWarning 4103 Information Information from MqttClient ({source}): '{logMessage}'. MqttClientLogInformation 4104 Trace Verbose from MqttClient ({source}): '{logMessage}'. MqttClientLogVerbose Tracing An Activity is created: in the Consumer when a message is received (initialized with the traceparent header, if submitted) in the Producer when a message is being sent (submitting the Activity.Id in the traceparent header ) when a sequence (e.g. a BatchSequence ) is being consumed when a subscriber is being invoked (either internally or from a consumer) This allows to trace the methods execution and follow a message across different services (distributed tracing). The following table summarizes the activities and the information being tracked. Id Description / Tags Silverback.Integration.Produce A message is being produced to a message broker. Tags: messaging.message_id messaging.destination [ messaging.kafka.message_key ] [ messaging.kafka.partition ] Silverback.Integration.Consume A consumed message is being processed. Tags: messaging.message_id messaging.destination [ messaging.sequence.activity ] [ messaging.kafka.message_key ] [ messaging.kafka.partition ] Silverback.Integration.Sequence A sequence of messages is being processed. Tags: none Silverback.Core.Subscribers.InvokeSubscriber A subscriber is being invoked to process a message. Tags: SubscriberType SubscriberMethod"
  },
  "contributing.html": {
    "href": "contributing.html",
    "title": "Contributing | Silverback",
    "keywords": "Contributing You are encouraged to contribute to Silverback! Please check out the how to contribute guide for guidelines about how to proceed. Source Code The full source code is available on GitHub Versioning The Directory.Build.props file in the root of the repository contains the current version of the NuGet packages being built and referenced. The <BaseVersion> and <VersionSuffix> tags can be used to increment the current version. Commits Please try to follow the Conventional Commits specification for the commit messages. Building (NuGet packages) The nuget packages can be built locally using the powershell script under /nuget/Update.ps1 . Add the -l switch to clear the local nuget cache as well (especially useful when building the same version over and over). Testing The main solution contains quite a few unit tests and additionally some samples are implemented in a separate solution. Contributors Thank you to all the present and future contributors . You are amazing!"
  },
  "index.html": {
    "href": "index.html",
    "title": "Home | Silverback",
    "keywords": ""
  },
  "releases.html": {
    "href": "releases.html",
    "title": "Releases | Silverback",
    "keywords": "Releases 4.3.2 Fixes Fix potential message loss in OutboxWorker 4.3.1 Fixes Fix deadlock in OutboxWorker when enforceMessageOrder=true (default) 4.3.0 What's new Upgrade to MQTTnet 4.2.0.706 Upgrade to Confluent.Kafka 2.1.1 Upgrade to Newtonsoft.Json 13.0.3 Add ReasonCode and ReasonString to MqttProduceException message 4.2.1 Fixes Prevent an ObjectDisposedException to be thrown by the BrokerCallbackInvoker during application shutdown Fix possible deadlock in BatchSequence with timeout 4.2.0 What's new Upgrade to Confluent.Kafka 2.1.0 4.1.2 Fixes Fix chained error policies attempts counter 4.1.1 Fixes Fix bug in outbox producer writing to the wrong endpoint [ #165 ] 4.1.0 What's new Upgrade to Confluent.Kafka 2.0.2 Replace the deprecated package Confluent.Kafka.SchemaRegistry.Serdes with Confluent.Kafka.SchemaRegistry.Serdes.Avro Simplify the Avro serializer configuration and add samples (see Kafka - Avro ) 4.0.1 What's new Upgrade to MQTTnet 4.1.4.563 4.0.0 What's new Upgrade to MQTTnet 4.1.3.436 Upgrade to Confluent.Kafka 1.9.3 Upgrade to Newtonsoft.Json 13.0.2 Disable server-side offset commit in <xref:Silverback.Messaging.Broker.KafkaConsumer> when GroupId is not set Breaking changes Some breaking changes in MQTTnet 4 are reflected into Silverback 3.8.0 What's new Topic name resolvers can be used to filter the messages to be produce: returning null will discard the message Fixes Fix error policies not being triggered consistently when batch consuming Make IntegrationSpy fully thread-safe Prevent errors when the IKafkaPartitionsRevokedCallback is invoked during application shutdown Improve error handling during connection to MQTT 3.7.3 Fixes Support topic names with symbols (e.g. hyphens) in mocked MQTT broker 3.7.2 Fixes Correctly invoke the IKafkaOffsetCommittedCallback when auto commit is disabled [ #167 ] 3.7.1 Fixes Improve message streams abort process to avoid first chance exceptions (e.g. during dispose) 3.7.0 What's new Implement basic support for Kafka transactions via <xref:Silverback.Messaging.Broker.KafkaTransactionalProducer> (Note: this is just a first step and a more comprehensive implementation is planned for the upcoming release 4.0.0) Skip chunking when processing single chunk messages Fixes Fix possible race condition in consumer pipeline 3.6.1 Fixes Handle race condition in BatchSequence with timeout Limit consumer status history 3.6.0 What's new Handle IAsyncEnumerable<T> returned by the subscriber and republished the contained messages Enrich Kafka messages moved by the <xref:Silverback.Messaging.Inbound.ErrorHandling.MoveMessageErrorPolicy> adding some extra headers containing some information about the source topic, partition, offset, etc. (see Message Headers ) Allow filters such as the <xref:Silverback.Messaging.Subscribers.KafkaGroupIdFilterAttribute> or <xref:Silverback.Messaging.Subscribers.MqttClientIdFilterAttribute> to be added to the subscribers at runtime via the configuration API (see Multiple Consumer Groups (in same process) and Multiple Clients (in same process) ) Add overload for Publish method in the error policies that forwards the exception as well as the envelope Throw TimeoutException from <xref:Silverback.Testing.KafkaTestingHelper> and <xref:Silverback.Testing.MqttTestingHelper> Improve MQTT connection related logs (info for successful reconnect and add broker name to log messages) Support shared sessions in mocked MQTT broker Fixes Ensure each consumed message gets a unique traceId (when the traceparent header is not present) Fix memory leak in consumer Fully validate messages, including nested objects 3.5.0 What's new Log MqttClient internal events (see Logging ) Upgrade to Confluent.Kafka 1.8.2 Upgrade to MQTTnet 3.0.16 Upgrade to RabbitMQ.Client 6.2.2 Update several dependencies Fixes Fix <xref:Silverback.Messaging.Broker.MqttConsumer> reconnection issues Handle edge cases related to MQTT acknowledgment timeout in <xref:Silverback.Messaging.Broker.MqttConsumer> Allow max retries specification and error policies chains with MQTT V3 3.4.0 What's new Support encryption key rotation (see Encryption ) 3.3.1 Fixes Fix AddHeaders<TMessage> and WithKafkaKey<TMessage> not being correctly invoked by all IProducer.Produce and IProducer.ProducerAsync overloads Add endpoint friendly name to all logs 3.3.0 What's new Optimize in-memory mocked Kafka (avoid spawning too many threads) Support multiple brokers (with overlapping topic names) in mocked Kafka and MQTT Add message validation for both producer and consumer (see Message Validation ) Add new AddInbound overloads specifying message type for a more compact configuration when using the typed deserializer (see Serialization ) Fixes Invoke the Kafka partition EOF callback for all connected consumers Ignore null or empty Kafka key in producer 3.2.0 What's new Add new Kafka partition EOF callback to be notified when the end of a partition is reached by the consumer (see Kafka Events and <xref:Silverback.Messaging.Broker.Callbacks.IKafkaPartitionEofCallback>) Allow multiple calls to IKafkaConsumerEndpointBuilder.Configure or IKafkaProducerEndpointBuilder.Configure for the same endpoint Observe a grace period in the <xref:Silverback.Messaging.HealthChecks.ConsumersHealthCheck> to prevent false positives during a normal Kafka rebalance Add optional friendly name to the endpoints (see IEndpointBuilder .WithName and Endpoint.FriendlyName ) Allow filtering the endpoints targeted by the <xref:Silverback.Messaging.HealthChecks.ConsumersHealthCheck> (see AddConsumersCheck ) 3.1.1 Fixes Invoke broker callbacks during the application shutdown to allow custom code to be run when disconnecting 3.1.0 What's new Add new ways to configure headers and kafka key (see Message Headers and Kafka Partitioning and Message Key ) New callbacks for Kafka log events (see Kafka Events ) Improve consumer status tracking introducing ConsumerStatus.Ready Revert the Kafka consumer status from Ready to Connected whenever partitions are revoked or a poll timeout occurs Adapt consumer health check to monitor the new status and report unhealthy if not Ready (see Health Monitoring ) Try to automatically recover from Kafka maximum poll interval exceed errors Improve Kafka static partition assignment with resolver function and fetching the available partitions (see Kafka Partitioning and Message Key ) Upgrade to Confluent.Kafka 1.7.0 Upgrade to MQTTnet 3.0.15 Fixes Prevent possible race condition causing messages to be skipped when a RetryPolicy kicks in for messages from multiple Kafka partitions simultaneously Prevent ObjectDisposedException to be thrown when Kafka events (e.g. statistics) are fired during the application shutdown Prevent ObjectDisposedException to be thrown when Consumer.Dispose is called multiple times Properly clear the trace context ( Activity ) when reconnecting the consumer to prevent the newly started consume loop to be tracked under the current message traceId Fix wrong prefix in MQTT log event names 3.0.1 Fixes Fix <xref:Silverback.Messaging.Outbound.TransactionalOutbox.Repositories.IOutboxWriter> lifecycle [ #128 ] 3.0.0 What's new Add support for MQTT (see Connecting to a Message Broker , Inbound Endpoint , Outbound Endpoint , ...) Simplify configuration and reduce boilerplate (see Subscribing and Connecting to a Message Broker ) Simplify subscribers registration and get rid of the ISubscriber interface (see Subscribing ) Scan subscribers automatically at startup to reduce cost of first message Connect brokers and handle graceful shutdown automatically (see Connecting to a Message Broker ) Improve endpoints configuration API (see Connecting to a Message Broker ) Add IServiceCollection.ConfigureSilverback extension method to conveniently split the configuration code (see Enabling Silverback ) Refactor Silverback.Integration to support streaming Create <xref:Silverback.Messaging.Messages.IMessageStreamEnumerable`1> (see Streaming ) Improve chunking support in conjunction with streaming, requiring only one chunk at a time to be loaded into memory Redesign sequences handling to support chunking, batch consuming and future sequences as well Improve Kafka partitions handling (see Kafka Partitioning and Message Key ) Process partitions independently and concurrently Add setting to produce to a specific partition Add setting to manually assign the consumer partitions Add option to throw an exception if no subscriber is handling a message that was published to the internal bus or was consumed from a message broker (see throwIfUnhandled argument in the <xref:Silverback.Messaging.Publishing.IPublisher> methods and ThrowIfUnhandled property in the <xref:Silverback.Messaging.IConsumerEndpoint>) Handle null messages as <xref:Silverback.Messaging.Messages.Tombstone>/<xref:Silverback.Messaging.Messages.Tombstone`1> (see Tombstone Message ) Replace Newtonsoft.Json with System.Text.Json to improve serialization and deserialization performance (the old serializers have been moved into the Silverback.Integration.Newtonsoft package, see Serialization ) Improve outbound routing customization options with endpoint name resolvers (see Outbound Messages Routing ) Add non-blocking Produce / ProduceAsync / RawProduce / RawProduceAsync overloads to <xref:Silverback.Messaging.Broker.IProducer>, better suitable for higher throughput scenarios (see Producer ) Refactor broker event handlers (see Broker Callbacks ) Expose IConsumer.StopAsync and IConsumer.StartAsync methods to pause and resume consumers Add log levels configuration (see Logging ) Improve (distributed) tracing (see Logging ) Allow header names customization (see Message Headers ) Add consumer status information and statistics (see Connecting to a Message Broker ) Add basic consumer health check (see Health Monitoring ) Allow broker behaviors to be registered as transient, meaning that an instance will be created per each producer or consumer (see Broker behaviors pipeline ) Improve code quality Enhance CI pipeline to use Roslyn analyzers Integrate SonarCloud ) Improve integration tests Increase automated tests coverage Enable nullable reference types and adjust all API Document the entire public API (see API Documentation ) Released some utilities to help writing automated tests involving Silverback.Integration (see Testing ) Upgrade to Confluent.Kafka 1.6.2 Upgrade to RabbitMQ.Client 6.2.1 Fixes Fix <xref:Silverback.Messaging.Outbound.TransactionalOutbox.OutboxWorker> not publishing custom headers [ #102 ] Breaking Changes Refactored <xref:Silverback.Messaging.Publishing.IPublisher> Removed the overloads to publish a batch of messages (see Publishing ) Cannot subscribe to collection of messages anymore (see Subscribing ), unless they are consumed from a message broker (see Streaming ) The chunks belonging to the same message must be contiguous (interleaved messages are at the moment not supported anymore) and in the same partition in case of Kafka Removed ISubscriber interface Removed BusConfigurator (moved all the configuration into the <xref:Silverback.Messaging.Configuration.ISilverbackBuilder> extension methods) Replaced BusConfigurator.Connect with ISilverbackBuilder.AddEndpointsConfigurator and ISilverbackBuilder.AddEndpoints (or ISilverbackBuilder.AddKafkaEndpoints etc.) to configure the endpoints, while the broker is connected automatically at startup (see Connecting to a Message Broker ) Replaced BusConfigurator.Subscribe methods with ISilverbackBuilder.AddDelegateSubscriber (see Subscribing ) Replaced BusConfigurator.HandleMessagesOfType methods with ISilverbackBuilder.HandleMessageOfType (see Subscribing ) BusConfigurator.ScanSubscribers is not needed anymore since it gets called automatically at startup (from an IHostedService ) Removed IServiceCollection.Add*Subscriber , IServiceCollection.Add*Behavior , IServiceCollection.Add*BrokerBehavior , IServiceCollection.AddEndpointsConfigurator , IServiceCollection.Add*OutboundRouter extension methods, use the same methods on the <xref:Silverback.Messaging.Configuration.ISilverbackBuilder> (using IServiceCollection.ConfigureSilverback to get an instance if the <xref:Silverback.Messaging.Configuration.ISilverbackBuilder> if necessary, as shown in Enabling Silverback ) Removed IBrokerOptionsBuilder.Add*BrokerBehavior , IBrokerOptionsBuilder.RegisterConfigurator , IBrokerOptionsBuilder.Add*OutboundRouter extension methods, use the same methods on the <xref:Silverback.Messaging.Configuration.ISilverbackBuilder> (using IServiceCollection.ConfigureSilverback to get an instance if the <xref:Silverback.Messaging.Configuration.ISilverbackBuilder> if necessary, as shown in Enabling Silverback ) Reorganized the Silverback.Messaging.Configuration namespace moving some broker specific types under Silverback.Messaging.Configuration.Kafka , Silverback.Messaging.Configuration.Rabbit or Silverback.Messaging.Configuration.Mqtt The visibility of some types has been changed to internal to favor a cleaner and clearer API where the public types are well documented and their backward compatibility is valued Removed Silverback prefix from exceptions name Removed the IRequest interface (it was implemented by both <xref:Silverback.Messaging.Messages.IQuery`1> and <xref:Silverback.Messaging.Messages.ICommand`1>) Changed Impl methods suffix with Core , this affects some virtual members in the <xref:Silverback.Messaging.Broker.Broker`2> and other base classes IConsumer.Received event replaced by a callback delegate IBroker.GetConsumer and IBrokerCollection.GetConsumer methods renamed to IBroker.AddConsumer and IBrokerCollection.AddConsumer IQueueProducer and IQueueConsumer renamed to <xref:Silverback.Messaging.Outbound.TransactionalOutbox.Repositories.IOutboxWriter> and <xref:Silverback.Messaging.Outbound.TransactionalOutbox.Repositories.IOutboxReader> The messages with a null body are by default mapped to a <xref:Silverback.Messaging.Messages.Tombstone>/<xref:Silverback.Messaging.Messages.Tombstone`1> (see Tombstone Message ) Database: Moved all entities (used with Entity Framework Core) to the Silverback.Database.Model namespace Replaced InboundMessage entity with <xref:Silverback.Database.Model.InboundLogEntry> Replaced OutboundMessage entity with <xref:Silverback.Database.Model.OutboxMessage> Removed TemporaryMessageChunk Modified schema of <xref:Silverback.Database.Model.StoredOffset> entity Moved and renamed some internally used types (e.g. QueuedMessage , DbQueuedMessage , ...) Complete redesign of the error policies Removed IMessageIdProvider and all related logic: the Id or MessageId property will not be automatically initialized anymore and its value will not be used as identifier for the outbound message anymore (refer to the Message Identifier page for further details on how to set a custom message id, if needed) WithConnectionTo<> , WithConnectionToKafka , WithConnectionToRabbitMQ and WithInMemoryBroker have been removed, please use the new WithConnectionToMessageBroker and AddKafka / AddRabbit methods (see Connecting to a Message Broker ) Replaced the internal messages for the Kafka events such as partitions revoked/assigned, offset commit, error, log and statistics with event handler interfaces (see Kafka Events ) Deprecated Silverback.Integration.InMemory , use Silverback.Integration.Kafka.Testing , Silverback.Integration.RabbitMQ.Testing , etc. instead Renamed PartitioningKeyMemberAttribute to <xref:Silverback.Messaging.Messages.KafkaKeyMemberAttribute> Silverback.Integration.Configuration has been discontinued Renamed Settings property to Options in the default <xref:Silverback.Messaging.Serialization.JsonMessageSerializer> (since the switch to System.Text.Json ) Removed LogWithLevel method from <xref:Silverback.Messaging.Inbound.ErrorHandling.SkipMessageErrorPolicy>, use the new WithLogLevels configuration instead Removed Parallel option from <xref:Silverback.Messaging.Subscribers.SubscribeAttribute> Renamed Offset to a more generic BrokerMessageIdentifier in the Silverback.Integration abstractions (including the envelopes) Some changes to the behaviors: Renamed Handle to HandleAsync in the <xref:Silverback.Messaging.Publishing.IBehavior>, <xref:Silverback.Messaging.Broker.Behaviors.IProducerBehavior> and <xref:Silverback.Messaging.Broker.Behaviors.IConsumerBehavior> Changed signature of the HandleAsync method (see Behaviors and Broker behaviors pipeline ) Changed some sort indexes and introduced some new broker behaviors, you may need to adjust the sort index of your custom behaviors (see Broker behaviors pipeline for the updated list of built-in behaviors) Replaced IBroker.Connect and IBroker.Disconnect with IBroker.ConnectAsync and IBroker.DisconnectAsync Some major changes to batch consuming: Removed all batch events ( BatchStartedEvent , BatchCompleteEvent , BatchProcessedEvent , BatchAbortedEvent ), refer to Streaming to learn how to leverage the new <xref:Silverback.Messaging.Messages.IMessageStreamEnumerable`1> Setting the batch size to 1 doesn't disable batching anymore, set the Batch to null in the <xref:Silverback.Messaging.ConsumerEndpoint> to disable it When batching is enabled the messages can be subscribed only via the <xref:Silverback.Messaging.Messages.IMessageStreamEnumerable`1> (see Streaming ), the subscribers to the single messages will not be invoked <xref:Silverback.Messaging.Sequences.Chunking.ChunkSettings> moved from Silverback.Messaging.LargeMessages namespace to Silverback.Messaging.Sequences.Chunking Replaced CoreEventIds , IntegrationEventIds , KafkaEventIds and RabbitEventIds with <xref:Silverback.Diagnostics.CoreLogEvents>, <xref:Silverback.Diagnostics.IntegrationLogEvents>, <xref:Silverback.Diagnostics.KafkaLogEvents> and <xref:Silverback.Diagnostics.RabbitLogEvents> (see also Logging ) Deprecated support for Entity Framework 2, only the version 3.0.1 of Silverback.Core.EntityFrameworkCore will work with Silverback 3.0.0 Modified message encryption for chunked messages and it will not be compatible with previous versions of Silverback (affects chunking+encryption only) 2.2.0 What's new Allow custom outbound routers to be registered as scoped or transient (instead of singleton only) 2.1.2 Fixes Fix delay in Retry policy [ #97 ] 2.1.1 What's new Add support for multiple message brokers (Kafka and RabbitMQ) in the same application (see Connecting to a Message Broker ) Add end-to-end message encryption (see Encryption ) Add dynamic custom routing of outbound messages (see Outbound Messages Routing ) Improve support for message headers (see Message Headers ) Add support for binary files (see Binary Files ) Improve message identifier handling: the <xref:Silverback.Messaging.Messages.IIntegrationMessage> is not required to have an Id property anymore (the x-message-id header will still be generated and if the property exists will continue to be automatically initialized) x-first-chunk-offset header added by default (see Message Headers ) Deserialize KafkaStasticsEvent JSON and provided its content as an object (in addition to the raw JSON) Add support for Apache Avro and schema registry (see Serialization ) Upgrade to Confluent.Kafka 1.4.2 Add consumer PrefetchSize and PrefetchCount settings to <xref:Silverback.Messaging.RabbitConsumerEndpoint> Add AcknowledgeEach to the <xref:Silverback.Messaging.RabbitConsumerEndpoint> to define the number of message processed before sending the acknowledgment to the server Upgrade to RabbitMQ.Client 6.0.0 Improve message type resolution performance and reliability in <xref:Silverback.Messaging.Serialization.JsonMessageSerializer> Add LogWithLevel method to <xref:Silverback.Messaging.Inbound.ErrorHandling.SkipMessageErrorPolicy> to specify the desired level for the \"Message skipped\" log entry (the default is now increased to Error ) Breaking Changes These changes shouldn't affect you unless you built your own <xref:Silverback.Messaging.Broker.IBroker> implementation or are interacting at low-level with the <xref:Silverback.Messaging.Broker.IBroker> (this is why has been decided to still mark this as a minor release): The <xref:Silverback.Messaging.Broker.IBroker> interface and <xref:Silverback.Messaging.Broker.Broker`2> abstract base class have been modified to explicitly declare which endpoint type is being handled by the broker implementation The <xref:Silverback.Messaging.Serialization.IMessageSerializer> interfaces has been changed The <xref:Silverback.Messaging.Broker.Behaviors.IConsumerBehavior> and <xref:Silverback.Messaging.Broker.Behaviors.IProducerBehavior> interfaces have been changed Changed the parameters order in some less used overloads in the IBrokerOptionBuilder Announced Breaking Changes These aren't real breaking changes but some methods have been marked as deprecated and will be removed in one of the next major releases: WithConnectionTo<> , WithConnectionToKafka and WithConnectionToRabbitMQ are deprecated (they will still be supported in this version), please use the new WithConnectionToMessageBroker and AddKafka / AddRabbit methods (see Connecting to a Message Broker ) 2.0.0 What's new Create Silverback.Integration.RabbitMQ package to connect Silverback with RabbitMQ (see Connecting to a Message Broker ) Enable subscription of messages with an empty body (you must subscribe to the <xref:Silverback.Messaging.Messages.IInboundEnvelope>) [ #61 ] Add hook to manually set the Kafka partition start offset when a partition is assigned to the consumer (see Kafka Events ) [ #57 ] Support for multiple consumer groups running in the same process (see Multiple Consumer Groups (in same process) ) [ #59 ] Publish KafkaStatisticsEvent also from the <xref:Silverback.Messaging.Broker.KafkaProducer> (previously done in <xref:Silverback.Messaging.Broker.KafkaConsumer> only) Several reliability and performance related improvements Breaking Changes The <xref:Silverback.Messaging.Broker.IBroker>, <xref:Silverback.Messaging.Broker.IProducer> and <xref:Silverback.Messaging.Broker.IConsumer> interfaces have been slightly modified (it shouldn't affect you unless you built your own <xref:Silverback.Messaging.Broker.IBroker> implementation) Many interfaces (such as <xref:Silverback.Messaging.Publishing.IBehavior>) and delegates have been slightly modified to pass around an IReadOnlyCollection instead of an IEnumerable , to avoid the possible issues related to multiple enumeration of an IEnumerable The IMessageKeyProvider interface has been renamed to IMessageIdProvider to prevent to be mistaken with the Kafka Key or Rabbit's Routing Key IInboundMessage / IOutboundMessage (plus all the related types) have been renamed to <xref:Silverback.Messaging.Messages.IInboundEnvelope>/<xref:Silverback.Messaging.Messages.IOutboundEnvelope> and the property containing the actual message has been renamed from Content to Message The MustUnwrap option has been removed from the inbound connector configuration (messages are unwrapped by default) 1.2.0 What's new Publish events to the internal bus as a consequence to the Kafka events such as partitions assigned or revoked (see Kafka Events ) [ #34 ] 1.1.0 What's new Add <xref:Silverback.Messaging.Configuration.IEndpointsConfigurator> interface to allow splitting the endpoints configuration across multiple types (see Connecting to a Message Broker ) Add support for distributed tracing (based on System.Diagnostics ) Add <xref:Silverback.Messaging.Broker.Behaviors.IProducerBehavior> and <xref:Silverback.Messaging.Broker.Behaviors.IConsumerBehavior> to create an extension point closer to the actual message broker logic (see Broker behaviors pipeline ) Breaking Changes Replaced ISortedBehavior with a generic <xref:Silverback.ISorted> interface 1.0.5 What's new Upgrade to Confluent.Kafka 1.3.0 Fixes Fix OutboundQueueHealthCheck [ #43 ] Remove automatic disposal of the <xref:Silverback.Messaging.Broker.KafkaProducer> when a KafkaException is thrown (creating too many instances of the producer over a short time span could lead to too many active TCP connections) Fix the bug preventing a <xref:Silverback.Messaging.KafkaConsumerEndpoint> pointing to multiple topics to be successfully subscribed 1.0.4 Fixes Fix mortal loop issue: it is finally safe to consume and produce the same type of messages from within the same process (in a natural way, without any extra configuration) Since version 1.0.0 the messages routed to an endpoint aren't forwarded to any subscriber directly Now the inbound connector has been fixed as well, preventing the inbound messages to be immediately routed once again to the outbound endpoint and eliminating all possible causes of mortal loops 1.0.3 What's new Deprecate PartitioningKeyMemberAttribute in favor of <xref:Silverback.Messaging.Messages.KafkaKeyMemberAttribute>, since the message key isn't used just for partitioning (see Kafka Partitioning and Message Key ) Fixes Forward Kafka message key as-is (not hashed anymore) to avoid possible collisions and simplify debugging 1.0.2 Fixes Reintroduce Add*Subscriber and Add*Behavior as IServiceCollection extension methods (for backward compatibility and greater flexibility) [ #41 ] Add WithInMemoryBroker and OverrideWithInMemoryBroker extension methods (see Testing ) 1.0.0 What's new Optimize message size (no wrappers anymore) Improve headers usage: identifiers, types, chunks information, etc. are now all sent in the headers Review severity of some log entries Improve and clean up internal implementation Improve exception handling (flattening of AggregateException ) Upgrade to Confluent.Kafka 1.2.2 Add automatic recovers from fatal errors in <xref:Silverback.Messaging.Broker.KafkaConsumer> (can be disabled via Endpoint configuration) Support .Net Core 3.0 and Entity Framework Core 3.0 Refactor packages (EF binding logic is now in a single package, versioned after the related EF version) Improve configuration API Improve and optimize performance (including #37 ) Improve database locks mechanism (used also to run the OutboundQueueWorker ) Fixes Fix issue requiring types not implementing <xref:Silverback.Messaging.Messages.IMessage> to be registered with HandleMessagesOfType<T> to consume them [ #33 ] Mitigate issue causing the <xref:Silverback.Background.DistributedBackgroundService> to sometime fail to acquire the database lock [ #39 ] Fix partition key value being lost when using the DeferredOutboundConnector Other small fixes to improve stability and reliability Breaking Changes By default the messages published via <xref:Silverback.Messaging.Publishing.IPublisher> that are routed to an outbound endpoint are not sent through to the internal bus and cannot therefore be subscribed locally, within the same process (see Outbound Endpoint ) Some changes in IInboundMessage and IOutboundMessage interfaces Changes to the schema of the outbox table ( Silverback.Messaging.Connectors.Model.OutboundMessage ) The configuration fluent API changed quite a bit, refer to the current documentation Important WithConnectionTo<KafkaBroker> has to be replaced with WithConnectionToKafka in order for all features to work properly. When failing to do so no message key will be generated, causing the messages to land in a random partition and/or preventing to publish to a compacted topic. (see Kafka Partitioning and Message Key ) Silverback.Integration.EntityFrameworkCore and Silverback.EventSourcing.EntityFrameworkCore have been deprecated ( Silverback.Core.EntityFrameworkCore contains all the necessary logic to use EF as store) KeyMemberAttribute has been renamed to PartitioningKeyMemberAttribute (see Connecting to a Message Broker ) 0.10.0 What's new Improve error handling: now all exceptions, including the ones thrown by the message serialzer can be handled through the error policies Improve logs: promoted some important logs to Information level, writing all processing errors as (at least) Warning and improved logged information quality (logged attributes) Add ability to modify messages and headers when moving them via <xref:Silverback.Messaging.Inbound.ErrorHandling.MoveMessageErrorPolicy> Refactor message processing to a cleaner, more extensible and predictable API and behavior Fixes Fixed several small (and not so small) issues and bugs 0.8.0 - 0.9.0 Released two versions mostly to fix bugs, do some small adjustments according to some user feedbacks and update the external dependencies (e.g. Confluent.Kafka 1.0.1). Fixes Fix exception loading error policies from JSON in Silverback.Integration.Configuration [ #24 ] 0.7.0 What's new Upgrade to Confluent.Kafka 1.0.0 Create a simple event store that perfectly integrates with the rest of the Silverback framework (see Event Sourcing ) Add Silverback.Integration.InMemory package to mock the message broker behavior in your unit tests Several small optimizations and improvements 0.6.0 What's new Add support for message headers Simplify message subscription even further: now all public methods of the types implementing the marker interface ISubscriber are automatically subscribed by default without having to annotate them with the <xref:Silverback.Messaging.Subscribers.SubscribeAttribute> (this behavior is customizable) Upgrade to Confluent.Kafka 1.0.0-RC1 0.3.x - 0.5.x Some releases where done adding quite a few features. What's new Add Silverback.Integration.Configuration package to load the inbound/outbound configuration from the app.settings json Add batch processing Add parallel subscribers Add delegate subscription as an alternative to <xref:Silverback.Messaging.Subscribers.SubscribeAttribute> based subscription Improve support for Rx.net Add support for legacy messages and POCO classes Add offset storage as an alternative and more optimized way to guarantee exactly once processing, storing just the offset of the last message instead of logging every message (see Inbound Endpoint ) Add behaviors as a convenient way to implement your cross-cutting concerns (like logging, validation, etc.) to be plugged into the internal bus publishing pipeline (see Behaviors ) Add message chunking to automatically split the larger messages and rebuild them on the other end (see Chunking ) ...much more...and a huge amount of refactorings Fixes Several fixes and optimizations 0.3.2 The very first public release of Silverback! It included: In-process message bus Inbound/outbound connector for message broker abstraction Kafka broker implementation Outbox table pattern implementation Exactly once processing ..."
  },
  "samples/kafka/avro.html": {
    "href": "samples/kafka/avro.html",
    "title": "Kafka - Avro | Silverback",
    "keywords": "Kafka - Avro This sample implements a producer and consumer which take advantage of the schema registry and serializes the messages as Avro. Common The message being exchanged is defined in a common project. // ------------------------------------------------------------------------------ // <auto-generated> // Generated by avrogen, version 1.7.7.5 // Changes to this file may cause incorrect behavior and will be lost if code // is regenerated // </auto-generated> // ------------------------------------------------------------------------------ using Silverback.Messaging.Messages; namespace Silverback.Examples.Messages { using System; using System.Collections.Generic; using System.Text; using global::Avro; using global::Avro.Specific; public partial class AvroMessage : ISpecificRecord { public static Schema _SCHEMA = Schema.Parse(\"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"AvroMessage\\\",\\\"namespace\\\":\\\"Silverback.Examples.Messages\\\",\" + \"\\\"fields\\\":[{\\\"name\\\":\\\"number\\\",\\\"type\\\":\\\"string\\\"}]}\"); private string _number; public virtual Schema Schema { get { return AvroMessage._SCHEMA; } } public string number { get { return this._number; } set { this._number = value; } } public virtual object Get(int fieldPos) { switch (fieldPos) { case 0: return this.number; default: throw new AvroRuntimeException(\"Bad index \" + fieldPos + \" in Get()\"); } } public virtual void Put(int fieldPos, object fieldValue) { switch (fieldPos) { case 0: this.number = (System.String) fieldValue; break; default: throw new AvroRuntimeException(\"Bad index \" + fieldPos + \" in Put()\"); } } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Kafka/Avro.Common Producer The producer uses a hosted service to publish some messages in the background. Startup EndpointsConfigurator Background Service using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Kafka.Avro.Producer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Kafka as message broker .WithConnectionToMessageBroker( options => options .AddKafka()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>(); // Add the hosted service that produces the random sample messages services.AddHostedService<ProducerBackgroundService>(); } public void Configure() { } } } using Silverback.Examples.Messages; using Silverback.Messaging.Configuration; namespace Silverback.Samples.Kafka.Avro.Producer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddKafkaEndpoints( endpoints => endpoints // Configure the properties needed by all consumers/producers .Configure( config => { // The bootstrap server address is needed to connect config.BootstrapServers = \"PLAINTEXT://localhost:9092\"; }) // Produce the AvroMessage to the samples-avro topic .AddOutbound<AvroMessage>( endpoint => endpoint .ProduceTo(\"samples-avro\") // Configure Avro serialization .SerializeAsAvro( avro => avro.Configure( schemaRegistry => { schemaRegistry.Url = \"localhost:8081\"; }, serializer => { serializer.AutoRegisterSchemas = true; })))); } } } using System; using System.Globalization; using System.Threading; using System.Threading.Tasks; using Microsoft.Extensions.DependencyInjection; using Microsoft.Extensions.Hosting; using Microsoft.Extensions.Logging; using Silverback.Examples.Messages; using Silverback.Messaging.Broker; using Silverback.Messaging.Publishing; namespace Silverback.Samples.Kafka.Avro.Producer { public class ProducerBackgroundService : BackgroundService { private readonly IServiceScopeFactory _serviceScopeFactory; private readonly ILogger<ProducerBackgroundService> _logger; public ProducerBackgroundService( IServiceScopeFactory serviceScopeFactory, ILogger<ProducerBackgroundService> logger) { _serviceScopeFactory = serviceScopeFactory; _logger = logger; } protected override async Task ExecuteAsync(CancellationToken stoppingToken) { // Create a service scope and resolve the IPublisher // (the IPublisher cannot be resolved from the root scope and cannot // therefore be directly injected into the BackgroundService) using var scope = _serviceScopeFactory.CreateScope(); var publisher = scope.ServiceProvider.GetRequiredService<IPublisher>(); var broker = scope.ServiceProvider.GetRequiredService<IBroker>(); int number = 0; while (!stoppingToken.IsCancellationRequested) { // Check whether the connection has been established, since the // BackgroundService will start immediately, before the application // is completely bootstrapped if (!broker.IsConnected) { await Task.Delay(100, stoppingToken); continue; } await ProduceMessageAsync(publisher, ++number); await Task.Delay(100, stoppingToken); } } private async Task ProduceMessageAsync(IPublisher publisher, int number) { try { await publisher.PublishAsync( new AvroMessage { number = number.ToString(CultureInfo.InvariantCulture) }); _logger.LogInformation(\"Produced {Number}\", number); } catch (Exception ex) { _logger.LogError(ex, \"Failed to produce {Number}\", number); } } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Kafka/Avro.Producer Consumer The consumer processes the messages and outputs their value to the standard output. Startup EndpointsConfigurator Subscriber using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Kafka.Avro.Consumer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Kafka as message broker .WithConnectionToMessageBroker( options => options .AddKafka()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>() // Register the subscribers .AddSingletonSubscriber<AvroMessageSubscriber>(); } public void Configure() { } } } using Confluent.Kafka; using Silverback.Examples.Messages; using Silverback.Messaging.Configuration; namespace Silverback.Samples.Kafka.Avro.Consumer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddKafkaEndpoints( endpoints => endpoints // Configure the properties needed by all consumers/producers .Configure( config => { // The bootstrap server address is needed to connect config.BootstrapServers = \"PLAINTEXT://localhost:9092\"; }) // Consume the samples-avro topic .AddInbound<AvroMessage>( endpoint => endpoint .ConsumeFrom(\"samples-avro\") // Configure Avro deserialization .DeserializeAvro( avro => avro.Configure( schemaRegistry => { schemaRegistry.Url = \"localhost:8081\"; })) .Configure( config => { // The consumer needs at least the bootstrap // server address and a group id to be able // to connect config.GroupId = \"sample-consumer\"; // AutoOffsetReset.Earliest means that the // consumer must start consuming from the // beginning of the topic, if no offset was // stored for this consumer group config.AutoOffsetReset = AutoOffsetReset.Earliest; }))); } } } using Microsoft.Extensions.Logging; using Silverback.Examples.Messages; namespace Silverback.Samples.Kafka.Avro.Consumer { public class AvroMessageSubscriber { private readonly ILogger<AvroMessageSubscriber> _logger; public AvroMessageSubscriber(ILogger<AvroMessageSubscriber> logger) { _logger = logger; } public void OnMessageReceived(AvroMessage message) => _logger.LogInformation(\"Received {MessageNumber}\", message.number); } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Kafka/Avro.Consumer"
  },
  "samples/kafka/basic.html": {
    "href": "samples/kafka/basic.html",
    "title": "Kafka - Basic | Silverback",
    "keywords": "Kafka - Basic This sample implements the simple possible producer and consumer. See also: Connecting to a Message Broker Common The message being exchanged is defined in a common project. namespace Silverback.Samples.Kafka.Basic.Common { public class SampleMessage { public int Number { get; set; } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Kafka/Basic.Common Producer The producer uses a hosted service to publish some messages in the background. Startup EndpointsConfigurator Background Service using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Kafka.Basic.Producer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Kafka as message broker .WithConnectionToMessageBroker( options => options .AddKafka()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>(); // Add the hosted service that produces the random sample messages services.AddHostedService<ProducerBackgroundService>(); } public void Configure() { } } } using Silverback.Messaging.Configuration; using Silverback.Samples.Kafka.Basic.Common; namespace Silverback.Samples.Kafka.Basic.Producer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddKafkaEndpoints( endpoints => endpoints // Configure the properties needed by all consumers/producers .Configure( config => { // The bootstrap server address is needed to connect config.BootstrapServers = \"PLAINTEXT://localhost:9092\"; }) // Produce the SampleMessage to the samples-basic topic .AddOutbound<SampleMessage>( endpoint => endpoint .ProduceTo(\"samples-basic\"))); } } } using System; using System.Threading; using System.Threading.Tasks; using Microsoft.Extensions.DependencyInjection; using Microsoft.Extensions.Hosting; using Microsoft.Extensions.Logging; using Silverback.Messaging.Broker; using Silverback.Messaging.Publishing; using Silverback.Samples.Kafka.Basic.Common; namespace Silverback.Samples.Kafka.Basic.Producer { public class ProducerBackgroundService : BackgroundService { private readonly IServiceScopeFactory _serviceScopeFactory; private readonly ILogger<ProducerBackgroundService> _logger; public ProducerBackgroundService( IServiceScopeFactory serviceScopeFactory, ILogger<ProducerBackgroundService> logger) { _serviceScopeFactory = serviceScopeFactory; _logger = logger; } protected override async Task ExecuteAsync(CancellationToken stoppingToken) { // Create a service scope and resolve the IPublisher // (the IPublisher cannot be resolved from the root scope and cannot // therefore be directly injected into the BackgroundService) using var scope = _serviceScopeFactory.CreateScope(); var publisher = scope.ServiceProvider.GetRequiredService<IPublisher>(); var broker = scope.ServiceProvider.GetRequiredService<IBroker>(); int number = 0; while (!stoppingToken.IsCancellationRequested) { // Check whether the connection has been established, since the // BackgroundService will start immediately, before the application // is completely bootstrapped if (!broker.IsConnected) { await Task.Delay(100, stoppingToken); continue; } await ProduceMessageAsync(publisher, ++number); await Task.Delay(100, stoppingToken); } } private async Task ProduceMessageAsync(IPublisher publisher, int number) { try { await publisher.PublishAsync( new SampleMessage { Number = number }); _logger.LogInformation(\"Produced {Number}\", number); } catch (Exception ex) { _logger.LogError(ex, \"Failed to produce {Number}\", number); } } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Kafka/Basic.Producer Consumer The consumer processes the messages and outputs their value to the standard output. Startup EndpointsConfigurator Subscriber using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Kafka.Basic.Consumer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Kafka as message broker .WithConnectionToMessageBroker( options => options .AddKafka()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>() // Register the subscribers .AddSingletonSubscriber<SampleMessageSubscriber>(); } public void Configure() { } } } using Confluent.Kafka; using Silverback.Messaging.Configuration; namespace Silverback.Samples.Kafka.Basic.Consumer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddKafkaEndpoints( endpoints => endpoints // Configure the properties needed by all consumers/producers .Configure( config => { // The bootstrap server address is needed to connect config.BootstrapServers = \"PLAINTEXT://localhost:9092\"; }) // Consume the samples-basic topic .AddInbound( endpoint => endpoint .ConsumeFrom(\"samples-basic\") .Configure( config => { // The consumer needs at least the bootstrap // server address and a group id to be able // to connect config.GroupId = \"sample-consumer\"; // AutoOffsetReset.Earliest means that the // consumer must start consuming from the // beginning of the topic, if no offset was // stored for this consumer group config.AutoOffsetReset = AutoOffsetReset.Earliest; }))); } } } using Microsoft.Extensions.Logging; using Silverback.Samples.Kafka.Basic.Common; namespace Silverback.Samples.Kafka.Basic.Consumer { public class SampleMessageSubscriber { private readonly ILogger<SampleMessageSubscriber> _logger; public SampleMessageSubscriber(ILogger<SampleMessageSubscriber> logger) { _logger = logger; } public void OnMessageReceived(SampleMessage message) => _logger.LogInformation(\"Received {MessageNumber}\", message.Number); } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Kafka/Basic.Consumer"
  },
  "samples/kafka/batch.html": {
    "href": "samples/kafka/batch.html",
    "title": "Kafka - Batch Processing | Silverback",
    "keywords": "Kafka - Batch Processing In this sample the consumed messages are processed in batch. See also: Inbound Endpoint - Batch processing Common The message being exchanged is defined in a common project. namespace Silverback.Samples.Kafka.Batch.Common { public class SampleMessage { public int Number { get; set; } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Kafka/Batch.Common Producer The producer uses a hosted service to publish some messages in the background. Startup EndpointsConfigurator Background Service using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Kafka.Batch.Producer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Kafka as message broker .WithConnectionToMessageBroker( options => options .AddKafka()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>(); // Add the hosted service that produces the random sample messages services.AddHostedService<ProducerBackgroundService>(); } public void Configure() { } } } using Silverback.Messaging.Configuration; using Silverback.Samples.Kafka.Batch.Common; namespace Silverback.Samples.Kafka.Batch.Producer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddKafkaEndpoints( endpoints => endpoints // Configure the properties needed by all consumers/producers .Configure( config => { // The bootstrap server address is needed to connect config.BootstrapServers = \"PLAINTEXT://localhost:9092\"; }) // Produce the SampleMessage to the samples-batch topic .AddOutbound<SampleMessage>( endpoint => endpoint .ProduceTo(\"samples-batch\"))); } } } using System; using System.Threading; using System.Threading.Tasks; using Microsoft.Extensions.DependencyInjection; using Microsoft.Extensions.Hosting; using Microsoft.Extensions.Logging; using Silverback.Messaging.Broker; using Silverback.Messaging.Publishing; using Silverback.Samples.Kafka.Batch.Common; namespace Silverback.Samples.Kafka.Batch.Producer { public class ProducerBackgroundService : BackgroundService { private readonly IServiceScopeFactory _serviceScopeFactory; private readonly ILogger<ProducerBackgroundService> _logger; public ProducerBackgroundService( IServiceScopeFactory serviceScopeFactory, ILogger<ProducerBackgroundService> logger) { _serviceScopeFactory = serviceScopeFactory; _logger = logger; } protected override async Task ExecuteAsync(CancellationToken stoppingToken) { // Create a service scope and resolve the IPublisher // (the IPublisher cannot be resolved from the root scope and cannot // therefore be directly injected into the BackgroundService) using var scope = _serviceScopeFactory.CreateScope(); var publisher = scope.ServiceProvider.GetRequiredService<IPublisher>(); var broker = scope.ServiceProvider.GetRequiredService<IBroker>(); int number = 0; while (!stoppingToken.IsCancellationRequested) { // Check whether the connection has been established, since the // BackgroundService will start immediately, before the application // is completely bootstrapped if (!broker.IsConnected) { await Task.Delay(100, stoppingToken); continue; } await ProduceMessageAsync(publisher, ++number); await Task.Delay(50, stoppingToken); } } private async Task ProduceMessageAsync(IPublisher publisher, int number) { try { await publisher.PublishAsync( new SampleMessage { Number = number }); _logger.LogInformation(\"Produced {Number}\", number); } catch (Exception ex) { _logger.LogError(ex, \"Failed to produce {Number}\", number); } } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Kafka/Batch.Producer Consumer The consumer processes the messages in batch and outputs the batch sum to the standard output. Startup EndpointsConfigurator Subscriber using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Kafka.Batch.Consumer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Kafka as message broker .WithConnectionToMessageBroker( options => options .AddKafka()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>() // Register the subscribers .AddSingletonSubscriber<SampleMessageBatchSubscriber>(); } public void Configure() { } } } using System; using Confluent.Kafka; using Silverback.Messaging.Configuration; namespace Silverback.Samples.Kafka.Batch.Consumer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddKafkaEndpoints( endpoints => endpoints // Configure the properties needed by all consumers/producers .Configure( config => { // The bootstrap server address is needed to connect config.BootstrapServers = \"PLAINTEXT://localhost:9092\"; }) // Consume the samples-batch topic .AddInbound( endpoint => endpoint .ConsumeFrom(\"samples-batch\") .Configure( config => { // The consumer needs at least the bootstrap // server address and a group id to be able // to connect config.GroupId = \"sample-consumer\"; // AutoOffsetReset.Earliest means that the // consumer must start consuming from the // beginning of the topic, if no offset was // stored for this consumer group config.AutoOffsetReset = AutoOffsetReset.Earliest; }) // Configure processing in batches of 100 messages, // with a max wait time of 5 seconds .EnableBatchProcessing(100, TimeSpan.FromSeconds(5)))); } } } using System.Collections.Generic; using System.Threading.Tasks; using Microsoft.Extensions.Logging; using Silverback.Samples.Kafka.Batch.Common; namespace Silverback.Samples.Kafka.Batch.Consumer { public class SampleMessageBatchSubscriber { private readonly ILogger<SampleMessageBatchSubscriber> _logger; public SampleMessageBatchSubscriber( ILogger<SampleMessageBatchSubscriber> logger) { _logger = logger; } public async Task OnBatchReceivedAsync(IAsyncEnumerable<SampleMessage> batch) { int sum = 0; int count = 0; await foreach (var message in batch) { sum += message.Number; count++; } _logger.LogInformation( \"Received batch of {Count} message -> sum: {Sum}\", count, sum); } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Kafka/Batch.Consumer"
  },
  "samples/kafka/binaryfile-streaming.html": {
    "href": "samples/kafka/binaryfile-streaming.html",
    "title": "Kafka - Files Streaming | Silverback",
    "keywords": "Kafka - Files Streaming This sample demonstrates how to deal with raw binary contents and large messages, to transfer some files through Kafka. See also: Binary Files , Chunking Producer The producer exposes two REST API that receive the path of a local file to be streamed. The second API uses a custom BinaryFileMessage to forward further metadata (the file name in this example). Startup EndpointsConfigurator CustomBinaryFileMessage API Controller using Microsoft.AspNetCore.Builder; using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Kafka.BinaryFileStreaming.Producer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Kafka as message broker .WithConnectionToMessageBroker( options => options .AddKafka()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>(); // Add API controllers and SwaggerGen services.AddControllers(); services.AddSwaggerGen(); } public void Configure(IApplicationBuilder app) { // Enable middlewares to serve generated Swagger JSON and UI app.UseSwagger().UseSwaggerUI( uiOptions => { uiOptions.SwaggerEndpoint( \"/swagger/v1/swagger.json\", $\"{GetType().Assembly.FullName} API\"); }); // Enable routing and endpoints for controllers app.UseRouting(); app.UseEndpoints(endpoints => { endpoints.MapControllers(); }); } } } using Silverback.Messaging.Configuration; using Silverback.Messaging.Messages; namespace Silverback.Samples.Kafka.BinaryFileStreaming.Producer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddKafkaEndpoints( endpoints => endpoints // Configure the properties needed by all consumers/producers .Configure( config => { // The bootstrap server address is needed to connect config.BootstrapServers = \"PLAINTEXT://localhost:9092\"; }) // Produce the binary files to the // samples-binary-file-streaming topic .AddOutbound<BinaryFileMessage>( endpoint => endpoint // Force producing to a specific partition (0 in this // case) to be able to scale to multiple producers // writing to the same topic. Assigning a different // partition to each one will ensure that the chunks // are always contiguous. // This isn't mandatory and necessary only when // horizontally scaling the producer. // (In the final solution the \"0\" constant value // should be replaced by a configuration setting.) .ProduceTo(\"samples-binary-file-streaming\", 0) // Split the binary files into chunks of 512 kB .EnableChunking(524288))); } } } using Silverback.Messaging.Messages; namespace Silverback.Samples.Kafka.BinaryFileStreaming.Producer.Messages { public class CustomBinaryFileMessage : BinaryFileMessage { [Header(\"x-filename\")] public string? Filename { get; set; } } } using System.IO; using System.Threading.Tasks; using Microsoft.AspNetCore.Mvc; using Silverback.Messaging.Messages; using Silverback.Messaging.Publishing; using Silverback.Samples.Kafka.BinaryFileStreaming.Producer.Messages; namespace Silverback.Samples.Kafka.BinaryFileStreaming.Producer.Controllers { [ApiController] [Route(\"[controller]\")] public class ProducerController : ControllerBase { private readonly IPublisher _publisher; public ProducerController(IPublisher publisher) { _publisher = publisher; } [HttpPost(\"binary-file\")] public async Task<IActionResult> ProduceBinaryFileAsync( string filePath, string? contentType) { // Open specified file stream using var fileStream = System.IO.File.OpenRead(filePath); // Create a BinaryFileMessage that wraps the file stream var binaryFileMessage = new BinaryFileMessage(fileStream); if (!string.IsNullOrEmpty(contentType)) binaryFileMessage.ContentType = contentType; // Publish the BinaryFileMessage that will be routed to the outbound // endpoint. The FileStream will be read and produced chunk by chunk, // without the entire file being loaded into memory. await _publisher.PublishAsync(binaryFileMessage); return NoContent(); } [HttpPost(\"custom-binary-file\")] public async Task<IActionResult> ProduceBinaryFileWithCustomHeadersAsync( string filePath, string? contentType) { // Open specified file stream using var fileStream = System.IO.File.OpenRead(filePath); // Create a CustomBinaryFileMessage that wraps the file stream. The // CustomBinaryFileMessage extends the BinaryFileMessage adding an extra // Filename property that is also exported as header. var binaryFileMessage = new CustomBinaryFileMessage { Content = fileStream, Filename = Path.GetFileName(filePath) }; if (!string.IsNullOrEmpty(contentType)) binaryFileMessage.ContentType = contentType; // Publish the BinaryFileMessage that will be routed to the outbound // endpoint. The FileStream will be read and produced chunk by chunk, // without the entire file being loaded into memory. await _publisher.PublishAsync(binaryFileMessage); return NoContent(); } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Kafka/BinaryFileStreaming.Producer Consumer The consumer simply streams the file to a temporary folder in the local file system. Startup EndpointsConfigurator CustomBinaryFileMessage Subscriber using Microsoft.Extensions.DependencyInjection; using Silverback.Samples.Kafka.BinaryFileStreaming.Consumer.Subscribers; namespace Silverback.Samples.Kafka.BinaryFileStreaming.Consumer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Kafka as message broker .WithConnectionToMessageBroker( options => options .AddKafka()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>() // Register the subscribers .AddSingletonSubscriber<BinaryFileSubscriber>(); } public void Configure() { } } } using Confluent.Kafka; using Silverback.Messaging.Configuration; using Silverback.Samples.Kafka.BinaryFileStreaming.Consumer.Messages; namespace Silverback.Samples.Kafka.BinaryFileStreaming.Consumer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddKafkaEndpoints( endpoints => endpoints // Configure the properties needed by all consumers/producers .Configure( config => { // The bootstrap server address is needed to connect config.BootstrapServers = \"PLAINTEXT://localhost:9092\"; }) // Consume the samples-binary-file-streaming topic .AddInbound( endpoint => endpoint // Manually assign the partitions to prevent the // broker to rebalance in the middle of a potentially // huge sequence of chunks. This is just an // optimization and isn't strictly necessary. // (The partitions resolver function returns the // untouched collection to assign all available // partitions.) .ConsumeFrom( \"samples-binary-file-streaming\", partitions => partitions) .Configure( config => { // The consumer needs at least the bootstrap // server address and a group id to be able // to connect config.GroupId = \"sample-consumer\"; // AutoOffsetReset.Earliest means that the // consumer must start consuming from the // beginning of the topic, if no offset was // stored for this consumer group config.AutoOffsetReset = AutoOffsetReset.Earliest; }) // Force the consumer to use the // BinaryFileMessageSerializer: this is not strictly // necessary when the messages are produced by // Silverback but it increases the interoperability, // since it doesn't have to rely on the // 'x-message-type' header value to switch to the // BinaryFileMessageSerializer. // // In this example the BinaryFileMessageSerializer is // also set to return a CustomBinaryFileMessage // instead of the normal BinaryFileMessage. This is // only needed because we want to read the custom // 'x-message-filename' header, otherwise // 'ConsumeBinaryFiles()' would work perfectly fine // (returning a basic BinaryFileMessage, without the // extra properties). .ConsumeBinaryFiles( serializer => serializer .UseModel<CustomBinaryFileMessage>()) // Retry each chunks sequence 5 times in case of an // exception .OnError(policy => policy.Retry(5)))); } } } using Silverback.Messaging.Messages; namespace Silverback.Samples.Kafka.BinaryFileStreaming.Consumer.Messages { public class CustomBinaryFileMessage : BinaryFileMessage { [Header(\"x-filename\")] public string? Filename { get; set; } } } using System; using System.IO; using System.Threading.Tasks; using Microsoft.Extensions.Logging; using Silverback.Samples.Kafka.BinaryFileStreaming.Consumer.Messages; namespace Silverback.Samples.Kafka.BinaryFileStreaming.Consumer.Subscribers { public class BinaryFileSubscriber { private const string OutputPath = \"../../temp\"; private readonly ILogger<BinaryFileSubscriber> _logger; public BinaryFileSubscriber(ILogger<BinaryFileSubscriber> logger) { _logger = logger; } public async Task OnBinaryFileMessageReceivedAsync( CustomBinaryFileMessage binaryFileMessage) { EnsureTargetFolderExists(); var filename = Guid.NewGuid().ToString(\"N\") + binaryFileMessage.Filename; _logger.LogInformation(\"Saving binary file as {Filename}...\", filename); // Create a FileStream to save the file using var fileStream = File.OpenWrite(Path.Combine(OutputPath, filename)); if (binaryFileMessage.Content != null) { // Asynchronously copy the message content to the FileStream. // The message chunks are streamed directly and the entire file is // never loaded into memory. await binaryFileMessage.Content.CopyToAsync(fileStream); } _logger.LogInformation( \"Written {FileStreamLength} bytes into {Filename}\", fileStream.Length, filename); } private static void EnsureTargetFolderExists() { if (!Directory.Exists(OutputPath)) Directory.CreateDirectory(OutputPath); } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Kafka/BinaryFileStreaming.Consumer"
  },
  "samples/mqtt/basic.html": {
    "href": "samples/mqtt/basic.html",
    "title": "MQTT - Basic | Silverback",
    "keywords": "MQTT - Basic This sample implements the simple possible producer and consumer. See also: Connecting to a Message Broker Common The message being exchanged is defined in a common project. namespace Silverback.Samples.Mqtt.Basic.Common { public class SampleMessage { public int Number { get; set; } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Mqtt/Basic.Common Producer The producer uses a hosted service to publish some messages in the background. Startup EndpointsConfigurator Background Service using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Mqtt.Basic.Producer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Mqtt as message broker .WithConnectionToMessageBroker( options => options .AddMqtt()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>(); // Add the hosted service that produces the random sample messages services.AddHostedService<ProducerBackgroundService>(); } public void Configure() { } } } using MQTTnet.Protocol; using Silverback.Messaging.Configuration; using Silverback.Samples.Mqtt.Basic.Common; namespace Silverback.Samples.Mqtt.Basic.Producer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddMqttEndpoints( endpoints => endpoints // Configure the client options .Configure( config => config .WithClientId(\"samples.basic.producer\") .ConnectViaTcp(\"localhost\")) // Produce the SampleMessage to the samples-basic topic .AddOutbound<SampleMessage>( endpoint => endpoint .ProduceTo(\"samples/basic\") .WithQualityOfServiceLevel( MqttQualityOfServiceLevel.AtLeastOnce) .Retain())); } } } using System; using System.Threading; using System.Threading.Tasks; using Microsoft.Extensions.DependencyInjection; using Microsoft.Extensions.Hosting; using Microsoft.Extensions.Logging; using Silverback.Messaging.Broker; using Silverback.Messaging.Publishing; using Silverback.Samples.Mqtt.Basic.Common; namespace Silverback.Samples.Mqtt.Basic.Producer { public class ProducerBackgroundService : BackgroundService { private readonly IServiceScopeFactory _serviceScopeFactory; private readonly ILogger<ProducerBackgroundService> _logger; public ProducerBackgroundService( IServiceScopeFactory serviceScopeFactory, ILogger<ProducerBackgroundService> logger) { _serviceScopeFactory = serviceScopeFactory; _logger = logger; } protected override async Task ExecuteAsync(CancellationToken stoppingToken) { // Create a service scope and resolve the IPublisher // (the IPublisher cannot be resolved from the root scope and cannot // therefore be directly injected into the BackgroundService) using var scope = _serviceScopeFactory.CreateScope(); var publisher = scope.ServiceProvider.GetRequiredService<IPublisher>(); var broker = scope.ServiceProvider.GetRequiredService<IBroker>(); int number = 0; while (!stoppingToken.IsCancellationRequested) { // Check whether the connection has been established, since the // BackgroundService will start immediately, before the application // is completely bootstrapped if (!broker.IsConnected) { await Task.Delay(100, stoppingToken); continue; } await ProduceMessageAsync(publisher, ++number); await Task.Delay(100, stoppingToken); } } private async Task ProduceMessageAsync(IPublisher publisher, int number) { try { await publisher.PublishAsync( new SampleMessage { Number = number }); _logger.LogInformation(\"Produced {Number}\", number); } catch (Exception ex) { _logger.LogError(ex, \"Failed to produce {Number}\", number); } } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Mqtt/Basic.Producer Consumer The consumer processes the messages and outputs their value to the standard output. Startup EndpointsConfigurator Subscriber using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Mqtt.Basic.Consumer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Mqtt as message broker .WithConnectionToMessageBroker( options => options .AddMqtt()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>() // Register the subscribers .AddSingletonSubscriber<SampleMessageSubscriber>(); } public void Configure() { } } } using MQTTnet.Protocol; using Silverback.Messaging.Configuration; using Silverback.Samples.Mqtt.Basic.Common; namespace Silverback.Samples.Mqtt.Basic.Consumer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddMqttEndpoints( endpoints => endpoints // Configure the client options .Configure( config => config .WithClientId(\"samples.basic.consumer\") .ConnectViaTcp(\"localhost\") // Send last will message if disconnecting // ungracefully .SendLastWillMessage( lastWill => lastWill .Message(new TestamentMessage()) .ProduceTo(\"samples/testaments\"))) // Consume the samples/basic topic .AddInbound( endpoint => endpoint .ConsumeFrom(\"samples/basic\") .WithQualityOfServiceLevel( MqttQualityOfServiceLevel.AtLeastOnce) // Silently skip the messages in case of exception .OnError(policy => policy.Skip()))); } } } using Microsoft.Extensions.Logging; using Silverback.Samples.Mqtt.Basic.Common; namespace Silverback.Samples.Mqtt.Basic.Consumer { public class SampleMessageSubscriber { private readonly ILogger<SampleMessageSubscriber> _logger; public SampleMessageSubscriber(ILogger<SampleMessageSubscriber> logger) { _logger = logger; } public void OnMessageReceived(SampleMessage message) => _logger.LogInformation(\"Received {MessageNumber}\", message.Number); } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Mqtt/Basic.Consumer"
  },
  "samples/mqtt/basic-v3.html": {
    "href": "samples/mqtt/basic-v3.html",
    "title": "MQTT - Basic (v3) | Silverback",
    "keywords": "MQTT - Basic (v3) This sample implements the simple possible producer and consumer, but using MQTT protocol version 3. See also: Connecting to a Message Broker Common The message being exchanged is defined in a common project. namespace Silverback.Samples.Mqtt.Basic.Common { public class SampleMessage { public int Number { get; set; } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Mqtt/Basic.Common Producer The producer uses a hosted service to publish some messages in the background. Startup EndpointsConfigurator Background Service using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Mqtt.Basic.ProducerV3 { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Mqtt as message broker .WithConnectionToMessageBroker( options => options .AddMqtt()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>(); // Add the hosted service that produces the random sample messages services.AddHostedService<ProducerBackgroundService>(); } public void Configure() { } } } using MQTTnet.Formatter; using MQTTnet.Protocol; using Silverback.Messaging.Configuration; using Silverback.Samples.Mqtt.Basic.Common; namespace Silverback.Samples.Mqtt.Basic.ProducerV3 { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddMqttEndpoints( endpoints => endpoints // Configure the client options .Configure( config => config .WithClientId(\"samples.basic.producer\") .ConnectViaTcp(\"localhost\") .UseProtocolVersion(MqttProtocolVersion.V310)) // Produce the SampleMessage to the samples-basic topic .AddOutbound<SampleMessage>( endpoint => endpoint .ProduceTo(\"samples/basic\") .WithQualityOfServiceLevel( MqttQualityOfServiceLevel.AtLeastOnce) .Retain())); } } } using System; using System.Threading; using System.Threading.Tasks; using Microsoft.Extensions.DependencyInjection; using Microsoft.Extensions.Hosting; using Microsoft.Extensions.Logging; using Silverback.Messaging.Broker; using Silverback.Messaging.Publishing; using Silverback.Samples.Mqtt.Basic.Common; namespace Silverback.Samples.Mqtt.Basic.ProducerV3 { public class ProducerBackgroundService : BackgroundService { private readonly IServiceScopeFactory _serviceScopeFactory; private readonly ILogger<ProducerBackgroundService> _logger; public ProducerBackgroundService( IServiceScopeFactory serviceScopeFactory, ILogger<ProducerBackgroundService> logger) { _serviceScopeFactory = serviceScopeFactory; _logger = logger; } protected override async Task ExecuteAsync(CancellationToken stoppingToken) { // Create a service scope and resolve the IPublisher // (the IPublisher cannot be resolved from the root scope and cannot // therefore be directly injected into the BackgroundService) using var scope = _serviceScopeFactory.CreateScope(); var publisher = scope.ServiceProvider.GetRequiredService<IPublisher>(); var broker = scope.ServiceProvider.GetRequiredService<IBroker>(); int number = 0; while (!stoppingToken.IsCancellationRequested) { // Check whether the connection has been established, since the // BackgroundService will start immediately, before the application // is completely bootstrapped if (!broker.IsConnected) { await Task.Delay(100, stoppingToken); continue; } await ProduceMessageAsync(publisher, ++number); await Task.Delay(100, stoppingToken); } } private async Task ProduceMessageAsync(IPublisher publisher, int number) { try { await publisher.PublishAsync( new SampleMessage { Number = number }); _logger.LogInformation(\"Produced {Number}\", number); } catch (Exception ex) { _logger.LogError(ex, \"Failed to produce {Number}\", number); } } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Mqtt/Basic.ProducerV3 Consumer The consumer processes the messages and outputs their value to the standard output. Startup EndpointsConfigurator Subscriber using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Mqtt.Basic.ConsumerV3 { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Mqtt as message broker .WithConnectionToMessageBroker( options => options .AddMqtt()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>() // Register the subscribers .AddSingletonSubscriber<SampleMessageSubscriber>(); } public void Configure() { } } } using MQTTnet.Formatter; using MQTTnet.Protocol; using Silverback.Messaging.Configuration; using Silverback.Samples.Mqtt.Basic.Common; namespace Silverback.Samples.Mqtt.Basic.ConsumerV3 { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddMqttEndpoints( endpoints => endpoints // Configure the client options .Configure( config => config .WithClientId(\"samples.basic.consumer\") .ConnectViaTcp(\"localhost\") .UseProtocolVersion(MqttProtocolVersion.V310) // Send last will message if disconnecting // ungracefully .SendLastWillMessage( lastWill => lastWill .Message(new TestamentMessage()) .ProduceTo(\"samples/testaments\"))) // Consume the samples/basic topic // Note: It is mandatory to specify the message type, since // MQTT 3 doesn't support message headers (aka user // properties) .AddInbound<SampleMessage>( endpoint => endpoint .ConsumeFrom(\"samples/basic\") .WithQualityOfServiceLevel( MqttQualityOfServiceLevel.AtLeastOnce) // Silently skip the messages in case of exception .OnError(policy => policy.Skip()))); } } } using Microsoft.Extensions.Logging; using Silverback.Samples.Mqtt.Basic.Common; namespace Silverback.Samples.Mqtt.Basic.ConsumerV3 { public class SampleMessageSubscriber { private readonly ILogger<SampleMessageSubscriber> _logger; public SampleMessageSubscriber(ILogger<SampleMessageSubscriber> logger) { _logger = logger; } public void OnMessageReceived(SampleMessage message) => _logger.LogInformation(\"Received {MessageNumber}\", message.Number); } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Mqtt/Basic.ConsumerV3"
  },
  "samples/mqtt/binaryfile-streaming.html": {
    "href": "samples/mqtt/binaryfile-streaming.html",
    "title": "MQTT - Files Streaming | Silverback",
    "keywords": "MQTT - Files Streaming This sample demonstrates how to deal with raw binary contents and large messages, to transfer some files through Mqtt. See also: Binary Files , Chunking Producer The producer exposes two REST API that receive the path of a local file to be streamed. The second API uses a custom BinaryFileMessage to forward further metadata (the file name in this example). Startup EndpointsConfigurator CustomBinaryFileMessage API Controller using Microsoft.AspNetCore.Builder; using Microsoft.Extensions.DependencyInjection; namespace Silverback.Samples.Mqtt.BinaryFileStreaming.Producer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Mqtt as message broker .WithConnectionToMessageBroker( options => options .AddMqtt()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>(); // Add API controllers and SwaggerGen services.AddControllers(); services.AddSwaggerGen(); } public void Configure(IApplicationBuilder app) { // Enable middlewares to serve generated Swagger JSON and UI app.UseSwagger().UseSwaggerUI( uiOptions => { uiOptions.SwaggerEndpoint( \"/swagger/v1/swagger.json\", $\"{GetType().Assembly.FullName} API\"); }); // Enable routing and endpoints for controllers app.UseRouting(); app.UseEndpoints(endpoints => { endpoints.MapControllers(); }); } } } using MQTTnet.Protocol; using Silverback.Messaging.Configuration; using Silverback.Messaging.Messages; namespace Silverback.Samples.Mqtt.BinaryFileStreaming.Producer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddMqttEndpoints( endpoints => endpoints // Configure the client options .Configure( config => config .WithClientId(\"samples.binary-file.producer\") .ConnectViaTcp(\"localhost\")) // Produce the binary files to the // samples-binary-file-streaming topic .AddOutbound<BinaryFileMessage>( endpoint => endpoint .ProduceTo(\"samples/binary-files\") .WithQualityOfServiceLevel( MqttQualityOfServiceLevel.AtLeastOnce))); } } } using Silverback.Messaging.Messages; namespace Silverback.Samples.Mqtt.BinaryFileStreaming.Producer.Messages { public class CustomBinaryFileMessage : BinaryFileMessage { [Header(\"x-filename\")] public string? Filename { get; set; } } } using System.IO; using System.Threading.Tasks; using Microsoft.AspNetCore.Mvc; using Silverback.Messaging.Messages; using Silverback.Messaging.Publishing; using Silverback.Samples.Mqtt.BinaryFileStreaming.Producer.Messages; namespace Silverback.Samples.Mqtt.BinaryFileStreaming.Producer.Controllers { [ApiController] [Route(\"[controller]\")] public class ProducerController : ControllerBase { private readonly IPublisher _publisher; public ProducerController(IPublisher publisher) { _publisher = publisher; } [HttpPost(\"binary-file\")] public async Task<IActionResult> ProduceBinaryFileAsync( string filePath, string? contentType) { // Open specified file stream using var fileStream = System.IO.File.OpenRead(filePath); // Create a BinaryFileMessage that wraps the file stream var binaryFileMessage = new BinaryFileMessage(fileStream); if (!string.IsNullOrEmpty(contentType)) binaryFileMessage.ContentType = contentType; // Publish the BinaryFileMessage that will be routed to the outbound // endpoint. The FileStream will be read and produced chunk by chunk, // without the entire file being loaded into memory. await _publisher.PublishAsync(binaryFileMessage); return NoContent(); } [HttpPost(\"custom-binary-file\")] public async Task<IActionResult> ProduceBinaryFileWithCustomHeadersAsync( string filePath, string? contentType) { // Open specified file stream using var fileStream = System.IO.File.OpenRead(filePath); // Create a CustomBinaryFileMessage that wraps the file stream. The // CustomBinaryFileMessage extends the BinaryFileMessage adding an extra // Filename property that is also exported as header. var binaryFileMessage = new CustomBinaryFileMessage { Content = fileStream, Filename = Path.GetFileName(filePath) }; if (!string.IsNullOrEmpty(contentType)) binaryFileMessage.ContentType = contentType; // Publish the BinaryFileMessage that will be routed to the outbound // endpoint. The FileStream will be read and produced chunk by chunk, // without the entire file being loaded into memory. await _publisher.PublishAsync(binaryFileMessage); return NoContent(); } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Mqtt/BinaryFileStreaming.Producer Consumer The consumer simply streams the file to a temporary folder in the local file system. Startup EndpointsConfigurator CustomBinaryFileMessage Subscriber using Microsoft.Extensions.DependencyInjection; using Silverback.Samples.Mqtt.BinaryFileStreaming.Consumer.Subscribers; namespace Silverback.Samples.Mqtt.BinaryFileStreaming.Consumer { public class Startup { public void ConfigureServices(IServiceCollection services) { // Enable Silverback services .AddSilverback() // Use Apache Mqtt as message broker .WithConnectionToMessageBroker( options => options .AddMqtt()) // Delegate the inbound/outbound endpoints configuration to a separate // class. .AddEndpointsConfigurator<EndpointsConfigurator>() // Register the subscribers .AddSingletonSubscriber<BinaryFileSubscriber>(); } public void Configure() { } } } using MQTTnet.Protocol; using Silverback.Messaging.Configuration; using Silverback.Samples.Mqtt.BinaryFileStreaming.Consumer.Messages; namespace Silverback.Samples.Mqtt.BinaryFileStreaming.Consumer { public class EndpointsConfigurator : IEndpointsConfigurator { public void Configure(IEndpointsConfigurationBuilder builder) { builder .AddMqttEndpoints( endpoints => endpoints // Configure the client options .Configure( config => config .WithClientId(\"samples.binary-file.consumer\") .ConnectViaTcp(\"localhost\")) // Consume the samples-binary-file-streaming topic .AddInbound( endpoint => endpoint .ConsumeFrom(\"samples/binary-files\") .WithQualityOfServiceLevel( MqttQualityOfServiceLevel.AtLeastOnce) // Force the consumer to use the // BinaryFileMessageSerializer: this is not strictly // necessary when the messages are produced by // Silverback but it increases the interoperability, // since it doesn't have to rely on the // 'x-message-type' header value to switch to the // BinaryFileMessageSerializer. // // In this example the BinaryFileMessageSerializer is // also set to return a CustomBinaryFileMessage // instead of the normal BinaryFileMessage. This is // only needed because we want to read the custom // 'x-message-filename' header, otherwise // 'ConsumeBinaryFiles()' would work perfectly fine // (returning a basic BinaryFileMessage, without the // extra properties). .ConsumeBinaryFiles( serializer => serializer .UseModel<CustomBinaryFileMessage>()) // Retry each chunks sequence 5 times in case of an // exception .OnError(policy => policy.Retry(5)))); } } } using Silverback.Messaging.Messages; namespace Silverback.Samples.Mqtt.BinaryFileStreaming.Consumer.Messages { public class CustomBinaryFileMessage : BinaryFileMessage { [Header(\"x-filename\")] public string? Filename { get; set; } } } using System; using System.IO; using System.Threading.Tasks; using Microsoft.Extensions.Logging; using Silverback.Samples.Mqtt.BinaryFileStreaming.Consumer.Messages; namespace Silverback.Samples.Mqtt.BinaryFileStreaming.Consumer.Subscribers { public class BinaryFileSubscriber { private const string OutputPath = \"../../temp\"; private readonly ILogger<BinaryFileSubscriber> _logger; public BinaryFileSubscriber(ILogger<BinaryFileSubscriber> logger) { _logger = logger; } public async Task OnBinaryFileMessageReceivedAsync( CustomBinaryFileMessage binaryFileMessage) { EnsureTargetFolderExists(); var filename = Guid.NewGuid().ToString(\"N\") + binaryFileMessage.Filename; _logger.LogInformation(\"Saving binary file as {Filename}...\", filename); // Create a FileStream to save the file using var fileStream = File.OpenWrite(Path.Combine(OutputPath, filename)); if (binaryFileMessage.Content != null) { // Asynchronously copy the message content to the FileStream. // The message chunks are streamed directly and the entire file is // never loaded into memory. await binaryFileMessage.Content.CopyToAsync(fileStream); } _logger.LogInformation( \"Written {FileStreamLength} bytes into {Filename}\", fileStream.Length, filename); } private static void EnsureTargetFolderExists() { if (!Directory.Exists(OutputPath)) Directory.CreateDirectory(OutputPath); } } } Full source code: https://github.com/BEagle1984/silverback/tree/master/samples/Mqtt/BinaryFileStreaming.Consumer"
  },
  "samples/samples.html": {
    "href": "samples/samples.html",
    "title": "Samples | Silverback",
    "keywords": "Samples In this section you find a collection of working samples based on Silverback and implementing a wide range of different use cases. The full source code can be found in the GitHub repository under the /samples folder. A README.md in the root of the samples folder explains how to run each of the samples."
  }
}