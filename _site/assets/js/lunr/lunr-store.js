var store = [{
        "title": "Introduction",
        "excerpt":"What’s Silverback?   Silverback is basically two things:     a message bus (actually a mediator) that can be used to decouple layers or components inside an application   an abstraction over a message broker like Apache Kafka or RabbitMQ.   Combining those two fundamental pieces allows to build reactive and resilient microservices, using a very simple and familiar programming model.    \t \tSilverback is used to produce the messages 1 and 3 to the message broker, while the messages 2 and 3 are also consumed locally, within the same application.   Packages   Silverback is modular and delivered in multiple packages, available through nuget.org.   Core   Silverback.Core   It implements a very simple, yet very effective, publish/subscribe in-memory bus that can be used to decouple the software parts and easily implement a Domain Driven Design approach.       Silverback.Core.Model   It contains some interfaces that will help organize the messages and write cleaner code, adding some semantic. It also includes a sample implementation of a base class for your domain entities.       Silverback.Core.EntityFrameworkCore   It contains the storage implementation to integrate Silverback with Entity Framework Core. It is needed to use a DbContext as storage for (temporary) data and to fire the domain events as part of the SaveChanges transaction.       Silverback.Core.Rx   Adds the possibility to create an Rx Observable over the internal bus.       Integration   Silverback.Integration   Contains the message broker and connectors abstraction. Inbound and outbound connectors can be attached to a message broker to either export some events/commands/messages to other microservices or react to the messages fired by other microservices in the same way as internal messages are handled.       Silverback.Integration.Kafka   An implementation of Silverback.Integration for the popular Apache Kafka message broker. It internally uses the Confluent.Kafka client library.       Silverback.Integration.Kafka.SchemaRegistry   Adds the support for Apache Avro and the schema registry on top of Silverback.Integration.Kafka.       Silverback.Integration.RabbitMQ   An implementation of Silverback.Integration for the popular RabbitMQ message broker. It internally uses the RabbitMQ.Client library.       Silverback.Integration.InMemory   Includes a mocked message broker to be used for testing only.       Silverback.Integration.Configuration   Contains the logic to read the broker endpoints configuration from the IConfiguration from Microsoft.Extensions.Configuration (appsettings.json, environment variables, etc.)       Silverback.Integration.HealthChecks   Contains the extensions for Microsoft.Extensions.Diagnostics.HealthChecks to monitor the connection to the message broker.       Event Sourcing   Silverback.EventSourcing   Contains an implementation of an event store that perfectly integrates within the Silverback ecosystem.       Read more   Have a look at the quickstart to see how simple it is to start working with it and how much you can achieve with very few lines of code.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/introduction",
        "teaser":null},{
        "title": "Features",
        "excerpt":"Silverback is a simple but feature rich framework to build reactive/event-driven applications or microservices   It includes an in-memory message bus that can be easily connected to a message broker to integrate with other applications or microservices. At the moment it supports Apache Kafka and RabbitMQ and other message brokers might be added in the future.   Some of its features are highlighted in the following chapters.   Simple yet powerful message bus   Enabling the bus is as simple as referencing a nuget package and adding a single line of code to your startup class.   The provided in-memory message bus is very flexible and can be used for a multitude of use cases.   Silverback also ships with native support for Rx.net (System.Reactive).   Message broker abstraction   The message broker integration happens configuratively at startup and it is then completely abstracted. The integration messages are published to internal bus as any other message  and Silverback takes care of deliveryng them to the correct endpoint, so that your code remains very clean and no detail about the message broker can leak into it.   Apache Kafka and RabbitMQ integration   Silverback provides a package to connect with the very popular Apache Kafka message broker or the equally popular RabbitMQ.   Integrating other message brokers wouldn’t be a big deal and some may be added in the future…or feel free to create your own IBroker implementation.   DDD and transactional messaging   One of the main challenges when adopting a microservices architecture and asynchronous messaging is atomically updating the database and sending the messages to notify the other microservices. Silverback solves this problem for you with the built-in ability to store the messages published by your domain entities in a temporary outbox table, updated as part of your regular transaction.   Silverback integrates of seemlessly with EntityFramework Core (but could be extended to other ORMs).   Error handling policies   Sooner or later you will run into an issue with a message that cannot be processed and you therefore have to handle the exception and decide what to do with the message. With Silverback you can configuratively specify the error handling policies for each inbound connector. The built-in policies are:     Skip: simply ingnore the message   Retry: retry the same message (delays can be specified)   Move: move the message to another topic/queue (or re-enqueue it at the end of the same one) We believe that combining this three policies you will be able to implement pretty much all use cases.   Distributed tracing   Silverback integrates with System.Diagnostics to ensure the entire flow can easily be traced, also when involving a message broker.   Modularity   Silverback is modular and shipped in multiple nuget packages to allow you to depend only on the parts you want to use.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/features",
        "teaser":null},{
        "title": "Releases",
        "excerpt":"3.0.0 (beta)   See the new (beta) website   2.2.0   What’s new     Allow custom outbound routers to be registered as scoped or transient (instead of singleton only)   2.1.1   What’s new     Multiple message brokers (Kafka and RabbitMQ) can be used together in the same application (see Connecting to a Message Broker)   End-to-End message encryption (see Encryption)   Dynamic custom routing of outbound messages (see Outbound Connector)   Better support for message headers (see Message Headers)   Binary files support (see Binary Files)   The IIntegrationMessage is not required to have an Id property anymore (the x-message-id header will still be generated and if the property exists will continue to be automatically initialized)   x-first-chunk-offset header added by default (see Message Headers)    The KafkaStasticsEvent JSON is now being deserialized and provided as object (in addition to the raw JSON)    Added support for Apache Avro and schema registry (see Serialization)    Upgrade to Confluent.Kafka 1.4.2    Added consumer PrefetchSize and PrefetchCount settings (see Endpoint)    Added AcknowledgeEach to the RabbitConsumerEndpoint to define the number of message processed before sending the acknowledgment to the server (see Endpoint)    Upgrade to RabbitMQ.Client 6.0.0   Improved message type resolution performance and reliability in JsonMessageSerializer   LogWithLevel method added to SkipMessageErrorPolicy to specify the desired level for the “Message skipped” log entry (the default is now increased to Error)   Breaking Changes  These changes shouldn’t affect you unless you built your own IBroker implementation or are interacting at low-level with the IBroker (this is why has been decided to still mark this as a minor release):     The IBroker inteface and Broker abstract base class have been modified to explicitly declare which endpoint type is being handled by the broker implementation   The IMessageSerializer interfaces has been changed   The IConsumerBehavior and IProducerBehavior interfaces have been changed and moved into Integration.Broker.Behaviors namespace   Changed the parameters order in some less used overloads in the IBrokerOptionBuilder   Announced Breaking Changes  These aren’t real breaking changes but some methods have been marked as deprecated and will be removed in one of the next major releases:     WithConnectionTo&lt;&gt;, WithConnectionToKafka and WithConnectionToRabbitMQ are deprecated (they will still be supported in this version), please use the new WithConnectionToMessageBroker and AddKafka/AddRabbit methods (see Connecting to a Message Broker)   2.0.0   What’s new     Created Silverback.Integration.RabbitMQ package to connect Silverback with RabbitMQ (see Connecting to a Message Broker)   Messages with an empty body can now be subscribed (you must subscribe to the IInboundEnvelope) [#61]   The Kafka partition start offset can now be manually set when a partition is assigned to the consumer (see Kafka Events) [#57]   Full support for multiple consumer groups running in the same process (see Multiple Consumer Groups (in same process)) [#59]   A KafkaStatisticsEvents is published also by the KafkaPRoducer (previously done in KafkaConsumer only)   Several reliability and performance related improvements   Breaking Changes     The IBroker, IProducer and IConsumer interfaces have been slightly modified (it shouldn’t affect you unless you built your own IBroker implementation)   Many interfaces (such as IBehavior) and delegates have been sligthly modified to pass around an IReadOnlyCollection&lt;T&gt; instead of an IEnumerable&lt;T&gt;, to avoid the possible issues related to multiple enumeration of an IEnumerable   The IMessageKeyProvider interface has been renamed to IMessageIdProvider to prevent to be mistaken with the Kafka Key or Rabbit’s Routing Key   IInboundMessage/IOutboundMessage (plus all the related types) have been renamed to IInboundEnvelope/IOutboundEnvelope and the property containing the actual message has been renamed from Content to Message   The MustUnwrap option has been removed from the inbound connector configuration (messages are unwrapped by default)   1.2.0   What’s new     Some new events are published to the internal bus as a consequence to the Kafka events such as partitions assigned or revoked (see Kafka Events) [#34]   1.1.0   What’s new     Added IEndpointsConfigurator interface to allow splitting the endpoints configuration across multiple types (see Connecting to a Message Broker)   Added support for distributed tracing (based on System.Diagnostics)   Added IProducerBehavior and IConsumerBehavior to create an extension point closer to the actual message broker logic (see Behaviors)   Breaking Changes     ISortedBehavior was removed and replaced by a generic ISorted interface   1.0.5   What’s new     Upgrade to Confluent.Kafka 1.3.0   Fixes     Fixed OutboundQueueHealthCheck [#43]   The KafkaProducer is not disposed by default anymore when a KafkaException in thrown (creating too many instances of the producer over a short time span could lead to too many active TCP connections)   Fixed the bug preventing a KafkaConsumerEndpoint pointing to multiple topics to be successfully subscribed   1.0.4   Fixes     It is finally safe to consume and produce the same type of messages from within the same process (in a natural way, without any extra configuration)            Since version 1.0.0 the messages routed to an endpoint aren’t forwarded to any subscriber directly       Now the inbound connector has been fixed as well, preventing the inbound messages to be immediately routed once again to the outbound endpoint and eliminating all possible causes of mortal loops           1.0.3   Fixes     Kafka message key is not hashed anymore to avoid possible collisions and simplify debugging   Not really a fix but PartitioningKeyMemberAttribute has been deprecated in favor of KafkaKeyMemberAttribute, since the message key isn’t used just for partitioning (see Kafka Message Key)   1.0.2   Fixes     Reintroduced Add*Subscriber and Add*Behavior as IServiceCollection extension methods (for backward compatibility and greater flexibility) [#41]   Added WithInMemoryBroker and OverrideWithInMemoryBroker extension methods (see Testing)   1.0.0   What’s new     Message size optimization (no wrappers anymore)   Better headers usage: identifiers, types, chunks information, etc. are now all sent in the headers   Reviewed severity of some log entries   Cleaner internal implementation   Better exception handling (flattening of AggregateException)   Upgrade to Confluent.Kafka 1.2.2   The Kafka consumer automatically recovers from fatal errors (can be disabled via Endpoint configuration)   Support for .Net Core 3.0 and Entity Framework Core 3.0   Refactored packages (EF binding logic is now in a single package, versioned after the related EF version)   Better and cleaner configuration API (see for example Using the Bus and Behaviors)   Some performance improvements and optimizations (including #37)   Improved database locks mechanism (used also to run the OutboundQueueWorker)   Fixes     Fixed issue requiring types not implementing IMessage to be registered with HandleMessagesOfType&lt;T&gt; to consume them [#33]   Mitigated issue causing the DistributedBackgroundService to sometime fail to acquire the database lock [#39]   Fixed partition key value being lost when using the DeferredOutboundConnector   Other small fixes to improve stability and reliability   Breaking Changes     By default the messages published via IPublisher that are routed to an outbound endpoint are not sent through to the internal bus and cannot therfore be subscribed locally, within the same process (see Outbound Connector)   Some changes in IInboundMessage and IOutboundMessage interfaces   Changes to the schema of the outbox table (Silverback.Messaging.Connectors.Model.OutboundMessage)   The configuration fluent API changed quite a bit, refer to the current documentation (e.g. Using the Bus and Connecting to a Message Broker)   WithConnectionTo&lt;KafkaBroker&gt; has to be replaced with WithConnectionToKafka in order for all features to work properly. When failing to do so no message key will be generated, causing the messages to land in a random partition and/or preventing to publish to a compacted topic. (see Kafka Message Key)      Silverback.Integration.EntityFrameworkCore and Silverback.EventSourcing.EntityFrameworkCore have been deprecated (Silverback.Core.EntityFrameworkCore contains all the necessary logic to use EF as store)   KeyMemberAttribute has been renamed to PartitioningKeyMemberAttribute (see Kafka Message Key)   0.10.0   What’s new     Better error handling: now all exceptions, including the ones thrown by the MessageSerializer can be handled through the error policies   Improved logs: promoted some important logs to Information level, writing all processing errors as (at least) Warning and improved logged information quality (logged attributes)   Add ability to modify messages and headers when moving them via MoveMessageErrorPolicy   Message processing refactoring leading to cleaner, more extensible and predictable API and behavior   Fixes     Several other small (and not so small) issues and bugs   0.8.0 - 0.9.0   Released two versions mostly to fix bugs, do some small adjustments according to some user feedbacks and update the external dependencies (e.g. Confluent.Kafka 1.0.1).   Fixes     Fixed exception loading error policies from json in Silverback.Integration.Configuration [#24]   0.7.0   What’s new     Confluent.Kafka 1.0.0 has finally been released and it has been integrated and tested with this version of Silverback   Created a simple event store that perfectly integrates with the rest of the Silverback framework (see Event Sourcing)   Silverback.Integration.InMemory to mock the message broker behavior in your unit tests   Several small optimizations and improvements   0.6.0   What’s new     Added support for message headers (only accessible from Behaviors or “low-level” Broker implementation)   Simplified message subscription even further: now all public methods of the types implementing the marker interface ISubscriber are automatically subscribed by default without having to annotate them with the SubscribeAttribute (this behavior is customizable)   Upgrade to Confluent.Kafka 1.0.0-RC1   0.3.x - 0.5.x   Some releases where done adding quite a few features-   What’s new     Silverback.Integration.Configuration package to load the inbound/outbound configuration from the app.settings json   Batch processing   Parallel subscribers   Delegate subscription as an alternative to SubscribeAttribute based subscription   Improved support for Rx.net   Support for legacy messages and POCO classes   Offset storage as an alternative and more optimized way to guarantee exactly once processing, storing just the offset of the last message instead of logging every message  (see Inbound Connector)   Behaviors as a convenient way to implement your cross-cutting concerns (like logging, validation, etc.) to be plugged into the internal bus publishing pipeline (see Behaviors)   Message chunking to automatically split the larger messages and rebuild them on the other end (see Chunking)   much more…a huge amount of refactorings   Fixes     Several fixes and optimizations   0.3.2   The very first public release of Silverback! It included:     In-process message bus   Inbound/outbound connector for message broker abstraction   Kafka broker implementation   Outbox table pattern implementation   Exactly once processing   …  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/releases",
        "teaser":null},{
        "title": "Introduction and Glossary",
        "excerpt":"Silverback is essentially a bus that can be either used internally to an application or connected to a message broker to integrate different applications or microservices.    \t     Silverback is used to produce the messages 1 and 3 to the message broker, while the messages 2 and 3 are also consumed locally, within the same application.   Glossary   The following list serves as introduction to the terminology and types used in Silverback.   Publisher  An object that can be used to publish messages to the internal in-memory bus. It is accessed injecting IPublisher or (better) the more specific IEventPublisher and ICommandPublisher into your services.   Subscriber  A method (or delegate) that is subscribed to the bus and will process some (or all) of the messages that will be published.   Broker  A message broker, like Apache Kafka or RabbitMQ. It is abstracted by the IBroker interface and is used internally by Silverback to bind the internal bus with a message broker. It can be use directly but that shouldn’t be necessary.   Producer  An object used to publish messages to the broker. It is abstracted by the IProducer interface.   Consumer  An object used to receive messages from the broker. It is abstracted by the IConsumer interface.   Endpoint  Identifies a specific topic or queue. It also contains all the settings to bind to that endpoint and is therefore specific to the message broker implementation.   Inbound Connector  Connects to an endpoint and relays the received messages to the internal bus, where they can be consumed by one or more subscribers.   Outbound Connector  Used to relay the messages published to the internal bus to the message broker.   Behavior  Multiple behaviors are chained to build a sort of pipeline to process the messages transiting across the internal bus, the consumer or the producer. They are used to implement cross-cutting concerns, isolate responsibilities and allow for greater flexibility. Some built-in behaviors are responbile for serialization, error policies enforcement, batch handling, encryption, etc.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart",
        "teaser":null},{
        "title": "Enabling the Bus",
        "excerpt":"Silverback’s main component is the internal in-memory message bus and pretty much all other features are built on top of that.   The first mandatory step to start using Silverback is to register the core services (internal bus) with the .net core dependency injection.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services.AddSilverback();     } }   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart/bus",
        "teaser":null},{
        "title": "Creating the Message model",
        "excerpt":"Basics   First of all we need to create a message class. The message class can be any POCO class, it just need to be serializable.   using Silverback.Messaging.Messages;  public class SampleMessage {     public string Content { get; set; } }   It is very much suggested to consider using the Silverback.Core.Model package (documented in the next chapter) to better organize your message and write better and more readable code.   Silverback.Core.Model   A hierarchy of interfaces is available in Silverback.Core.Model to help specify the meaning of each message and produce in better, cleaner and more readable code.   The internal messages are being sent through the internal in-memory bus and don’t leave the service scope, while the integration messages are those messages exchanged between different microservices, through a message broker like Apache Kafka or RabbitMQ.   Event though strongly suggested, it’s not mandatory to use the proposed hierarchy from Silverback.Core.Model and everything can be achieved using POCO classes as messages and using the generic IPublisher to publish them.   In the following chapters you will find an overview of the different message types and their meaning but first of all we need to refernce the Siverback.Core.Model package and register it with the dependency injection.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services.AddSilverback().UseModel();     } }    Internal Messages   This messages can be used internally to the microservice bus but cannot be relayed to the message broker. See Translating Message for a convenient way to map the internal message to an IIntegrationMessage.   Events   IEvent is to be used to notify thing that happened inside a service and may be of some interest for one or more other service. The events are a fire-and-forget message type and no response is expected.   The IDomainEvent extends IEvent and the types implementing this interface are usually published only from within the domain entities (actually adding them to the internal collection and letting them be published during the save changes transaction). See also DDD and Domain Events.   Commands   ICommand or ICommand&lt;TResult&gt; are used to trigger an action in another service or component and are therefore very specific and usually consumed by one single subscriber. This messages can return a value (TResult).   Queries   IQuery&lt;TResult&gt; works exactly like ICommand&lt;TResult&gt;. This messages are obviously always returning something since they represent a request for data (query).   Integration messages   The IIntegrationMessage interface identifies those messages that are either published to the message broker or received through it (Note that IIntegrationMessage implements IMessage, obviously).   Integration Event   IIntegrationEvent can be used to export events to other microservices or, more generally, other applications.   IEventPublisher can be used to publish these events and they will automatically be routed to the message broker if an outbound connector was properly configured. See Connecting to a Message Broker for details.   Integration Command   IIntegrationCommand is used to trigger an action on another microservices (or application).   ICommandPublisher can be used to publish these messages and they will automatically be routed to the message broker if an outbound connector was properly configured. See Connecting to a Message Broker for details.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart/model",
        "teaser":null},{
        "title": "Publishing",
        "excerpt":"Basic Publishing   To publish the message you just need an instance of IPublisher (or derived interfaces if using Silverback.Core.Model, as shown later on).   using Silverback.Messaging.Publishing;  public class PublishingService {     private readonly IPublisher _publisher;      public PublishingService(IPublisher publisher)     {         _publisher = publisher;     }      public async Task PublishSomething()     {         await _publisher.PublishAsync(new SampleMessage          {              Content = \"whatever\"         });     } }   The publisher always exposes a synchronous and an asynchronous version of each method. The second option is of course to be preferred to take advantage of non-blocking async/await.   Return values   In some cases you will of course return a respoonse after having processed the message..   public async Task&lt;Report&gt; PublishSomething() {     var result = await _publisher.PublishAsync(new ReportQuery() { ... });      return result.Single(); }   Please note the required call to Single(), because Silverback allows you to have multiple subscribers for the same message and therefore collect multiple return values. This is not needed if using IQueryPublisher or ICommandPublisher described in the Model page.   Batches   It is possible to publish multiple messages with a single call to Publish or PublishAsync. The effect is very different than looping and calling the publish for each message since the collection will be handled as a batch, enabling parallel processing. It is usually suggested to publish multiple messages with the overloads accepting an IEnumerable&lt;&gt; and let the subscribers decide between parallel or sequential processing.   The entire batch will be processed inside the same dependency injection scope, thus allowing to handle it as a single transaction.   Silverback.Core.Model   Silverback.Core.Model has been introduced in the previous page Creating the Message model.   Each message type (IEvent, ICommand and IQuery) also comes with its specialized IPublisher as quickly shown in the following sub-chapters.   Events   The messages implementing IEvent, IDomainEvent or IIntegrationEvent can be published using an IEventPublisher&lt;TEvent&gt; can be used to publish them.   using Silverback.Messaging.Publishing;  public class PublishingService {     private readonly IEventPublisher _publisher;      public PublishingService(IEventPublisher publisher)     {         _publisher = publisher;     }      public async Task PublishEvent()     {         var myEvent = new MyEvent() { ... };         await _publisher.PublishAsync(myEvent);     } }   Commands   The messages that implement ICommand, ICommand&lt;TResult&gt; or IIntegrationCommand can be published using an ICommandPublisher&lt;TCommand&gt;.   using Silverback.Messaging.Publishing;  public class PublishingService {     private readonly ICommandPublisher _publisher;      public PublishingService(ICommandPublisher publisher)     {         _publisher = publisher;     }      public async Task ExecuteCommand()     {         var command = new MyCommand() { ... };         await _publisher.ExecuteAsync(command);     } }  using Silverback.Messaging.Publishing;  public class PublishingService {     private readonly ICommandPublisher _publisher;      public PublishingService(ICommandPublisher publisher)     {         _publisher = publisher;     }      public async Task&lt;MyResult&gt; ExecuteCommand()     {         var command = new MyCommand() { ... };         var result = await _publisher.ExecuteAsync(command);         return result;     } }   Queries   The IQueryPublisher ca be used  to publish the messages implementing the IQuery&lt;TResult&gt; interface.   using Silverback.Messaging.Publishing;  public class PublishingService {     private readonly IQueryPublisher _publisher;      public PublishingService(IQueryPublisher publisher)     {         _publisher = publisher;     }      public async Task&lt;MyResult&gt; GetResults()     {         var query = new MyQuery() { ... };         var result = await _publisher.ExecuteAsync(myQuery);         return result;     } }  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart/publish",
        "teaser":null},{
        "title": "Subscribing",
        "excerpt":"Now all is left to do is write a subscriber method to process the produced messages.   Type based subscription   The default and usually preferred way to subscribe is by implementing the marker interface ISubscriber.   using Silverback.Messaging.Subscribers;  public class SubscribingService : ISubscriber {     public async Task OnMessageReceived(SampleMessage message)     {         // ...your message handling loging...     } }   Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .AddScopedSubscriber&lt;SubscribingService&gt;();     } }    All subscribers must be registered with the service provider as shown in the second code snippet above and all public methods are automatically subscribed by default (see  the explicit method subscription chapter, if more control over is desired).   All Add*Subscriber methods are available also as extensions to the IServiceCollection and it isn’t therefore mandatory to call them immediately after AddSilverback.   Registering types not implementing ISubscriber   If you don’t want to implement ISubscriber you can register other types (directly or using a base classe or interface).    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .AddScopedSubscriber&lt;SubscribingService&gt;();     }          public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Subscribe&lt;SubscribingService&gt;();     } }   …or…   Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .AddScopedSubscriber&lt;ICustomSubscriber, SubscribingService1&gt;()             .AddScopedSubscriber&lt;ICustomSubscriber, SubscribingService2&gt;()     }          public void Configure(BusConfigurator busConfigurator)     {         busConfigurator.Subscribe&lt;ICustomSubscriber&gt;();     } }    This could be useful to avoid a reference to Silverback in lower layers.   Explicit method subscription   You can explicitely subscribe a method using the SubscribeAttribute (this allows you to subscribe non-public methods as well).   using Silverback.Messaging.Subscribers;  public class SubscribingService : ISubscriber {     [Subscribe]     public async Task OnMessageReceived(SampleMessage message)     {         // ...your message handling loging...     } }   The SubscribeAttribute exposes three extra properties, that can be used to enable parallelism:     Exclusive: A boolean value indicating whether the method can be executed concurrently to other methods handling the same message. The default value is true (the method will be executed sequentially to other subscribers).   Parallel: A boolean value indicating whether the method can be executed concurrently when multiple messages are fired at the same time (e.g. in a batch). The default value is false (the messages are processed sequentially).   MaxDegreeOfParallelism: Limit the number of messages that are processed concurrently. Used only together with Parallel = true and mostly useful when performing CPU-bound work (as opposed to non-blocking I/O). The default value is Int32.Max and means that there is no limit to the degree of parallelism.   public class SubscribingService : ISubscriber {     [Subscribe(Parallel = true, MaxDegreeOfParallelism = 10)]     public async Task OnMessageReceived(SampleMessage message)     {         // ...your message handling loging...     } }   It is also possible to completely disable the automatic subscription of the public methods.    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator.Subscribe&lt;ISubscriber&gt;(             autoSubscribeAllPublicMethods: false);     } }    Delegate based subscription   It is also possible to subscribe an inline lambda or integrate an existing method without having to modify the codebase to add the SubscribeAttribute.    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator.Subscribe(             (IReadOnlyCollection&lt;IMessage&gt; message) =&gt;                 HandleMessage(message));     } }    Multiple overloads of the Subscribe method exist and you can optionally provide a SubscriptionOptions instance to enable parallelism (analog to the properties set to the SubscribeAttribute).    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator.Subscribe(             (MyMessage message, MyService service) =&gt;                 service.HandleMessage(message),             new SubscriptionOptions              {                  Parallel = true,                  Exclusive = false              });     } }    Supported methods and parameters   The subscribed method can either be synchronous or asynchronous, but the asynchronous approach should be preferred if aiming at taking advantage of non-blocking I/O.   The first parameter must be the message or the collection of messages. The following collection are supported:     IEnumerable&lt;TMessage&gt; or IReadOnlyCollection&lt;TMessage&gt;: To be able to handle a batch of messages at once. It will receive also the single messages (in a collection with a single item). (Silverback will in any case always forward a materialized IList of messages, but explicitly declaring the paramter as IReadOnlyCollection&lt;T&gt; avoids any false positive “possible multiple enumeration of IEnumerable” issue that may be detected by a static code analysis tool.)   Observable&lt;TMessage&gt;: Silverback.Core.Rx allows you to handle your messages in a reactive programming fashion.   Using a collection as parameter allows you to handle a batch of messages at once, allowing more control. The methods with a collection as parameter will still be called for single messages and methods with a single message as input parameter will be called for each message in a batch (in parallel, if allowed by the specified configuration).   using Silverback.Messaging.Subscribers;  public class SubscribingService : ISubscriber {     [Subscribe(Parallel=true)]     public async Task OnMessageReceived(Observable&lt;SampleMessage&gt; stream) =&gt;         stream...Subscribe(...); }   The method can have other parameters that will be resolved using the service provider. Very useful also to integrate existing code.    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator.Subscribe(             (BasketCheckoutMessage message, CheckoutService service) =&gt;                  service.Handle(message));     } }    Return values   A subscriber can also have a return value that can be collected by the publisher.   using Silverback.Messaging.Subscribers;  public class SubscribingService : ISubscriber {     public async Task&lt;SampleResult&gt; OnMessageReceived(SampleMessage message)     {         ...          return new SampleResult(...);     } }   Return new messages (republishing)   A subscribed method can also optionally return a message or a collection of messages (either IEnumerable&lt;TMessage&gt;, IReadOnlyCollection&lt;TMessage&gt; or Observable&lt;TMessage&gt;, if using Silverback.Core.Rx) that will be automatically republished to the internal bus.   Silverback recognizes per default only the messages implementing IMessage but you can register your own types.    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .HandleMessagesOfType&lt;MyCustomType&gt;();     } }    Using assembly scanning   You may use a Dependency Injection framework such as Autofac providing assembly scanning.   You can of course use such framework to register the subscribers, the only thing to keep in mind is that they need to be registered both as the marker interface (ISubscriber, unless configured otherwise) and as the type itself.   Example using Autofac:  public class SubscribersModule : Module {     protected override void Load(ContainerBuilder builder)     {         builder.RegisterAssemblyTypes(Assembly.GetExecutingAssembly())             .Where(t =&gt; t.IsAssignableTo&lt;ISubscriber&gt;())             .AsImplementedInterfaces()             .AsSelf()             .InstancePerLifetimeScope();     } }   Bootstrapping   The very first publish will take a bit longer and use more resources, since all subscribers have to be resolved (instantiated) once, in order for Silverback to scan the subscriber methods and figure out which message type is handled.   This operation can be performed at startup, preloading the necessary information.   It will of course still cause all subscribers to be instantiated, but it’s done in a more predictable and controlled way, without affecting the application performance later on (e.g. when handling the first HTTP request).    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator.ScanSubscribers();     } }   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart/subscribe",
        "teaser":null},{
        "title": "DDD and Domain Events",
        "excerpt":"One of the core features of Silverback is the ability to publish the domain events as part of the DbContext save changes transaction in order to guarantee consistency.   The Silverback.Core.Model package contains a sample implementation of a DomainEntity but you can also implement you own type.   In case of a custom implementation the only constraint is that you must implement the IMessagesSource interface in order for Silverback to be able to access the associated events.   using Silverback.Domain;  namespace Sample {     public class Basket : DomainEntity, IAggregateRoot     {         private readonly List&lt;BasketItem&gt; _items = new List&lt;BasketItem&gt;();          private Basket()         {         }          public Basket(Guid userId)         {             UserId = userId;             Created = DateTime.UtcNow;         }          [Key]         public int Id { get; private set; }         public IEnumerable&lt;BasketItem&gt; Items =&gt; _items.AsReadOnly();         public Guid UserId { get; private set; }         public DateTime Created { get; private set; }         public DateTime? CheckoutDate { get; private set; }          public void Checkout()         {             CheckoutDate = DateTime.UtcNow;              AddEvent&lt;BasketCheckoutEvent&gt;();         }     } }   The AddEvent&lt;TEvent&gt;() method adds the domain event to the events collection, to be published when the entity is saved.   To enable this mechanism we just need to override the various SaveChanges methods to plug-in the DbContextEventsPublisher contained in the Silverback.Core.EntityFrameworkCore package.   using Microsoft.EntityFrameworkCore; using Silverback.EntityFrameworkCore; using Silverback.Messaging.Publishing;  namespace Sample {    public class SampleDbContext : DbContext     {         private readonly DbContextEventsPublisher _eventsPublisher;          public SampleDbContext(IPublisher publisher)         {             _eventsPublisher = new DbContextEventsPublisher(publisher, this);         }          public SampleDbContext(DbContextOptions options, IPublisher publisher)             : base(options)         {             _eventsPublisher = new DbContextEventsPublisher(publisher, this);         }          public override int SaveChanges()             =&gt; SaveChanges(true);          public override int SaveChanges(bool acceptAllChangesOnSuccess)             =&gt; _eventsPublisher.ExecuteSaveTransaction(() =&gt; base.SaveChanges(acceptAllChangesOnSuccess));          public override Task&lt;int&gt; SaveChangesAsync(CancellationToken cancellationToken = default)             =&gt; SaveChangesAsync(true, cancellationToken);          public override Task&lt;int&gt; SaveChangesAsync(             bool acceptAllChangesOnSuccess,             CancellationToken cancellationToken = default)             =&gt; _eventsPublisher.ExecuteSaveTransactionAsync(() =&gt;                 base.SaveChangesAsync(acceptAllChangesOnSuccess, cancellationToken));     } }  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart/ddd",
        "teaser":null},{
        "title": "Connecting to a Message Broker",
        "excerpt":"To connect Silverback to a message broker we need a reference to Silverback.Integration, plus the concrete implementation (Silverback.Integration.Kafka  or Silverback.Integration.RabbitMQ). We can then add the broker to the DI and configure the connected endpoints.   Sample configuration   The following example is very simple and there are of course many more configurations and possibilities. Some more details are given in the dedicated Broker Configuration section.   The basic concepts:     WithConnectionToMessageBroker registers the services necessary to connect to a message broker   AddKafka, AddRabbit, AddInMemoryBroker, etc. register the message broker implementation(s)   AddInbound is used to automatically relay the incoming messages to the internal bus and they can therefore be subscribed as seen in the previous chapters   AddOutbound works the other way around and subscribes to the internal bus to forward the integration messages to the message broker   Connect automatically creates and starts all the consumers.   Basic Kafka configuration   The following sample demonstrates how to setup some inbound and outbound endpoints against an Apache Kafka broker.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddInboundConnector()                 .AddOutboundConnector());     }      public void Configure(BusConfigurator busConfigurator)     {         busConfigurator.Connect(endpoints =&gt; endpoints             .AddInbound(                 new KafkaConsumerEndpoint(\"basket-events\")                 {                     Configuration = new KafkaConsumerConfig                     {                         BootstrapServers = \"PLAINTEXT://kafka:9092\",                         GroupId = \"order-service\"                     }                 })             .AddInbound(                 new KafkaConsumerEndpoint(\"payment-events\")                 {                     Configuration = new KafkaConsumerConfig                     {                         BootstrapServers = \"PLAINTEXT://kafka:9092\",                         GroupId = \"order-service\"                     }                 })             .AddOutbound&lt;IIntegrationEvent&gt;(                 new KafkaProducerEndpoint(\"order-events\")                 {                     Configuration = new KafkaProducerConfig                     {                         BootstrapServers = \"PLAINTEXT://kafka:9092\"                     }                 }));     } }    Basic RabbitMQ configuration   The following sample demonstrates how to setup some inbound and outbound endpoints against a RabbitMQ broker.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddRabbit()                 .AddInboundConnector()                 .AddOutboundConnector());     }      public void Configure(BusConfigurator busConfigurator)     {         busConfigurator.Connect(endpoints =&gt; endpoints             .AddInbound(                 new RabbitExchangeConsumerEndpoint(\"basket-events\")                 {                     Connection = new RabbitConnectionConfig                     {                         HostName = \"localhost\",                         UserName = \"guest\",                         Password = \"guest\",                     },                     Exchange = new RabbitExchangeConfig                     {                         IsDurable = true,                         IsAutoDeleteEnabled = false,                         ExchangeType = ExchangeType.Fanout                     },                     QueueName = \"basket-events-order-service-queue\",                     Queue = new RabbitQueueConfig                     {                         IsDurable = true,                         IsExclusive = true,                         IsAutoDeleteEnabled = false                     }                 })             .AddInbound(                 new RabbitExchangeConsumerEndpoint(\"payment-events\")                 {                     Connection = new RabbitConnectionConfig                     {                         HostName = \"localhost\",                         UserName = \"guest\",                         Password = \"guest\",                     },                     Exchange = new RabbitExchangeConfig                     {                         IsDurable = true,                         IsAutoDeleteEnabled = false,                         ExchangeType = ExchangeType.Fanout                     },                     QueueName = \"payment-events-order-service-queue\",                     Queue = new RabbitQueueConfig                     {                         IsDurable = true,                         IsExclusive = true,                         IsAutoDeleteEnabled = false                     }                 })             .AddOutbound&lt;IIntegrationEvent&gt;(                 new RabbitExchangeProducerEndpoint(\"order-events\")                 {                     Connection = new RabbitConnectionConfig                     {                         HostName = \"localhost\",                         UserName = \"guest\",                         Password = \"guest\"                     },                     Exchange = new RabbitExchangeConfig                     {                         IsDurable = true,                         IsAutoDeleteEnabled = false,                         ExchangeType = ExchangeType.Fanout                     }                 }));     } }    Multiple brokers   It is possible to use multiple message brokers together in the same application. The following sample demonstrates how to consume from both Apache Kafka and RabbitMQ.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddRabbit()                 .AddInboundConnector());     }      public void Configure(BusConfigurator busConfigurator)     {         busConfigurator.Connect(endpoints =&gt; endpoints             .AddInbound(                 new RabbitExchangeConsumerEndpoint(\"rabbit-events\")                 {                     ...                 })             .AddInbound(                 new KafkaConsumerEndpoint(\"kafka-events\")                 {                     ...                 }));     } }    Using IEndpointsConfigurator   The endpoints configuration can be split into multiple types implementing the IEndpointsConfigurator interface.   private class MyFeatureConfigurator : IEndpointsConfigurator {     public void Configure(IEndpointsConfigurationBuilder builder)     {         builder             .AddOutbound&lt;IMyFeatureEvents&gt;(                 new KafkaProducerEndpoint(\"my-feature-events\")                 {                     ...                 }             )             .AddInbound(                 new KafkaConsumerEndpoint(\"my-feature-commands\")                 {                     ...                 }             );     } }   The configurators can be registered using either the RegisterConfigurator or AddEndpointsConfigurator methods.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .RegisterConfigurator&lt;MyFeatureConfigurator&gt;());     } }   …or…   Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka());          services.AddEndpointsConfigurator&lt;MyFeatureConfigurator&gt;());     } }    Using assembly scanning   You can of course use the assembly scanning capabilities of your  Dependency Injection framework (e.g. Autofac) to register all the IEndpointsConfigurator.   Example using Autofac:  public class EndpointsConfiguratorsModule : Module {     protected override void Load(ContainerBuilder builder)     {         builder.RegisterAssemblyTypes(Assembly.GetExecutingAssembly())             .Where(t =&gt; t.IsAssignableTo&lt;IEndpointsConfigurator&gt;())             .AsImplementedInterfaces();     } }   Graceful shutdown   It is important to properly close the consumers using the Disconnect method before exiting. The offsets have to be committed and the broker has to be notified (it will then proceed to reassign the partitions as needed).    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         var brokers = busConfigurator.Connect(...);          appLifetime.ApplicationStopping.Register(() =&gt; brokers.Disconnect());     } }    Health Monitoring   The Silverback.Integration.HealthChecks package contains some extensions for Microsoft.Extensions.Diagnostics.HealthChecks that can be used to monitor the connection to the message broker.   Currently two checks exists:     AddOutboundEndpointsCheck: Adds an health check that sends a ping message to all the outbound endpoints.   AddOutboundQueueCheck: Adds an health check that monitors the outbound queue (outbox table), verifying that the messages are being processed.   The usage is very simple, you just need to configure the checks in the Startup.cs, as shown in the following example.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services.AddHealthChecks()             .AddOutboundEndpointsCheck()             .AddOutboundQueueCheck();     }      public void Configure(IApplicationBuilder app)     {         app.UseHealthChecks(\"/health\");     } }    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart/message-broker",
        "teaser":null},{
        "title": "Translating Messages",
        "excerpt":"It is not uncommon to be willing to slightly transform the internal message before exporting it to the outside world (e.g. you may not want to export the full entity related to the domain event). You can easily achieve this with a subscriber that just maps/translates the messages.   public class MapperService : ISubscriber {     public IMessage MapCheckoutEvent(CheckoutDomainEvent message) =&gt;          new CheckoutIntegrationEvent         {             UserId = message.Source.UserId,             Total = mesage.Source.Total,             ...         }; }  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart/translating",
        "teaser":null},{
        "title": "Message Headers",
        "excerpt":"Custom headers   There are multiple options to add custom headers to a message:     using an IBehavior or IProducerBehavior, as shown in the Behaviors chapter of the quickstart   annotating some properties with the HeaderAttribute (see next chapter)   or you could use the IBroker / IProducer directly, as explained in the Using IBroker section   Using HeaderAttribute   The HeaderAttribute usage is very simple: you just have to decorate the properties you want to publish as headers and specify a name for the header.   The headers value will also automatically be mapped back to the property upon consuming if the property declares a setter.   using Silverback.Messaging.Messages;  namespace Sample {     public class OrderCreatedEvent     {         public List&lt;LineItems&gt; Items { get; set; }          [Header(\"x-order-type\", PublishDefaultValue = true)]         [JsonIgnore]         public OrderType OrderType { get; set; }          [Header(\"x-books-order\")]         public bool ContainsBooks =&gt; Items.Any(item =&gt; item.Type == \"book\")          [Header(\"x-dvd-order\")]         public bool ContainsDvd =&gt; Items.Any(item =&gt; item.Type == \"dvd\")     } }   The PublishDefaultValue boolean property defines whether the header has to be published even if the property is set to the default value for its data type. The default is false.   Note that the JsonIgnoreAttribute can be used to prevent the same properties to be serialized in the JSON body, when using the JsonMessageSerializer.   Only the message type will be scanned, therefore the properties decorated with the HeaderAttribute must be in the root of the message object.   Default headers   Silverback will add some headers to the produced messages. They may vary depending on the scenario. Here is the list of the default headers that may be sent.                  Header Key       Description                       x-message-id       The message unique identifier.                 x-message-type       The assembly qualified name of the message type. Used by the default JsonMessageSerializer.                 x-failed-attempts       If an exception if thrown the failed attempts will be incremented and stored as header. This is necessary for the error policies to work.                 x-source-endpoint       This will be set by the Move is being moved from.                 x-chunk-id       The unique id of the message chunk, used when chunking is enabled.                 x-chunks-count       The total number of chunks the message was split into, used when chunking is enabled.                 x-first-chunk-offset       The IOffset value of the first chunk of the same message, used when chunking is enabled.                 x-batch-id       The unique id assigned to the messages batch, used mostly for tracing, when batch processing is enabled.                 x-batch-size       The total number of messages in the batch, used mostly for tracing, when batch processing is enabled.                 traceparent       The current Activity.Id, used by the IConsumer implementation to set the Activity.ParentId, thus enabling distributed tracing across the message broker. Note that an Activity is automatically started by the default IProducer implementation. See System.Diagnostics documentation for details about Activity and distributed tracing in asp.net core and W3C Trace Context proposal for details about the headers.                 tracestate       The Activity.TraceStateString. See also the W3C Trace Context proposal for details.                 tracebaggage       The string representation of the Activity.Baggage dictionary. See System.Diagnostics documentation for details.                 content-type       The content type of the binary file, used when producing or consuming an IBinaryFileMessage.                 x-kafka-message-key       When using Kafka, the kafka message key will also be submitted as header (see Kafka Message Key (Partitioning) to know how to define a message key)           Some constants for the headers name are also provided as reported in the following table.                  Header Key       Constant                       x-message-id       DefaultMessageHeaders.MessageId                 x-message-type       DefaultMessageHeaders.MessageType                 x-failed-attempts       DefaultMessageHeaders.FailedAttempts                 x-source-endpoint       DefaultMessageHeaders.SourceEndpoint                 x-chunk-id       DefaultMessageHeaders.ChunkId                 x-chunks-count       DefaultMessageHeaders.ChunksCount                 x-first-chunk-offset       DefaultMessageHeaders.FirstChunkOffset                 x-batch-id       DefaultMessageHeaders.BatchId                 x-batch-size       DefaultMessageHeaders.BatchSize                 traceparent       DefaultMessageHeaders.TraceId                 tracestate       DefaultMessageHeaders.TraceState                 tracebaggage       DefaultMessageHeaders.TraceBaggage                 content-type       DefaultMessageHeaders.ContentType                 x-kafka-message-key       KafkaMessageHeaders.KafkaKey          ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart/headers",
        "teaser":null},{
        "title": "Behaviors",
        "excerpt":"The behaviors can be used to build a custom pipeline (similar to the asp.net pipeline), easily adding your cross-cutting functionalities such as logging, validation, etc.   IBehavior   The behaviors implementing the IBehavior interface will be invoked by the IPublisher internals every time a message is published to the internal bus (this includes the inbound/outbound messages, but they will be wrapped into an IInboundEvelope or IOutboundEnvelope).   At every call to IPublisher.Publish the Handle method of each registered behavior is called, passing in the collection of messages and the delegate to the next step in the pipeline. This gives you the flexibility to execute any sort of code before and after the messages have been actually published (before or after calling the next() step). You can for example modify the messages before publishing them, validate them (like in the above example), add some logging / tracing, etc.   IBehavior example   The following example demonstrates how to use a behavior to trace the messages.   using Silverback.Messaging.Publishing;  public class TracingBehavior : IBehavior {     private readonly ITracer _tracer;      public TracingBehavior(ITracer tracer)     {         _tracer = tracer;     }      public async Task&lt;IReadOnlyCollection&lt;object&gt;&gt; Handle(         IReadOnlyCollection&lt;object&gt; messages,          MessagesHandler next)     {         tracer.TraceProcessing(messages);         var result = await next(messages);         tracer.TraceProcessed(messages);          return result;     } }   The Handle receives a collection of object because a bunch of messages can be published at once via IPublisher or the consumer can be configured to process the messages in batch.   IInboundEnvelope and IOutboundEnvelope are internally used by Silverback to wrap the messages being sent to or received from the message broker and will be received by the IBroker. Those interfaces contains the message plus the additional data like endpoint, headers, offset, etc.   The IBehavior implementation have simply to be registered for DI.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .AddScopedBehavior&lt;TracingBehavior&gt;();     } }    All Add*Behavior methods are available also as extensions to the IServiceCollection and it isn’t therefore mandatory to call them immediately after AddSilverback.   IProducerBehavior and IConsumerBehavior   The IProducerBehavior and IConsumerBehavior are similar to the IBehavior but work at a lower level, much closer to the message broker.   IProducerBehavior example   The following example demonstrate how to set a custom message header on each outbound message.   public class CustomHeadersBehavior : IProducerBehavior {     public async Task Handle(         ProducerPipelineContext context,          RawOutboundEnvelopeHandler next)     {         context.Envelope.Message.Headers.Add(\"generated-by\", \"silverback\");          await next(context);     } }    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToKafka(options =&gt; options                 .AddSingletonBrokerBehavior&lt;CustomHeadersBehavior&gt;()             );     } }    IConsumerBehavior example   The following example demonstrate how to log the headers received with each inbound message.   public class LogHeadersBehavior : IConsumerBehavior {     private readonly ILogger&lt;LogHeadersBehavior&gt; _logger;      public LogHeadersBehavior(ILogger&lt;LogHeadersBehavior&gt; logger)     {         _logger = logger;     }      public async Task Handle(         ConsumerPipelineContext context,          IServiceProvider serviceProvider,         RawInboundEnvelopeHandler next)     {         foreach (var envelope in context.Envelopes)         {             foreach (var header in envelope.Headers)             {                 _logger.LogTrace(                     \"{key}={value}\",                     header.Key,                     header.Value);             }         }          await next(context, serviceProvider);     } }    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToKafka(options =&gt; options                 .AddSingletonBrokerBehavior&lt;LogHeadersBehavior&gt;()             );     } }    The Handle method reaceives an instance of IServiceProvider that can be either the root service provider or the scoped service provider for the processing of the consumed message (depending on the position of the behavior in the pipeline).   Limitations   Because of the way the Silverback’s broker integration works IProducerBehavior and IConsumerBehavior implementations can only be registered as singleton. An IProducerBehaviorFactory or IConsumerBehaviorFactory can be used to create an instance per each IConsumer or IProducer that gets intantiated.   If a scoped instance is needed you have to either inject the IServiceScopeFactory (or IServiceProvider) or use an IBehavior (that can still be used to accomplish most of the tasks, as shown in the next examples).   public class TracingBehavior : IBehavior {     private readonly IDbLogger _dbLogger;      public TracingBehavior(IDbLogger _dbLogger)     {         _dbLogger = dbLogger;     }      public async Task&lt;IReadOnlyCollection&lt;object&gt;&gt; Handle(         IReadOnlyCollection&lt;object&gt; messages,          MessagesHandler next)     {         foreach (var envelope in messages.OfType&lt;IInboundEnvelope&gt;())         {             _dbLogger.LogInboundMessage(                 envelope.Message.GetType(),                  envelope.Headers,                 envelope.Endpoint,                 envelope.Offset);         }          await _dbLogger.SaveChangesAsync();          return await next(messages);     } }   Ordering   The order in which the behaviors are executed does obviously matter and it is possible to precisely define it implementing the ISorted interface.   public class SortedBehavior : IBehavior, ISorted {     public int SortIndex =&gt; 120;      public Task&lt;IReadOnlyCollection&lt;object&gt;&gt; Handle(         IReadOnlyCollection&lt;object&gt; messages,          MessagesHandler next)     {         // ...your logic...          return next(messages);     } }   The sort index of the built-in behaviors is described in the next chapter.   Built-in behaviors   Silverback itself strongly relies on the behaviors to implement its features and combine them all together. In this chapter you find the list of the existing behaviorS and their respective sort index.   IBehavior   This behaviors act in the internal bus pipeline.                  Name       Index       Description                       OutboundProducerBehavior       200       Produces the IOutboundEnvelope&lt;TMessage&gt; through the correct IOutboundConnector instance.                 OutboundRouterBehavior       300       Routes the messages to the outbound endpoint by wrapping them in an IOutboundEnvelope&lt;TMessage&gt; that is republished to the bus.           The sort index is counterintoutive, as the OutboundRouterBehavior is actually needed before the OutboundProducerBehavior, but that happen in two separate and consecutive pipelines to give you the chance to subscribe to the IOutboundEnvelope if needed. So the OutboundRouterBehavior creates the IOutboundEnvelope and publishes it to the internal bus for the next OutboundProducerBehavior to catch it and forward it to the configured IOutboundConnector (at this point the message is not forwaded anymore to the next behavior in the pipeline).   IProducerBehavior   This behaviors build the producer pipeline and contain the actual logic to properly serialize the messages according to the applied configuration.                  Name       Index       Description                       ActivityProducerBehavior       100       Starts an Activity and adds the tracing information to the message headers.                 HeadersWriterProducerBehavior       200       Maps the properties decorated with the HeaderAttribute to the message headers.                 MessageIdInitializerProducerBehavior       300       It ensures that an x-message-id header is always produced.                 BrokerKeyHeaderInitializer       310       Provided by the message broker implementation (e.g. KafkaMessageKeyInitializerProducerBehavior or RabbitRoutingKeyInitializerProducerBehavior), sets the message key header that will be used by the IProducer implementation to set the actual message key.                 BinaryFileHandlerProducerBehavior       500       Switches to the BinaryFileMessageSerializer if the message being produced implements the IBinaryFileMessage interface.                 SerializerProducerBehavior       900       Serializes the message being produced using the configured IMessageSerializer.                 EncryptorProducerBehavior       950       Encrypts the message according to the EncryptionSettings.                 ChunkSplitterProducerBehavior       1000       Splits the messages into chunks according to the ChunkSettings.           IConsumerBehavior   This behaviors are the foundation of the consumer pipeline and contain the actual logic to deserialize the incoming messages.                  Name       Index       Description                       ActivityConsumerBehavior       100       Starts an Activity with the tracing information from the message headers.                 InboundProcessorConsumerBehavior       200       Handles the retry policies, batch consuming and scope management of the messages that are consumed via an inbound connector.                 ChunkAggregatorConsumerBehavior       300       Temporary stores and aggregates the message chunks to rebuild the original message.                 DecryptorConsumerBehavior       400       Decrypts the message according to the EncryptionSettings.                 BinaryFileHandlerProducerBehavior       500       Switches to the BinaryFileMessageSerializer if the message being consumed is a binary message (according to the x-message-type header.                 DeserializerConsumerBehavior       600       Deserializes the messages being consumed using the configured IMessageSerializer.                 HeadersReaderConsumerBehavior       700       Maps the headers with the properties decorated with the HeaderAttribute.          ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart/behaviors",
        "teaser":null},{
        "title": "Testing",
        "excerpt":"The Silverback.Integration.InMemory package allows to perform end-to-end tests without having to integrate with a real message broker.   Unit Tests   Here an example of an xUnit test build using the InMemoryBroker.   public class InMemoryBrokerTests {     private readonly IServiceProvider _serviceProvider;      // Configure DI during setup     public InMemoryBrokerTests()     {         var services = new ServiceCollection();          // Loggers are a prerequisite         services.AddSingleton&lt;ILoggerFactory, NullLoggerFactory&gt;();         services.AddSingleton(typeof(ILogger&lt;&gt;), typeof(NullLogger&lt;&gt;));          services             // Register Silverback as usual             .AddSilverback()             // Register the InMemoryBroker instead of             // the real broker (e.g. KafkaBroker)             .WithInMemoryBroker()             // Register the subscriber under test             .AddScopedSubscriber&lt;MySubscriber&gt;();          // ...register all other types you need...          _serviceProvider = services.BuildServiceProvider();     }      [Fact]     public void SampleTest()     {         // Arrange          // Configure the Bus         _serviceProvider.GetRequiredService&lt;BusConfigurator&gt;()             // Configure inbound and outbound endpoints             .Connect(endpoints =&gt; endpoints                 .AddInbound(new KafkaConsumerEndpoint(\"test-topic\"));          // Create a producer to push to test-topic         var producer = _serviceProvider             .GetRequiredService&lt;IBroker&gt;()             .GetProducer(new KafkaProducerEndpoint(\"test-topic\"));          // Act         producer.Produce(new TestMessage { Content = \"hello!\" });         producer.Produce(new TestMessage { Content = \"hello 2!\" });          // Assert         // ...your assertions...     } }   Integration Tests   An alternative technique is to leverage the ASP.NET Core integration tests to perform a full test based on the real configuration applied in the application’s startup class.   The following code shows the most simple integration test possible, in which an object is published to the broker and e.g. a subscriber is called.   public class IntegrationTests : IClassFixture&lt;WebApplicationFactory&lt;Startup&gt;&gt; {     private readonly WebApplicationFactory&lt;Startup&gt; _factory;      public IntegrationTests(WebApplicationFactory&lt;Startup&gt; factory)     {         _factory = factory.WithWebHostBuilder(builder =&gt;         {             builder.ConfigureTestServices(services =&gt;             {                 // Replace the usual broker (e.g. KafkaBroker)                 // with the InMemoryBroker                 services.OverrideWithInMemoryBroker();             });         };     }      [Fact]     public async Task SampleTest()     {         // Arrange          // Resolve a producer to push to test-topic         var producer = _factory.Server.Host.Services             .GetRequiredService&lt;IBroker&gt;()             .GetProducer(new KafkaProducerEndpoint(\"tst-topic\"));          // Act         await producer.ProduceAsync(new TestMessage { Content = \"abc\" });          // Assert         // ...your assertions...     } }   As topics represents APIs there might be more complex scenarios in which one wants to test the serialization as well to ensure compatibility. The code below shows a test which assumes that an use case is to consume from a topic, transform the record and produce to another topic.   public class IntegrationTests     : IClassFixture&lt;WebApplicationFactory&lt;Startup&gt;&gt; {     private readonly WebApplicationFactory&lt;Startup&gt; _factory;      public IntegrationTests(WebApplicationFactory&lt;Startup&gt; factory)     {         _factory = factory.WithWebHostBuilder(builder =&gt;         {             builder.ConfigureTestServices(services =&gt;             {                 // Replace the usual broker (e.g. KafkaBroker)                 // with the InMemoryBroker                 services.OverrideWithInMemoryBroker();             });         };     }      [Fact]     public async Task SampleTest()     {          // Arrange         const string record = @\"{             \"\"FIRST_NAME\"\": \"\"Xy\"\",             \"\"LAST_NAME\"\":\"\"Zz\"\",             \"\"AGE\"\":32         }\";          byte[] recordBytes = Encoding.UTF8.GetBytes(record);         using IServiceScope scope = _webApplicationFactory.Services.CreateScope();         var broker = scope.ServiceProvider.GetRequiredService&lt;IBroker&gt;();          // Internal events are directly emitted to the bus.         // Events for which an endpoint is configured are emitted as IOutboundEnvelope&lt;T&gt; to the bus.         IList&lt;IOutboundEnvelope&lt;Person&gt;&gt; externalEvents = new List&lt;IOutboundEnvelope&lt;Person&gt;&gt;();         scope.ServiceProvider.GetRequiredService&lt;BusConfigurator&gt;()             .Subscribe&lt;IOutboundEnvelope&lt;MappedPerson&gt;&gt;(e =&gt; externalEvents.Add(e));          // Calling this producer is like there would be an \"incoming\" record.         IProducer producer = broker.GetProducer(new KafkaProducerEndpoint(\"MyTopicFromWhichTheApplicationConsumes\"));          // Act         await producer.ProduceAsync(recordBytes);          // Assert         externalEvents.Count.Should().Be(1);          IOutboundEnvelope&lt;MappedPerson&gt; message = externalEvents.Single();         message.Endpoint.Name.Should().Be(\"MyTopicToWhichTheApplicationWrites\");         string actualMessage = Encoding.UTF8.GetString(message.RawMessage);          const string expectedMessage = @\"{             \"\"FirstName\"\": \"\"Xy\"\",             \"\"LastName\"\":\"\"Zz\"\",             \"\"Age\"\":32         }\";          // Might you want to create a own FluentAssertionExtension, to do something like this.         actualMessage.Should().BeEquivalentJsonTo(expectedMessage);     } }   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart/testing",
        "teaser":null},{
        "title": "Event Sourcing",
        "excerpt":"Silverback.EventSourcing is a basic implementation of an event store that perfectly integrates within the Silverback ecosystem. At the moment only a version using Entity Framework Core is implemented, allowing to store the events in a database but other implementations may be added in the future.   Configuration   The only needed configuration is the call to UseDbContext&lt;TDbContext&gt; when initializing Silverback.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services.AddSilverback().UseDbContext&lt;MyDbContext&gt;()     } }    Creating the Event Store   Creating an event store is very straightforward and requires basically just 3 components: an aggregate entity model, the event store model and a repository.   Aggregate Entity model   The aggregate entity have to extend EventSourcingDomainEntity (or a custom class implementing IEventSourcingDomainEntity). The two generic type parameters refer to the type of the key (entity unique identifier) and the base type for the domain events (can be omited if you don’t need domain events).   public class Person : EventSourcingDomainEntity&lt;int, PersonDomainEvent&gt; {     public Person()     {     }      public Person(IEnumerable&lt;IEntityEvent&gt; events) : base(events)     {     }      public string Name { get; private set; }     public string SocialSecurityNumber { get; private set; }     public int Age { get; private set; }     public string PhoneNumber { get; private set; } }   The domain entity must have a constructor able to rebuild the entity state from the stored events.   The AddAndApplyEvent protected method must be used to add new events.   public class Person : EventSourcingDomainEntity&lt;int, PersonDomainEvent&gt; {     public void ChangeName(string newName) =&gt;         AddAndApplyEvent(new NameChangedEvent         {             NewName = newName         });      public void ChangeAge(int newAge) =&gt;         AddAndApplyEvent(new AgeChangedEvent         {             NewAge = newAge         });      public void ChangePhoneNumber(string newPhoneNumber) =&gt;         AddAndApplyEvent(new PhoneNumberChangedEvent         {             NewPhoneNumber = newPhoneNumber         }); }   An Apply method is needed for each event type to modify the entity current state according to the described mutation.   public class Person : EventSourcingDomainEntity&lt;int, PersonDomainEvent&gt; {     private void Apply(NameChangedEvent @event) =&gt; Name = @event.NewName;       private void Apply(AgeChangedEvent @event) =&gt; Age = @event.NewAge;      private void Apply(PhoneNumberChangedEvent @event, bool isReplaying)     {         PhoneNumber = @event.NewPhoneNumber;          // Fire domain event only if the event is new         if (!isReplaying)             AddEvent&lt;PhoneNumberChangedDomainEvent&gt;();     } }   The apply method can be private but it must have a specific signature: its name must begin with “Apply” and have a parameter of the specific event type (or base type). It can also receive an additional boolean parameter (isReplaying) that will let you differentiate between new events and events that are being reapplied because loaded from the store.   The events are just models inheriting from EntityEvent (or another custom class implementing IEntityEvent).   public class NameChangedEvent : EntityEvent  {      public string NewName { get; set; }  }  public class AgeChangedEvent : EntityEvent  {      public int NewAge { get; set; }  }  public class PhoneNumberChangedEvent : EntityEvent  {      public string NewPhoneNumber { get; set; }  }   Event Store model   The event store basically consists of an EventStore entity and related event (they either inherit from EventStoreEntity and EventEntity or implement the interfaces IEventStoreEntity and IEventEntity respectively).   public class PersonEventStore : EventStoreEntity&lt;PersonEvent&gt; {     [Key]     public int Id { get; set; }      public string SocialSecurityNumber { get; set; } }  public class PersonEvent : EventEntity {     [Key]     public int Id { get; private set; } }   The event store record can be extended with extra fields (see SocialSecurityNumber in the example above).   It is advised to add some indexes and a concurrency token, to ensure proper performance and consistency.   A DbSet must also be mapped to the defined event store entity and that’s it.   public class MyDbContext : DbContext {     public MyDbContext(DbContextOptions options) : base(options)     {     }      public DbSet&lt;PersonEventStore&gt; Persons { get; set; } }   EventStore repository   The repository is the component that is storing the aggregate entity in form of single events, being able to rebuild it afterwards.   The repository must inherit from DbContextEventStoreRepository and the 4 generic type parameters refer respectively to:     the aggregate entity   its unique key   the event store entity   its related event entity   The only thing left to implement is the mapping between the aggregate entity and the event store entity.   public class PersonEventStoreRepository     : DbContextEventStoreRepository&lt;Person, int, PersonEventStore, PersonEvent&gt; {     public PersonEventStoreRepository(DbContext dbContext)         : base(dbContext)     {     }      protected override PersonEventStore GetNewEventStoreEntity(         Person aggregateEntity) =&gt;         new PersonEventStore         {             Id = aggregateEntity.Id,             SocialSecurityNumber = aggregateEntity.SocialSecurityNumber         }; }   Storing and retrieving entities   Using the EventStoreRepository to store and retrieve aggregate entities is fairly simple. Have a look at the following code snippet to get an idea.   public class PersonService {     private readonly MyDbContext _dbContext;     private readonly PersonEventStoreRepository _repository =         new PersonEventStoreRepository(_dbContext);      public async Task&lt;Person&gt; CreatePerson(string name, int age)     {         var person = new Person();         person.ChangeName(\"Sergio\");         person.ChangeAge(35);          person = await repo.StoreAsync(person);         await _dbContext.SaveChangesAsync();          return person;     }      public async Task&lt;Person&gt; ChangePhoneNumber(         int personId,         string newPhoneNumber)     {         var person = repo.Get(p =&gt; p.Id == personId);          person.ChangePhoneNumber(newPhoneNumber);          person = await repo.StoreAsync(person);         await _dbContext.SaveChangesAsync();          return person;     } }   Merging events / handling conflicts   You may need to merge events coming from different sources and/or being received with a certain latency. In the example below the Apply method checks whether another (newer) conflicting event was added already in the meantime.   private void Apply(NameChangedEvent @event, bool isReplaying) {     // Skip if a newer event exists     if (!isReplaying &amp;&amp; Events.Any(e =&gt;          e is NameChangedEvent &amp;&amp;         e.Timestamp &gt; @event.Timestamp))     {         return;     }      Name = @event.NewName; }  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/quickstart/event-sourcing",
        "teaser":null},{
        "title": "Endpoint",
        "excerpt":"The endpoint object contains all information that identify the topic/queue that is being connected and all the configurations. The endpoint object is therefore very specific and every broker type will define it’s own implementation of IEndpoint.   Kafka   Silverback.Integration.Kafka uses 2 different classes to specify inbound and outbound endpoints configuration.   For a more in-depth documentation about the Kafka configuration refer to the confluent-kafka-dotnet documentation.   KafkaProducerEndpoint   Used for outbound endpoints, exposes the following properties:                  Property       Description                       Name       The name of the topic. This is set in the constructor.                 Serializer       The IMessageSerializer to be used to deserialize the messages. The default is a JsonMessageSerializer using UTF-8 encoding. See Serialization for details.                 Encryption       Enable end-to-end message encryption. See Encryption for details.                 Chunk       Enable chunking to efficiently deal with large messages. See Chunking for details.                 Configuration       An instance of KafkaProducerConfig, that’s just an extension of Confluent.Kafka.ProducerConfig.                 Configuration.BootstrapServers, …       All properties inherited from Confluent.Kafka.ProducerConfig. See confluent-kafka-dotnet documentation for details.                 Configuration.ThrowIfNotAcknowledged       When set to true an exception will be thrown in the producer if no acknowledge is received by the broker (PersistenceStatus.PossiblyPersisted). The default is true.           new KafkaProducerEndpoint(\"basket-events\") {     Configuration = new KafkaProducerConfig     {         BootstrapServers = \"PLAINTEXT://kafka:9092\"     },     Chunk = ... }   KafkaConsumerEndpoint   Used for inbound endpoints, exposes the following properties:                  Property       Description                       Names       The name of the topics to be consumed (yes, you can subscribe multiple topics at once). This is set in the constructor.                 Serializer       The IMessageSerializer to be used to serialize the messages. The default is a JsonMessageSerializer using UTF-8 encoding.                 Encryption       Enable end-to-end message encryption. See Encryption for details.                 Configuration       An instance of KafkaConsumerConfig, that’s just an extension of Confluent.Kafka.ConsumerConfig.                 Configuration.BootstrapServers, Configuration.GroupId, …       All properties inherited from Confluent.Kafka.ConsumerConfig. See confluent-kafka-dotnet documentation for details.                 Configuration.CommitOffsetEach       When auto-commit is disable, defines the number of message processed before committing the offset to the server. The most reliable level is 1 but it reduces throughput.                 EnableAutoRecovery       When set to true the consumer will be automatically restarted if a KafkaException is thrown while polling/consuming. The default is true.           new KafkaConsumerEndpoint(     \"order-events\",      \"inventory-events\") {     Configuration = new KafkaConsumerConfig     {         BootstrapServers = \"PLAINTEXT://kafka:9092\",         GroupId = \"my-consumer\",         AutoOffsetReset = AutoOffsetResetType.Earliest     } }   You can decide whether to use one consumer per topic or subscribe multiple topics with the same consumer (passing multiple topic names in the endpoint constructor, as shown in the example above). There are advantages and disadvantages of both solutions and the best choice really depends on your specific requirements, the amount of messages being produced, etc. Anyway the main difference is that when subscribing multiple topics you will still consume one message after the other but they will simply be interleaved (this may or may not be an issue, it depends) and on the other hand each consumer will use some resources, so creating multiple consumers will result in a bigger overhead.   RabbitMQ   Silverback.Integration.RabbitMQ is a bit more intricated and uses 4 different classes to specify inbound and outbound endpoints configuration.   For a more in-depth documentation about the RabbitMQ configuration refer to the RabbitMQ tutorials and documentation.   RabbitQueueProducerEndpoint   Used for outbound endpoints that produce directly to a queue, exposes the following properties:                  Property       Description                       Name       The name of the queue. This is set in the constructor.                 Serializer       The IMessageSerializer to be used to deserialize the messages. The default is a JsonMessageSerializer using UTF-8 encoding. See Serialization for details.                 Encryption       Enable end-to-end message encryption. See Encryption for details.                 Chunk       Enable chunking to efficiently deal with large messages. See Chunking for details.                 Connection       An instance of RabbitConnectionConfig. It exposes the properties necessary to setup the connection with RabbitMQ.                 Connection.HostName, …       All properties exposed by the RabbitMQ.Client.ConnectionFactory. See RabbitMQ .NET/C# Client API Guide for details.                 Queue       An instance of RabbitQueueConfig that specifies the queue configuration.                 Queue.IsDurable       Specifies whether the queue will survive a broker restart. The default is true.                 Queue.IsAutoDeleteEnabled       Specifies whether the queue will be automatically deleted when the last consumer unsubscribes. The default is false.                 Queue.IsExclusive       Specifies whether the queue is used by only one connection and will be deleted when that connection closes. The default is false.                 Queue.Arguments       The optional arguments dictionary used by plugins and broker-specific features to configure values such as message TTL, queue length limit, etc.                 ConfirmationTimeout       The maximum amount of time to wait for the message produce to be acknowledge before considering it failed. Set it to null to proceed without waiting for a positive or negative acknowledgment. The default is a quite conservative 5 seconds.           new RabbitQueueProducerEndpoint(\"inventory-commands-queue\") {     Connection = new RabbitConnectionConfig     {         HostName = \"localhost\",         UserName = \"guest\",         Password = \"guest\"     },     Queue = new RabbitQueueConfig     {         IsDurable = true,         IsExclusive = false,         IsAutoDeleteEnabled = false     } }   RabbitExchangeProducerEndpoint   Used for outbound endpoints that produce to an exchange, exposes the following properties:                  Property       Description                       Name       The name of the exchange. This is set in the constructor.                 Serializer       The IMessageSerializer to be used to deserialize the messages. The default is a JsonMessageSerializer using UTF-8 encoding. See Serialization for details.                 Encryption       Enable end-to-end message encryption. See Encryption for details.                 Chunk       Enable chunking to efficiently deal with large messages. See Chunking for details.                 Connection       An instance of RabbitConnectionConfig. It exposes the properties necessary to setup the connection with RabbitMQ.                 Connection.HostName, …       All properties exposed by the RabbitMQ.Client.ConnectionFactory. See RabbitMQ .NET/C# Client API Guide for details.                 Exchange       An instance of RabbitExchangeConfig that specifies the exchange configuration.                 Exchange.ExchangeType       The exchange type. It should match with one of the constants declared in the RabbitMQ.Client.ExchangeType static class.                 Exchange.IsDurable       Specifies whether the queue will survive a broker restart. The default is true.                 Exchange.IsAutoDeleteEnabled       Specifies whether the queue will be automatically deleted when the last consumer unsubscribes. The default is false.                 Exchange.Arguments       The optional arguments dictionary used by plugins and broker-specific features to configure values such as message TTL, queue length limit, etc.                 ConfirmationTimeout       The maximum amount of time to wait for the message produce to be acknowledge before considering it failed. Set it to null to proceed without waiting for a positive or negative acknowledgment. The default is a quite conservative 5 seconds.           new RabbitExchangeProducerEndpoint(\"order-events\") {     Connection = new RabbitConnectionConfig     {         HostName = \"localhost\",         UserName = \"guest\",         Password = \"guest\"     },     Exchange = new RabbitExchangeConfig     {         IsDurable = true,         IsAutoDeleteEnabled = false,         ExchangeType = ExchangeType.Fanout     } }      RabbitQueueConsumerEndpoint   Used for inbound endpoints that consume directly from a queue, exposes the following properties:                  Property       Description                       Name       The name of the queue. This is set in the constructor.                 Serializer       The IMessageSerializer to be used to deserialize the messages. The default is a JsonMessageSerializer using UTF-8 encoding. See Serialization for details.                 Encryption       Enable end-to-end message encryption. See Encryption for details.                 Connection       An instance of RabbitConnectionConfig. It exposes the properties necessary to setup the connection with RabbitMQ.                 Connection.HostName, …       All properties exposed by the RabbitMQ.Client.ConnectionFactory. See RabbitMQ .NET/C# Client API Guide for details.                 Queue       An instance of RabbitQueueConfig that specifies the queue configuration.                 Queue.IsDurable       Specifies whether the queue will survive a broker restart. The default is true.                 Queue.IsAutoDeleteEnabled       Specifies whether the queue will be automatically deleted when the last consumer unsubscribes. The default is false.                 Queue.IsExclusive       Specifies whether the queue is used by only one connection and will be deleted when that connection closes. The default is false.                 Queue.Arguments       The optional arguments dictionary used by plugins and broker-specific features to configure values such as message TTL, queue length limit, etc.                 AcknowledgeEach       Defines the number of message processed before sending the acknowledgment to the server. The most reliable level is 1 but it reduces throughput.                 PrefetchSize       Defines the QoS prefetch size parameter for the consumer. See RabbitMQ Consumer Prefetch documentation for details.                 PrefetchCount       Defines the QoS prefetch count parameter for the consumer. See RabbitMQ Consumer Prefetch documentation for details.           new RabbitQueueConsumerEndpoint(\"inventory-commands-queue\") {     Connection = new RabbitConnectionConfig     {         HostName = \"localhost\",         UserName = \"guest\",         Password = \"guest\"     },     Queue = new RabbitQueueConfig     {         IsDurable = true,         IsExclusive = false,         IsAutoDeleteEnabled = false     } }   RabbitExchangeConsumerEndpoint   Used for inbound endpoints that consume from an exchange, exposes the following properties:                  Property       Description                       Name       The name of the exchange. This is set in the constructor.                 Serializer       The IMessageSerializer to be used to deserialize the messages. The default is a JsonMessageSerializer using UTF-8 encoding. See Serialization for details.                 Encryption       Enable end-to-end message encryption. See Encryption for details.                 Connection       An instance of RabbitConnectionConfig. It exposes the properties necessary to setup the connection with RabbitMQ.                 Connection.HostName, …       All properties exposed by the RabbitMQ.Client.ConnectionFactory. See RabbitMQ .NET/C# Client API Guide for details.                 Exchange       An instance of RabbitExchangeConfig that specifies the exchange configuration.                 Exchange.ExchangeType       The exchange type. It should match with one of the constants declared in the RabbitMQ.Client.ExchangeType static class.                 Exchange.IsDurable       Specifies whether the queue will survive a broker restart. The default is true.                 Exchange.IsAutoDeleteEnabled       Specifies whether the queue will be automatically deleted when the last consumer unsubscribes. The default is false.                 Exchange.Arguments       The optional arguments dictionary used by plugins and broker-specific features to configure values such as message TTL, queue length limit, etc.                 QueueName       The desired queue name. If null or empty a random name will be generated by RabbitMQ. (A queue is always necessary to consume.)                 Queue       An instance of RabbitQueueConfig that specifies the queue configuration.                 Queue.IsDurable       Specifies whether the queue will survive a broker restart. The default is true.                 Queue.IsAutoDeleteEnabled       Specifies whether the queue will be automatically deleted when the last consumer unsubscribes. The default is false.                 Queue.IsExclusive       Specifies whether the queue is used by only one connection and will be deleted when that connection closes. The default is false.                 Queue.Arguments       The optional arguments dictionary used by plugins and broker-specific features to configure values such as message TTL, queue length limit, etc.                 RoutingKey       The routing key (aka binding key) to be used to bind with the exchange.                 ConfirmationTimeout       The maximum amount of time to wait for the message produce to be acknowledge before considering it failed. Set it to null to proceed without waiting for a positive or negative acknowledgment. The default is a quite conservative 5 seconds.                 AcknowledgeEach       Defines the number of message processed before sending the acknowledgment to the server. The most reliable level is 1 but it reduces throughput.                 PrefetchSize       Defines the QoS prefetch size parameter for the consumer. See RabbitMQ Consumer Prefetch documentation for details.                 PrefetchCount       Defines the QoS prefetch count parameter for the consumer. See RabbitMQ Consumer Prefetch documentation for details.           new RabbitExchangeConsumerEndpoint(\"order-events\") {     Connection = new RabbitConnectionConfig     {         HostName = \"localhost\",         UserName = \"guest\",         Password = \"guest\"     },     Exchange = new RabbitExchangeConfig     {         IsDurable = true,         IsAutoDeleteEnabled = false,         ExchangeType = ExchangeType.Fanout     },     QueueName = \"my-consumer-group\",     Queue = new RabbitQueueConfig     {         IsDurable = true,         IsExclusive = false,         IsAutoDeleteEnabled = false     } }     ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/configuration/endpoint",
        "teaser":null},{
        "title": "Outbound Connector",
        "excerpt":"The outbound connector is used to automatically relay the integration messages (published to the internal bus) to the message broker. Multiple outbound endpoints can be configured and Silverback will route the messages according to their type (based on the TMessage parameter passed to the AddOutbound&lt;TMessage&gt; method.   Implementations   Multiple implementations of the connector are available, offering a variable degree of reliability.   Basic   The basic OutboundConnector is very simple and relays the messages synchronously. This is the easiest, better performing and most lightweight option but it doesn’t allow for any transactionality (once the message is fired, is fired) nor resiliency to the message broker failure.    \t     Messages 1, 2 and 3 are directly produced to the message broker.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddOutboundConnector());     }      public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddOutbound&lt;IIntegrationEvent&gt;(                     new KafkaProducerEndpoint(\"basket-events\")                     {                         ...                     }));     } }    Deferred   The DeferredOutboundConnector stores the messages into a local queue to be forwarded to the message broker in a separate step.   This approach has two main advantages:     Fault tollerance: you depend on the database only and if the message broker is unavailable the produce will be automatically retried later on   Transactionality: when using the database a storage you can commit the changes to the local database and the outbound messages inside a single atomic transaction (this pattern is called transactional outbox)    \t     Messages 1, 2 and 3 are stored in the outbox table and produced by a separate thread or process.   Database outbox table   The DbOutboundConnector will store the outbound messages into a database table.   When using entity framework (UseDbContext&lt;TDbContext&gt; contained in the Silverback.EntityFrameworkCore package) the outbound messages are stored into a DbSet and are therefore implicitly saved in the same transaction used to save all other changes.   The DbContext must include a DbSet&lt;OutboundMessage&gt; and an OutboundWorker is to be started to process the outbound queue. See also the sample DbContext.   The current OutboundWorker cannot be horizontally scaled and starting multiple instances will cause the messages to be produced multiple times. In the following example a distributed lock in the database is used to ensure that only one instance is running and another one will immediatly take over when it stops (the DbContext must include a DbSet&lt;Lock&gt; as well, see also the sample DbContext).    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()              // Initialize Silverback to use MyDbContext as database storage.             .UseDbContext&lt;MyDbContext&gt;()              // Setup the lock manager using the database             // to handle the distributed locks.             // If this line is omitted the OutboundWorker will still             // work without locking.              .AddDbDistributedLockManager()              .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 // Use a deferred outbound connector                 .AddDbOutboundConnector()                  // Add the IHostedService processing the outbound queue                 // (overloads are available to specify custom interval,                 // lock timeout, etc.)                 .AddDbOutboundWorker();     } }    Custom outbound queue   You can easily create another implementation targeting another kind of storage, simply creating your own IOutboundQueueProducer and IOutboundQueueConsumer and plug them in.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .AddDbDistributedLockManager()              .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddOutboundConnector&lt;SomeCustomQueueProducer&gt;()                 .AddOutboundWorker&lt;SomeCustomQueueConsumer&gt;();     } }    Subscribing locally   The published messages that are routed to an outbound endpoint cannot be subscribed locally (within the same process), unless explicitely desired.    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddOutbound&lt;IIntegrationEvent&gt;(...)                 .PublishOutboundMessagesToInternalBus()             );     } }    What said above is only partially true, as you can subscribe to the wrapped message (IOutboundEnvelope&lt;TMessage&gt;) even without calling PublishOutboundMessagesToInternalBus.   Producing the same message to multiple endpoints   An outbound route can point to multiple endpoints resulting in every message being broadcasted to all endpoints.    \t     Messages 1, 2 and 3 are published to both topics simultaneously.    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddOutbound&lt;IIntegrationCommand&gt;(                     new KafkaProducerEndpoint(\"topic-1\")                     {                         ...                     },                     new KafkaProducerEndpoint(\"topic-2\")                     {                         ...                     }));     } }    A message will also be routed to all outbound endpoint mapped to a type that matches the message type. In the example below an OrderCreatedMessage (that inherits from OrderMessage) would be sent to both endpoints.    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddOutbound&lt;OrderMessage&gt;(                     new KafkaProducerEndpoint(\"topic-1\")                     {                         ...                     })                 .AddOutbound&lt;OrderCreatedMessage&gt;(                     new KafkaProducerEndpoint(\"topic-1\")                     {                         ...                     }));     } }    Dynamic custom routing   By default Silverback routes the messages according to their type and the static configuration defined at startup. In some cases you may need more flexibility, being able to apply your own routing rules. In such cases it is possible to implement a fully customized router.    \t     The messages are dynamically routed to the appropriate endpoint.   In the following example a custom router is used to route the messages according to their priority (a copy is also sent to a catch-all topic).   public class PrioritizedRouter : OutboundRouter&lt;IPrioritizedCommand&gt; {     private static readonly IProducerEndpoint HighPriorityEndpoint =         new KafkaProducerEndpoint(\"high-priority\")         {             ...         };     private static readonly IProducerEndpoint NormalPriorityEndpoint =         new KafkaProducerEndpoint(\"normal-priority\")         {             ...         };     private static readonly IProducerEndpoint LowPriorityEndpoint =         new KafkaProducerEndpoint(\"low-priority\")         {             ...         };     private static readonly IProducerEndpoint AllMessagesEndpoint =         new KafkaProducerEndpoint(\"all\")         {             ...         };      public override IEnumerable&lt;IProducerEndpoint&gt; Endpoints     {         get         {             yield return AllMessagesEndpoint;             yield return LowPriorityEndpoint;             yield return NormalPriorityEndpoint;             yield return HighPriorityEndpoint;         }     }      public override IEnumerable&lt;IProducerEndpoint&gt; GetDestinationEndpoints(         IPrioritizedCommand message,         MessageHeaderCollection headers)     {         yield return AllMessagesEndpoint;                  switch (message.Priority)         {             case MessagePriority.Low:                 yield return LowPriorityEndpoint;                 break;             case MessagePriority.High:                 yield return HighPriorityEndpoint;                 break;             default:                 yield return NormalPriorityEndpoint;                 break;         }     } }    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options.AddKafka())             .AddSingletonOutboundRouter&lt;PrioritizedRouter&gt;();     }      public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddOutbound&lt;IPrioritizedCommand, PrioritizedRouter&gt;());     } }   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/configuration/outbound",
        "teaser":null},{
        "title": "Inbound Connector",
        "excerpt":"The inbound connector is used to automatically consume a topic/queue and relay the messages to the internal bus.   The inbound connector abstracts the message broker completely and the messages are automatically acknowledged if the subscribers complete without throwing an exception (unless error handling policies are defined and unless batch processing).   Implementations   Multiple implementations are available, offering a variable degree of reliability.   Basic   The basic InboundConnector is very simple and just forwards the consumed messages to the internal bus. If no exception is thrown, the message is committed and the next one is consumed.    \t     The messages are consumed directly.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddInboundConnector());     }      public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddInbound(                     new KafkaConsumerEndpoint(\"basket-events\")                     {                         ...                     }));     } }    Exactly-once processing   Silverback is able to keep track of the messages that have been consumed in order to guarantee that each message is processed exactly once.   Offset storage   The DbOffsetStoredInboundConnector will store the offset of the latest processed message (of each topic/partition) into a database table.    \t     The offsets are being stored to prevent the very same message to be consumed twice.   The Silverback.EntityFrameworkCore package is also required and the DbContext must include a DbSet&lt;StoredOffset&gt;. See also the sample DbContext.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .UseDbContext&lt;MyDbContext&gt;()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddDbOffsetStoredInboundConnector());     } }    Logged   The DbLoggedInboundConnector will store all the processed messages into a database table. This has the double purpose of serving as a log in addition to preventing double processing.    \t     The inbound messages are logged to prevent two messages with the same key to be consumed.   The Silverback.EntityFrameworkCore package is also required and the DbContext must include a DbSet&lt;InboundMessage&gt;. See also the sample DbContext.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .UseDbContext&lt;MyDbContext&gt;()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddDbLoggedInboundConnector());     } }    Custom store   You can easily implement your own storage for the offsets or the messages, simply creating your own IOffsetStore or IInboundLog and plugging them in.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddLoggedInboundConnector&lt;SomeCustomInboundLog&gt;());     } }     Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddOffsetStoredInboundConnector&lt;SomeCustomOffsetStore&gt;());     } }    Error handling   If an exceptions is thrown by the methods consuming the incoming messages (subscribers) the consumer will stop, unless some error policies are defined.                  Policy       Description                       Skip       This is the simplest policy: just ignore the message and go ahead. Use the LogWithLevel method to specify the log level to be applied the “message skipped” log entry (default is Error).                 Retry       Define how many times and at which interval to retry to process the message. Be aware that this will block the consumer.                 Move       Used to re-publish the message to the specified endpoint, this policy is very flexible and allow quite a few scenarios: move to same topic to retry later on without blocking, move to a retry topic to delay the retry or move to a failed messages topic. The message can also be transformed, to allow adding useful information (e.g. source, error type, etc.) that will allow for better handling while reprocessing.                 Chain       Combine different policies, for example to move the message to a dead letter after some retries.            Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddInbound(                     new KafkaConsumerEndpoint(\"some-events\")                     {                         ...                     },                     policy =&gt; policy.Chain(                         policy.Retry().MaxFailedAttempts(3),                         policy.Move(new KafkaProducerEndpoint(\"bad-messages\")                             {                                 ...                             }                         ))));     } }    If the processing still fails after the last policy is applied the inbound connector will return the exception to the consumer, causing it to stop. A Retry (with limited amount of attempts) alone is therefore not recommendend and it should be combined with Skip or Move.   Retries   Retry and Move policies can be used to retry over and over the same message. Use MaxFailedAttempts to limit the number of attempts.   policy.Chain(     policy.Retry(TimeSpan.FromSeconds(1)).MaxFailedAttempts(3),     policy.Skip().LogWithLevel(LogLevel.Critical))   A message can be moved to the same topic to simply be moved to the end of the queue.   The Retry policy will prevent the message broker to be polled for the entire comulative duration of the attempts and it could lead to timeouts. With Kafka you should for example set the max.poll.interval.ms settings to an higher value.   Apply rules   Use ApplyTo and Exclude methods to decide which exceptions must be handled by the error policy or take advantage of  ApplyWhen to specify a custom apply rule.   policy.Move(new KafkaProducerEndpoint(\"same-endpoint\") { ... })     .Exclude&lt;MyException&gt;()     .ApplyWhen((msg, ex) =&gt; msg.Xy == myValue)   Publishing messages (events)   Messages can be published when a policy is applied, in order to execute custom code.    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddInbound(                     new KafkaConsumerEndpoint(\"some-events\")                     {                         ...                     },                     policy =&gt; policy.Chain(                         policy                             .Retry(TimeSpan.FromMilliseconds(500))                             .MaxFailedAttempts(3),                         policy                             .Skip()                             .Publish(msg =&gt; new ProcessingFailedEvent(msg))                     )));     } }    public void OnProcessingFailed(ProcessingFailedEvent @event) {     _processingStatusService.SetFailed(@event.Message.Id);      _mailService.SendNotification(\"Failed to process message!\"); }   Batch processing   The inbound connector can be configured to process the messages in batches.    \t     The messages are processed in batches.                  Property       Description                       Batch.Size       The number of messages to be processed in batch. The default is 1.                 Batch.MaxWaitTime       The maximum amount of time to wait for the batch to be filled. After this time the batch will be processed even if the desired Size is not reached. Set it to TimeSpan.MaxValue to disable this feature. The default is TimeSpan.MaxValue.                 Batch.MaxDegreeOfParallelism       The maximum number of parallel threads used to process the messages in the batch. The default is 1.            Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddInbound(                     new KafkaConsumerEndpoint(\"basket-events\")                     {                         ...                     },                     settings: new InboundConnectorSettings                     {                         Batch = new Messaging.Batch.BatchSettings                         {                             Size = 5,                             MaxWaitTime = TimeSpan.FromSeconds(5)                         }                     }));     } }    The batch is consider a unit of work: it will be processed in the same DI scope, it will be atomically committed, the error policies will be applied to the batch as a whole and all messages will be acknowledged at once when the batch is successfully processed.   Some additional events are published to the internal bus when batch processing:                  Event       Description                       BatchStartedEvent       Fired when the batch has been filled and just before the first message is published. This event can be subscribed to perform some operations before the messages are processed.                 BatchCompleteEvent       Fired when all the messages in a batch have been published.                 BatchProcessedEvent       Fired after all messages have been successfully processed. It can tipically be used to commit the transaction.                 BatchAbortedEvent       Fired when an exception occured during the processing of the batch. It can tipically be used to rollback the transaction.           The usage should be similar to the following examples.   public class InventoryService : ISubscriber {     private DbContext _db;      public InventoryService(MyDbContext db)     {         _db = db;     }      public void OnBatchStarted(BatchStartedEvent message)     {         _logger.LogInformation(             $\"Processing batch '{message.BatchId} \" +             $\"({message.BatchSize} messages)\");     }      public void OnMessageReceived(InventoryUpdateEvent @event)     {         // Process the event (but don't call SaveChanges)     }      public async Task OnBatchProcessed(BatchProcessedEvent message)     {         // Commit all changes in a single transaction         await _db.SaveChangesAsync();          _logger.LogInformation(             $\"Successfully processed batch '{message.BatchId} \" +             $\"({message.BatchSize} messages)\");     }          public void OnBatchAborted(BatchAbortedEvent message)     {         _logger.LogError(             $\"An error occurred while processing batch '{message.BatchId} \" +             $\"({message.BatchSize} messages)\");     } }   …or…   public class InventoryService : ISubscriber {     private DbContext _db;      public InventoryService(MyDbContext db)     {         _db = db;     }      public void OnBatchStarted(BatchStartedEvent message)     {     }      public async Task OnMessageReceived(         IReadOnlyCollection&lt;InventoryUpdateEvent&gt; events)     {         _logger.LogInformation(             $\"Processing {events.Count} messages\");          // Process all items         foreach (var event in events)         {             ...         }          // Commit all changes in a single transaction         await _db.SaveChangesAsync();          _logger.LogInformation(             $\"Successfully processed {events.Count} messages\");     } }   The method OnMessageReceived could declare an argument of type IReadOnlyCollection&lt;InventoryUpdateEvent&gt; instead of IEnumerable&lt;InventoryUpdateEvent&gt;. (Silverback will in any case always forward a materialized IList of messages, but explicitly declaring the paramter as IReadOnlyCollection&lt;T&gt; avoids any false positive “possible multiple enumeration of IEnumerable” issue that may be detected by a static code analysis tool.)   Multi-threaded consuming   Multiple consumers can be created for the same endpoint to consume in parallel in multiple threads (you need multiple partitions in Kafka).    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddInbound(                     new KafkaConsumerEndpoint(\"basket-events\")                     {                         ...                     },                     settings: new InboundConnectorSettings                     {                         Consumers: 2                     }));     } }   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/configuration/inbound",
        "teaser":null},{
        "title": "Mixed Configurations",
        "excerpt":"There is no limit to the amount of endpoints, configurations and implementations of the inbound/outbound connectors used within a single bus or broker.   In the following example different outbound connectors are mixed. Silverback will simply use the first one by default, unless the type is not explicitly specified when configuring the endpoints.    Startup.cs  public class Startup {     protected override void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .UseDbContext&lt;MyDbContext&gt;()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 // Use the DeferredOutboundConnector by default                 .AddDeferredOutboundConnector()                 // ...but register the simple OutboundConnector as well                 .AddOutboundConnector());     }      public void Configure(BusConfigurator busConfigurator)     {         ConfigureNLog(serviceProvider);          busConfigurator             .Connect(endpoints =&gt; endpoints                 // This endpoint will use DeferredOutboundConnector                 .AddOutbound&lt;IEvent&gt;(new KafkaConsumerEndpoint(\"order-events\")                 {                     ...                 })                 // ...and this endpoint will use the simple OutboundConnector instead                 .AddOutbound&lt;SomeCommand, OutboundConnector&gt;(new KafkaConsumerEndpoint(\"some-commands\")                 {                     ...                 }));     } }   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/configuration/mixed",
        "teaser":null},{
        "title": "External Configuration",
        "excerpt":"Alternatively the package Silverback.Integration.Configuration can be used to setup the endpoints from the IConfiguration provided by Microsoft.Extensions.Configuration (configuration usually coming either from the appsettings.json and/or the environment variables).   To do so the startup code has to be slightly adapted as follows.    Startup.cs  public class Startup {     public Startup(IConfiguration configuration)     {         Configuration = configuration;     }      public IConfiguration Configuration { get; }      public void Configure(IApplicationBuilder app, BusConfigurator busConfigurator))     {         busConfigurator.Connect(endpoints =&gt; endpoints             .ReadConfig(Configuration, app.ApplicationServices));     } }    And here is an example of configuration in the appsettings.json file:  {   \"Silverback\": {     \"Using\": [ \"Silverback.Integration.Kafka\", \"Silverback.Core.Model\" ],     \"Inbound\": [       {         \"Endpoint\": {           \"Type\": \"KafkaConsumerEndpoint\",           \"Name\": \"catalog-events\",           \"Configuration\": {             \"BootstrapServers\": \"PLAINTEXT://kafka:9092\",             \"GroupId\": \"basket-service\",             \"ClientId\": \"basket-service\",             \"AutoOffsetReset\": \"Earliest\"           },         \"Settings\": {           \"Consumers\": 2,           \"Batch\": {             \"Size\": 100,             \"MaxWaitTime\": \"00:00:02.500\"           }         },        },         \"ErrorPolicies\": [           {             \"Type\": \"Retry\",             \"MaxFailedAttempts\": 5,             \"DelayIncrement\": \"00:00:30\"           },           {             \"Type\": \"Move\",             \"Endpoint\": {               \"Type\": \"KafkaProducerEndpoint\",               \"Name\": \"basket-failedmessages\"             }           }         ]       }     ],     \"Outbound\": [       {         \"MessageType\": \"IIntegrationEvent\",         \"Endpoint\": {           \"Type\": \"KafkaProducerEndpoint\",           \"Name\": \"basket-events\",           \"Configuration\": {             \"BootstrapServers\": \"PLAINTEXT://kafka:9092\",             \"ClientId\": \"basket-service\"           },           \"Chunk\": {               \"Size\": 3           }         }       }     ]   } }   This approach is discouraged and the package may even be discontinued in a future release because not all features and all the possible configurations can be supported. The configuration will also quickly become very verbose and repetitive. For these reasons it is advised to use the fluent API to configure Silverback.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/configuration/external",
        "teaser":null},{
        "title": "Serialization",
        "excerpt":"Being flexible when serializing and deserializing the messages sent over the message broker is crucial for interoperability and these mechanisms are therfore completely customizable.   Default JSON serialization   The default JsonMessageSerializer internally uses Newtonsoft.Json to serialize the messages as json. The messages are then transformed in a byte array using the UTF8 encoder.   A few headers are added to the message, in particular x-message-type is used by the JsonMessageSerializer to know the message type when deserializing. It also leverages the Newtonsoft’s automatic type handling to automatically resolve the actual type of the nested properties.   The deserializer function provided by JsonMessageSerializer will obviously try to map the message to a type with the exact assembly qualified name found in the x-message-type header. It is therefore a good practice to share the message models among the services, maybe through nuget.   This is the suggested serialization strategy when both producer and consumer are based on Silverback but may not be ideal for interoperability.   Have a look at the Message Headers section for an overview on the headers that are appended to the messages.   Fixed-type JSON for interoperability   If you are consuming a message coming from another system (not based on Silverback), chances are that the type name is not being delivered as header.   In that case you can resort to the typed JsonMessageSerializer&lt;TMessage&gt;. This serializer works like the default one but the message type is hard-coded, instead of being expected in the header.    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddInbound(                     new KafkaConsumerEndpoint(\"order-events\")                     {                         Serializer = new JsonMessageSerializer&lt;OrderEvent&gt;                     }));     } }    The JsonMessageSerializer can be also be tweaked modifying its Settings and Encoding.   Apache Avro   The AvroSerializer contained in the Silverback.Integration.Kafka.SchemaRegistry package can be used to connect with a schema registry and exchange messages in Apache Avro format.    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddOutbound&lt;OrderEvent&gt;(                     new KafkaConsumerEndpoint(\"order-events\")                     {                         Serializer = new AvroMessageSerializer&lt;OrderEvent&gt;                         {                             SchemaRegistryConfig = new SchemaRegistryConfig                             {                                 Url = \"schema-registry:8081\"                             },                             AvroSerializerConfig = new AvroSerializerConfig                             {                                 AutoRegisterSchemas = true                             }                         },                     }));     } }    The C# message models can be generated from an Avro schema using AvroGen.   This serializer is built for Kafka but it could work with other brokers, as long as a schema registry is available.   Custom serializer   In some cases you may want to build your very own custom serializer implementing IMessageSerializer directly.   public class MyCustomSerializer : IMessageSerializer {     public byte[] Serialize(object message, MessageHeaderCollection messageHeaders)     {         ...     }      public object Deserialize(byte[] message, MessageHeaderCollection messageHeaders)     {         ...     }      public byte[] Serialize(         object message,         MessageHeaderCollection messageHeaders,         MessageSerializationContext context)     {         ...     }      public object Deserialize(         byte[] message,         MessageHeaderCollection messageHeaders,         MessageSerializationContext context)     {         ...     }      public Task&lt;byte[]&gt; SerializeAsync(         object message,         MessageHeaderCollection messageHeaders,         MessageSerializationContext context)     {         ...     }      public Task&lt;object&gt; DeserializeAsync(         byte[] message,         MessageHeaderCollection messageHeaders,         MessageSerializationContext context)     {         ...     } }   You may need to implement IKafkaMessageSerializer if you want to have full control over the serialization of the Kafka key as well.   Binary Files   Please refer to the Binary Files page if you need to prduce or consume raw binary files.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/advanced/serialization",
        "teaser":null},{
        "title": "Chunking",
        "excerpt":"The message brokers are usually very efficient at handling huge amount of relatively small messages. In order to make the most out of it you may want to split your largest messages (e.g. containing binary data) into smaller chunks. Silverback can handle such scenario transparently, reassembling the message automatically in the inbound connector before pushing it to the internal bus.   Producer configuration   The producer endpoint can be configured to split the message into chunks by specifying their maximum size (in byte).    \t     The messages are being split into small chunks.   new KafkaProducerEndpoint(\"silverback-examples-events\") {     Configuration = new KafkaProducerConfig     {         ...     },     Chunk = new ChunkSettings     {         Size = 100000     } }   Consumer configuration   An IChunkStore implementation is needed to temporary store the chunks until the full message has been received. Silverback has two built-in implementations: the InMemoryChunkStore and the DbChunkStore.    \t     The message chunks are automatically aggregated once the full message is received.   In-Memory temporary store   The InMemoryChunkStore should be your default choice as it doesn’t require any extra storage. As the name suggests, the chunks are simply kept in memory until the full message can be rebuilt and consumed.   This approach has of course some limitations and requires that all chunks are received by the same consumer: with Kafka this is usually ensured by setting the same key to all chunks, thing that is done automatically by Silverback setting the same key and the same headers to all chunks (except for the chunk index header, of course).    \t     Kafka: without a key the messages are written to a random partition.   \t     Kafka: with the key being set the chunks related to the same message will land in the same partition.   On the other hand it isn’t a big deal if multiple producers are writing in the same topic/queue and the chunks of different message are interleaved. Silverback will commit the offsets only when no chunk is left in memory, to prevent any message loss.    \t     Silverback handles the inteleaved chunks transparently.   It isn’t guaranteed that you will never consume a message twice, if you use multiple producers to write into the topic or queue. It is up to you to ensure idempotency or enable exactly-once processing through the inbound connector (see Inbound Connector). {.notice–important}    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .UseDbContext&lt;MyDbContext&gt;()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddInMemoryChunkStore());     } }    The AddInMemoryChunkStore method has some optional parameters to configure the maximum chunks retention (in case of a producer failure, you may receive only part of the chunks of a given message and without this setting they would stay in memory forever). {.notice–info}   Database temporary store   The DbChunkStore will temporary store the chunks into a database table and the inbound connector will rebuild the original message as soon as all chunks have been received. This table can be shared between multiple consumers, making it safer when chunks aren’t properly written to the same partition.    \t     The chunks are persisted to a database table that is shared across the consumers.   The Silverback.EntityFrameworkCore package is also required and the DbContext must include a DbSet&lt;TemporaryMessageChunk&gt;. See also the sample DbContext.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .UseDbContext&lt;MyDbContext&gt;()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddDbChunkStore());     } }    The AddDbChunkStore method has some optional parameters to configure the maximum chunks retention (in case of a producer failure, you may receive only part of the chunks of a given message and without this setting they would stay in the database forever). {.notice–info}   Custom chunk store   It is of course possible to create other implementations of IChunkStore to use another kind of storage for the message chunks.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .UseDbContext&lt;MyDbContext&gt;()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddChunkStore&lt;SomeCustomChunkStore&gt;());     } }    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/advanced/chunking",
        "teaser":null},{
        "title": "Encryption",
        "excerpt":"The end-to-end message encryption in Silverback is handled transparently in the produce/consume pipeline and works independently from the used serializer or other features like chunking.    \t     The messages are transparently encrypted and decrypted.   Symmetric encryption   Enabling the end-to-end encryption using a symmetric algorithm just require an extra configuration in the endpoint (a KafkaConsumerEndpoint is used in the example below but the Encryption property is available in all enpoint types).   new KafkaConsumerEndpoint(\"sensitive-data\") {     Configuration = new KafkaProducerConfig     {         BootstrapServers = \"PLAINTEXT://kafka:9092\"     },     Encryption = new SymmetricEncryptionSettings     {         //AlgorithmName = \"AES\", (AES is the default algorithm)         Key = encryptionKey     } }   The SymmetricEncryptionSettings class exposes all common properties of a symmetric algorithm (block size, initialization vector, …).   The AlgorithmName is used to load the algorithm implementation using the System.Security.Cryptography.SymmetricAlgorithm.Create(string algName) method. Some default algorithms available in .net core are: \"AES\" (default in Silverback), \"TripleDes\" and \"Rijndael\".   Random initialization vector   If no static initialization vector is provided, a random one is automatically generated per each message and prepended to the actual encrypted message. The consumer will automatically extract and use it.   It is recommended to stick to this default behavior, for increased security.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/advanced/encryption",
        "teaser":null},{
        "title": "Binary Files",
        "excerpt":"Serializing a binary file (a byte array) using the regular JsonMessageSerializer would mean to encode it in base64 and convert it to a UTF-8 encoded byte array. Beside not being very elegant this approach may cause you some trouble when integrating with other systems expecting the raw file content. This procedure would also result in the transferred byte array to be approximately a 30% bigger than the file itself.   In this page it’s shown how to use an IBinaryFileMessage to more efficiently transfer binary files.   Producing using IBinaryFileMessage   The IBinaryFileMessage interface is meant to transfer files over the message broker and is natively supported by Silverback. This means that the raw file content will be transferred in its original form.   For convenience the BinaryFileMessage class already implements the IBinaryFileMessage interface. This class exposes a ContentType property as well, resulting in the content-type header to be produced.   public class FileTransferService {     private readonly IPublisher _publisher;      public FileTransferService(IPublisher publisher)     {         _publisher = publisher;     }      public async Task TransferFile(byte[] content, string contentType)     {         await _publihser.PublishAsync(             new BinaryFileMessage(content, contentType));     } }   Otherwise you can implement the interface yourself or extend the BinaryFileMessage (e.g. to add some additional headers, as explained in the Message Headers section of the quickstart).   public class MyBinaryFileMessage : BinaryFileMessage {     [Header(\"x-user-id\")]     public Guid UserId { get; set; } }   Consuming an IBinaryFileMessage   You don’t need to do anything special to consume a binary file, if all necessary headers are in place (ensured by Silverback, if it was used to produce the message). The message will be wrapped again in a BinaryFileMessage that can be subscribed like any other message.   public class FileConsumerService : ISubscriber {     public async Task OnFileReceived(IBinaryFileMessage message)     {         // ...your file handling logic...     } }   If the message wasn’t produced by Silverback chances are that the message type header is not there. In that case you need to explicitly configure the BinaryFileMessageSerializer in the inbound endpoint.    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddInbound(                     new KafkaConsumerEndpoint(\"basket-events\")                     {                         Serializer = BinaryFileMessageSerializer.Default                     }));     } }    If you need to read additional headers you can either subscribe to an IInboundEnvelope&lt;BinaryFileMessage&gt; or extend the BinaryFileMessage (or implement the IBinaryFileMessage interface from scratch). In this case you need to adapt the serializer configuration as well.   public class MyBinaryFileMessage : BinaryFileMessage {     [Header(\"x-user-id\")]     public Guid UserId { get; set; } }    Startup.cs  public class Startup {     public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddInbound(                     new KafkaConsumerEndpoint(\"basket-events\")                     {                         Serializer = new BinaryFileMessageSerializer&lt;MyBinaryFileMessage&gt;()                     }));     } }   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/advanced/binary-files",
        "teaser":null},{
        "title": "Obervable Bus",
        "excerpt":"Using the Silverback.Core.Rx package it is possible to create an Observable over the complete stream of message from the internal message bus and use Rx.NET to handle more advanced use cases.   This feature must be enabled at startup as shown in the following example.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services.AddSilverback().AsObservable();     } }    Observable registers a new dependency IMessageObservable&lt;&gt; that can be injected into the resolved objects.   public class RxSubscriber : IDisposable {     private readonly IDisposable _subscription;      public RxSubscriber(IMessageObservable&lt;IEvent&gt; observable)     {         _subscription = observable.Subscribe(HandleMessage);     }      public void HandleMessage(IEvent message)     {         ...     }      public void Dispose()     {         _subscription?.Dispose();     } }  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/advanced/rx",
        "teaser":null},{
        "title": "Using IBroker",
        "excerpt":"It may be useful to use the lower level IBroker and related IConsumer and IProducer to implement some more advanced use cases.   Using IBroker directly you get full control over message acknowledgement and you can implement your fully customized logics, taking advantage of Rx.NET if you wish.   Configuration   The only required service is the broker.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka());     } }    Consumer   Retrieving an IConsumer and start consuming messages is just a matter of a few lines of code.   public class MyCustomMessageProcessor {     private readonly IConsumer _consumer;      public MyCustomMessageProcessor(KafkaBroker broker)     {         _consumer = _broker.GetConsumer(new KafkaConsumerEndpoint(\"topic-name\")         {             Configuration = new KafkaConsumerConfig             {                 BootstrapServers = \"PLAINTEXT://kafka:9092\"             }         });          _consumer.Received += OnMessageReceived;          _broker.Connect();     }      private static void OnMessageReceived(object sender, ReceivedMessageEventArgs args)     {         foreach (var envelope in args.Envelopes)         {             // ...your processing logic...         }     } }   A single call to the Connect method is required for each broker and GetConsumer must be called before Connect.   The messages are acknowledged automatically if the Received event handlers return without exceptions. To disable this behavior remove the InboundProcessorConsumerBehavior from the IServiceCollection (you will need to manually handle the DI scope as well).   Wrapping with an Observable   You can of course very easily wrap the Received event with an Observable and enjoy the uncountable possibilities of Rx.net.   var consumer = _broker.GetConsumer(...);  Observable.FromEventPattern&lt;MessageReceivedEventArgs&gt;(     h =&gt; consumer.Received += h,     h =&gt; consumer.Received -= h) .Subscribe(args =&gt;  {     Process(args.Message);     consumer.Acknowledge(args.Offset); });   The Achknowledge method can be used to commit one or more messages at once.   Producer   The producer works exactly the same as the consumer.   var producer = _broker.GetProducer(new KafkaProducerEndpoint(\"topic-name\") {     Configuration = new KafkaProducerConfig     {         BootstrapServers = \"PLAINTEXT://kafka:9092\"     } });  await producer.ProduceAsync(message, headers);   GetProducer (unlike GetConsumer) can be called at any time, even after Connect as been called.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/advanced/broker",
        "teaser":null},{
        "title": "Message Identifier",
        "excerpt":"Silverback will ensure that an x-message-id header is always sent with each message. This header is important not only for tracing purpose but also to enable exactly-once consuming, chunking and other features.   By default it will be initialized with a new Guid upon producing, unless already set (see the Message Headers section in the quick start to see how to set an header).   IMessageIdProvider   An IMessageIdProvider is used to automatically initialize the message model with the message id that is later published as header.   The default implementation of IMessageIdProvider looks for a public property called either Id or MessageId and supports Guid or String as data type.   If needed you can implement your own IMessageIdProvider and register it for dependency injection.   public class SampleMessageIdProvider : IMessageIdProvider {     public bool CanHandle(object message) =&gt; message is SampleMessage;      public string EnsureIdentifierIsInitialized(object message)     {         var sampleMessage = (SampleMessage)message;          if (sampleMessage.Sequence = 0)             sempleMessage.Sequence = SequenceHelper.GetNext();          return sampleMessage.Sequence.ToString();     } }    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddMessageIdProvider&lt;SampleMessageIdProvider&gt;());     } }    This feature will probably be removed in one of the next releases. It is advised to use an IBehavior to accomplish this kind of task, if still necessary.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/advanced/message-id",
        "teaser":null},{
        "title": "IInboundEnvelope",
        "excerpt":"When a message is consumed Silverback wraps it into an IInboundEnvelope and pushes it to the message bus. Both the IInboundEnvelope or the contained message in its pure form can be subscribed.   You can take advantage of this mechanism to gain access to the transport information of the message, since the IInboundEnvelope holds all this information like endpoint, offset and headers data.   Subscribing to the IInboundEnvelope works exactly the same as subscribing to any other message.   using Silverback.Messaging.Subscribers;  public class SubscribingService : ISubscriber {     public async Task OnWrappedMessageReceived(IInboundEnvelope&lt;SampleMessage&gt; envelope)     {         // ...your message handling logic...     }      public async Task OnPureMessageReceived(SampleMessage message)     {         // ...your message handling logic...     } }   Subscribing to the non-generic IInboundEnvelope or IInboundRawEnvelope it is possible to catch even the messages with an empty body.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/advanced/iinboundenvelope",
        "teaser":null},{
        "title": "Kafka Message Key (partitioning)",
        "excerpt":"Apache Kafka require a message key for different purposes, such as:     Partitioning: Kafka can guarantee ordering only inside the same partition and it is therefore important to be able to route correlated messages into the same partition. To do so you need to specify a key for each message and Kafka will put all messages with the same key in the same partition.   Compacting topics: A topic can be configured with cleanup.policy=compact to instruct Kafka to keep only the latest message related to a certain object, identified by the message key. In other words Kafka will retain only 1 message per each key value.    \t     The messages with the same key are guaranteed to be written to the same partition.   Silverback will always generate a message key (same value as the x-message-id header) but it also offers a convenient way to specify a custom key. It is enough to decorate the properties that must be part of the key with KafkaKeyMemberAttribute.   public class MultipleKeyMembersMessage : IIntegrationMessage {     public Guid Id { get; set; }      [KafkaKeyMember]     public string One { get; set; }          [KafkaKeyMember]     public string Two { get; set; }      public string Three { get; set; } }   The message key will also be received as header (see Message Headers for details).  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/kafka/message-key",
        "teaser":null},{
        "title": "Multiple Consumer Groups (in same process)",
        "excerpt":"In some cases you may want to consume multiple times the same topic, to perform independent tasks. You achieve this by attaching multiple consumers to the same topic, using a different consumer group id.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .WithConnectionToMessageBroker(options =&gt; options                 .AddKafka()                 .AddInboundConnector()                 .AddOutboundConnector());     }      public void Configure(BusConfigurator busConfigurator)     {         busConfigurator             .Connect(endpoints =&gt; endpoints                 .AddInbound(                     new KafkaConsumerEndpoint(\"document-events\")                     {                         Configuration = new KafkaConsumerConfig                         {                             BootstrapServers = \"PLAINTEXT://kafka:9092\",                             GroupId = \"group1\"                         }                     })                 .AddInbound(                     new KafkaConsumerEndpoint(\"document-events\")                     {                         Configuration = new KafkaConsumerConfig                         {                             BootstrapServers = \"PLAINTEXT://kafka:9092\",                             GroupId = \"group2\"                         }                     })); }    By default Silverback would call every matching subscriber method for each message, regardless of the consumer group and you basically have two choices to work around this: using the KafkaGroupIdFilterAttribute or manually inspecting the IInboundEnvelope.   Using the attribute   public class MySubscriber : ISubscriber {     [KafkaGroupIdFilter(\"group1\")]     private void PerformTask1(MyEvent @event) =&gt; ...      [KafkaGroupIdFilter(\"group2\")]     private void PerformTask2(MyEvent @event) =&gt; ... }   Subscribing to the IInboundEnvelope   public class MySubscriber : ISubscriber {     public void OnMessageReceived(IInboundEnvelope&lt;MyEvent&gt; envelope)     {         switch (((KafkaConsumerEndpoint)envelope.Endpoint).Configuration.GroupId)         {             case \"group1\":                 PerformTask1(envelope.Message);                 break;             case \"group2\":                 PerformTask2(envelope.Message);                 break;         }     }      private void PerformTask1(MyEvent @event) =&gt; ...      private void PerformTask2(MyEvent @event) =&gt; ... }  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/kafka/multiple-consumer-groups",
        "teaser":null},{
        "title": "Kafka Events",
        "excerpt":"To let you catch the Kafka events (fired for example when the partions are assigned or revoked) they are mapped to some events that are published to the internal bus.   Consumer events                  Event       Description                       KafkaPartitionsAssignedEvent       The event fired when a new consumer group partition assignment has been received by a consumer. Corresponding to each of this events there will be a KafkaPartitionsRevokedEvent. This event is important if you need to manually set the start offset (see the sample code below in the samples section).                 KafkaPartitionsRevokedEvent       The event fired prior to a group partition assignment being revoked. Corresponding to each of this events there will be a KafkaPartitionsAssignedEvent.                 KafkaOffsetsCommittedEvent       The event fired to report the result of the offset commits.                 KafkaErrorEvent       The event fired when an error is reported by the Confluent.Kafka.Consumer (e.g. connection failures or all brokers down). Note that the system (either the Kafka client itself or Silverback) will try to automatically recover from all errors automatically, so these errors have to be considered purely informational. The subscriber can set the Handled property to prevent Silverback to log the error (useful if you are logging it already).                 KafkaStatisticsEvent       The event fired when statistics are received. Statistics are provided as a JSON formatted string as defined in the librdkafka documentation. You can enable statistics and set the statistics interval using the StatisticsIntervalMs configuration parameter (disabled by default).           Sample use cases   Setting starting offsets on partitions assignment   In the following example the KafkaPartitionsAssignedEvent is subscribed in order to reset the start offsets and replay the past messages.   public class KafkaEventsSubscriber : ISubscriber {     public void OnPartitionsAssigned(KafkaPartitionsAssignedEvent message)     {         message.Partitions = message.Partitions             .Select(topicPartitionOffset =&gt;                 new TopicPartitionOffset(                     topicPartitionOffset.TopicPartition,                     Offset.Beginning))             .ToList();     } }  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/kafka/events",
        "teaser":null},{
        "title": "Routing Key",
        "excerpt":"With RabbitMQ a routing key can be used to route the mssages to a specific queue or filter the messages in a topic. See also the routing and topics tutorials on the official RabbitMQ web site.    \t     The messages are routed according to the routing key.   Silverback offers a convenient way to specify the routing key. It is enough to decorate a property with RabbitRoutingKeyAttribute.   public class MyMessage : IIntegrationMessage {     public Guid Id { get; set; }      [RabbitRoutingKey]     public string Key { get; set; }          ... }  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/rabbit/routing-key",
        "teaser":null},{
        "title": "Sample DbContext (EF Core)",
        "excerpt":"Default Tables   Some features rely on data being stored in a persistent storage such as a database. This chapter highlights the DbSet’s that have to be added to your DbContext when using Silverback in combination with EF Core (via the Silverback.Core.EntityFrameworkCore).   Here a breakdown of the use cases that require a DbSet:     Using an outbox table (see Outbound Connector) will require a DbSet&lt;OutboundMessage&gt; and possibly a DbSet&lt;Lock&gt;, to enable horizontal scaling.   Either a DbSet&lt;StoredOffset&gt; or a DbSet&lt;InboundMessage&gt; is necessary to ensure exactly-once processing (see Inbound Connector).   When consuming chunked messages (see Chunking), you may want to temporary store the received chunks into a database table, until all chunks are received and the full message can be rebuilt and processed and you therefore need a DbSet&lt;TemporaryMessageChunk&gt; to be configured.   This is what a DbContext built to support all the aforementioned features will look like.   using Microsoft.EntityFrameworkCore; using Silverback.Background.Model; using Silverback.EntityFrameworkCore; using Silverback.Messaging.Connectors.Model; using Silverback.Messaging.LargeMessages;  namespace Sample {    public class SampleDbContext : DbContext     {         public SampleDbContext(DbContextOptions options)             : base(options)         {         }          public DbSet&lt;OutboundMessage&gt; OutboundMessages { get; set; }         public DbSet&lt;InboundMessage&gt; InboundMessages { get; set; }         public DbSet&lt;StoredOffset&gt; StoredOffsets { get; set; }         public DbSet&lt;TemporaryMessageChunk&gt; Chunks { get; set; }         public DbSet&lt;Lock&gt; Locks { get; set; }          protected override void OnModelCreating(ModelBuilder modelBuilder)         {             modelBuilder.Entity&lt;OutboundMessage&gt;()                 .ToTable(\"Silverback_OutboundMessages\");              modelBuilder.Entity&lt;InboundMessage&gt;()                 .ToTable(\"Silverback_InboundMessages\")                 .HasKey(t =&gt; new { t.MessageId, t.EndpointName });              modelBuilder.Entity&lt;StoredOffset&gt;()                 .ToTable(\"Silverback_StoredOffsets\");              modelBuilder.Entity&lt;TemporaryMessageChunk&gt;()                 .ToTable(\"Silverback_MessageChunks\")                 .HasKey(t =&gt; new { t.OriginalMessageId, t.ChunkId });         }     } }   InboundMessage and TemporaryMessageChunk declare a composite primary key via annotation, thing that isn’t supported yet by EF Core. It is therefore mandatory to explictly redeclare their primary key via the HasKey fluent API.   DDD and Transactional Messages   Some additional changes are required in order for the events generated by the domain entities to be fired as part of the SaveChanges transaction. More details on this topic can be found in the DDD and Domain Events chapter of the quickstart.   using Microsoft.EntityFrameworkCore; using Silverback.EntityFrameworkCore; using Silverback.Messaging.Publishing;  namespace Sample {    public class SampleDbContext : DbContext     {         private readonly DbContextEventsPublisher _eventsPublisher;          public SampleDbContext(IPublisher publisher)         {             _eventsPublisher = new DbContextEventsPublisher(publisher, this);         }          public SampleDbContext(DbContextOptions options, IPublisher publisher)             : base(options)         {             _eventsPublisher = new DbContextEventsPublisher(publisher, this);         }          // ...DbSet properties and OnModelCreating...          public override int SaveChanges()             =&gt; SaveChanges(true);          public override int SaveChanges(bool acceptAllChangesOnSuccess)             =&gt; _eventsPublisher.ExecuteSaveTransaction(() =&gt; base.SaveChanges(acceptAllChangesOnSuccess));          public override Task&lt;int&gt; SaveChangesAsync(CancellationToken cancellationToken = default)             =&gt; SaveChangesAsync(true, cancellationToken);          public override Task&lt;int&gt; SaveChangesAsync(             bool acceptAllChangesOnSuccess,             CancellationToken cancellationToken = default)             =&gt; _eventsPublisher.ExecuteSaveTransactionAsync(() =&gt;                 base.SaveChangesAsync(acceptAllChangesOnSuccess, cancellationToken));     } }  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/extra/dbcontext",
        "teaser":null},{
        "title": "Distributed Background Services",
        "excerpt":"To implement the OutboundWorker we had to create a database based locking mechanism, to ensure that only a single instance of our worker was running. You can take advantage of this implementation to build your IHostedService.   DistributedBackgroundService   Two base classes are available in Silverback.Core: DistributedBackgroundService implements the basic locking mechanism, while RecurringDistributedBackgroundService adds on top of it the ability to run a task as specified intervals.   using Silverback.Background;  namespace Sample {     public class MyBackroundService         : RecurringDistributedBackgroundService     {         private readonly IMyService _myService;          public MyBackroundService(             IMyService _myService,              IDistributedLockManager distributedLockManager,              ILogger&lt;OutboundQueueWorkerService&gt; logger)             : base(                 TimeSpan.FromMinutes(5), // interval                 distributedLockManager, logger)         {         }          protected override Task ExecuteRecurringAsync(             CancellationToken stoppingToken) =&gt;              _myService.DoWork(stoppingToken);     } }   A DistributedLockSettings object can be passed to the constructor of the base class to customize lock timeout, heartbeat interval, etc.   Lock Manager   To enable the distributed locks an IDistributedLockManager implementation (probably a DbDistributedLockManager) must be registered for dependency injection as shown in the next code snippet.   The Silverback.EntityFrameworkCore package is also required and the DbContext must configure a DbSet&lt;Lock&gt;. See also the sample DbContext.    Startup.cs  public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services             .AddSilverback()             .UseDbContext&lt;MyDbContext&gt;()             .AddDbDistributedLockManager();     } }    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/extras/background-services",
        "teaser":null},{
        "title": "Contributing",
        "excerpt":"You are encouraged to contribute to Silverback! Please check out the how to contribute guide for guidelines about how to proceed.   Repository   The full source code is available on GitHub   Building   Versioning   The Directory.Build.props file in the root of the repository contains the current version of the nuget packages being built (and referenced). The &lt;BaseVersion&gt; and &lt;VersionSuffix&gt; tags can be used to increment the current version.   Nuget packages   The nuget packages can be built locally using the powershell scipt under /nuget/Update.ps1. Add the -l switch to clear the local nuget cache as well (especially useful when building the same verison over and over).   Testing   The main solution contains quite a few unit tests and additionally some samples are implemented in a separate solution.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/source/contributing",
        "teaser":null},{
        "title": "Samples",
        "excerpt":"A solution containing a few examples can be found under the samples/Examples directory of the Silverback repository. The same solution, targeting .NET Core 2.2 can be found in samples/Examples-2.2.   It includes a sample consumer (Silverback.Examples.ConsumerA) and a sample producer (Silverback.Examples.Main) implementing several common use cases. Just run both console applications to see the samples in action.    \t   Environment   To execute the samples you need a running Apache Kafka (obviously) and SQL Server.   Use the docker compose file in the root of the repository to startup the environment.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/silverback/docs/source/samples",
        "teaser":null},]
