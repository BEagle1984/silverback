// Copyright (c) 2023 Sergio Aquilini
// This code is licensed under MIT license (see LICENSE file for details)

/*******************************************************************************************
  Note: These proxies are generated using Silverback.Tools.KafkaConfigClassGenerator
        located under /tools/
********************************************************************************************/

using System.Diagnostics.CodeAnalysis;
using Confluent.Kafka;

namespace Silverback.Messaging.Configuration.Kafka;

/// <content>
///     The autogenerated part of the <see cref="KafkaClientConfiguration{TClientConfig}" /> class.
/// </content>
[SuppressMessage("", "SA1649", Justification = "Autogenerated all at once")]
[SuppressMessage("", "SA1402", Justification = "Autogenerated all at once")]
[SuppressMessage("", "CA1200", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1623", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1629", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "CA1044", Justification = "Accessors generated according to wrapped class")]
public partial record KafkaClientConfiguration<TClientConfig>
{
    /// <summary>
    ///     SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512. **NOTE**: Despite the name, you may not configure more than one mechanism.
    /// </summary>
    public SaslMechanism? SaslMechanism
    {
        get => ClientConfig.SaslMechanism;
        init => ClientConfig.SaslMechanism = value;
    }

    /// <summary>
    ///     This field indicates the number of acknowledgements the leader broker must receive from ISR brokers
    ///     before responding to the request: Zero=Broker does not send any response/ack to client, One=The
    ///     leader will write the record to its local log but will respond without awaiting full acknowledgement
    ///     from all followers. All=Broker will block until message is committed by all in sync replicas (ISRs).
    ///     If there are less than min.insync.replicas (broker configuration) in the ISR set the produce request
    ///     will fail.
    /// </summary>
    public Acks? Acks
    {
        get => ClientConfig.Acks;
        init => ClientConfig.Acks = value;
    }

    /// <summary>
    ///     Client identifier.
    ///     <br /><br />default: rdkafka
    ///     <br />importance: low
    /// </summary>
    public string? ClientId
    {
        get => ClientConfig.ClientId;
        init => ClientConfig.ClientId = value;
    }

    /// <summary>
    ///     Initial list of brokers as a CSV list of broker host or host:port. The application may also use `rd_kafka_brokers_add()` to add brokers during runtime.
    ///     <br /><br />default: ''
    ///     <br />importance: high
    /// </summary>
    public string? BootstrapServers
    {
        get => ClientConfig.BootstrapServers;
        init => ClientConfig.BootstrapServers = value;
    }

    /// <summary>
    ///     Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic's `max.message.bytes` limit (see Apache Kafka documentation).
    ///     <br /><br />default: 1000000
    ///     <br />importance: medium
    /// </summary>
    public int? MessageMaxBytes
    {
        get => ClientConfig.MessageMaxBytes;
        init => ClientConfig.MessageMaxBytes = value;
    }

    /// <summary>
    ///     Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference (zero-copy) at the expense of larger iovecs.
    ///     <br /><br />default: 65535
    ///     <br />importance: low
    /// </summary>
    public int? MessageCopyMaxBytes
    {
        get => ClientConfig.MessageCopyMaxBytes;
        init => ClientConfig.MessageCopyMaxBytes = value;
    }

    /// <summary>
    ///     Maximum Kafka protocol response message size. This serves as a safety precaution to avoid memory exhaustion in case of protocol hickups. This value must be at least `fetch.max.bytes`  + 512 to allow for protocol overhead; the value is adjusted automatically unless the configuration property is explicitly set.
    ///     <br /><br />default: 100000000
    ///     <br />importance: medium
    /// </summary>
    public int? ReceiveMessageMaxBytes
    {
        get => ClientConfig.ReceiveMessageMaxBytes;
        init => ClientConfig.ReceiveMessageMaxBytes = value;
    }

    /// <summary>
    ///     Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one.
    ///     <br /><br />default: 1000000
    ///     <br />importance: low
    /// </summary>
    public int? MaxInFlight
    {
        get => ClientConfig.MaxInFlight;
        init => ClientConfig.MaxInFlight = value;
    }

    /// <summary>
    ///     Period of time in milliseconds at which topic and broker metadata is refreshed in order to proactively discover any new brokers, topics, partitions or partition leader changes. Use -1 to disable the intervalled refresh (not recommended). If there are no locally referenced topics (no topic objects created, no messages produced, no subscription or no assignment) then only the broker list will be refreshed every interval but no more often than every 10s.
    ///     <br /><br />default: 300000
    ///     <br />importance: low
    /// </summary>
    public int? TopicMetadataRefreshIntervalMs
    {
        get => ClientConfig.TopicMetadataRefreshIntervalMs;
        init => ClientConfig.TopicMetadataRefreshIntervalMs = value;
    }

    /// <summary>
    ///     Metadata cache max age. Defaults to topic.metadata.refresh.interval.ms * 3
    ///     <br /><br />default: 900000
    ///     <br />importance: low
    /// </summary>
    public int? MetadataMaxAgeMs
    {
        get => ClientConfig.MetadataMaxAgeMs;
        init => ClientConfig.MetadataMaxAgeMs = value;
    }

    /// <summary>
    ///     When a topic loses its leader a new metadata request will be enqueued with this initial interval, exponentially increasing until the topic metadata has been refreshed. This is used to recover quickly from transitioning leader brokers.
    ///     <br /><br />default: 250
    ///     <br />importance: low
    /// </summary>
    public int? TopicMetadataRefreshFastIntervalMs
    {
        get => ClientConfig.TopicMetadataRefreshFastIntervalMs;
        init => ClientConfig.TopicMetadataRefreshFastIntervalMs = value;
    }

    /// <summary>
    ///     Sparse metadata requests (consumes less network bandwidth)
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? TopicMetadataRefreshSparse
    {
        get => ClientConfig.TopicMetadataRefreshSparse;
        init => ClientConfig.TopicMetadataRefreshSparse = value;
    }

    /// <summary>
    ///     Apache Kafka topic creation is asynchronous and it takes some time for a new topic to propagate throughout the cluster to all brokers. If a client requests topic metadata after manual topic creation but before the topic has been fully propagated to the broker the client is requesting metadata from, the topic will seem to be non-existent and the client will mark the topic as such, failing queued produced messages with `ERR__UNKNOWN_TOPIC`. This setting delays marking a topic as non-existent until the configured propagation max time has passed. The maximum propagation time is calculated from the time the topic is first referenced in the client, e.g., on produce().
    ///     <br /><br />default: 30000
    ///     <br />importance: low
    /// </summary>
    public int? TopicMetadataPropagationMaxMs
    {
        get => ClientConfig.TopicMetadataPropagationMaxMs;
        init => ClientConfig.TopicMetadataPropagationMaxMs = value;
    }

    /// <summary>
    ///     Topic blacklist, a comma-separated list of regular expressions for matching topic names that should be ignored in broker metadata information as if the topics did not exist.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? TopicBlacklist
    {
        get => ClientConfig.TopicBlacklist;
        init => ClientConfig.TopicBlacklist = value;
    }

    /// <summary>
    ///     A comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg. Consumer: consumer,cgrp,topic,fetch
    ///     <br /><br />default: ''
    ///     <br />importance: medium
    /// </summary>
    public string? Debug
    {
        get => ClientConfig.Debug;
        init => ClientConfig.Debug = value;
    }

    /// <summary>
    ///     Default timeout for network requests. Producer: ProduceRequests will use the lesser value of `socket.timeout.ms` and remaining `message.timeout.ms` for the first message in the batch. Consumer: FetchRequests will use `fetch.wait.max.ms` + `socket.timeout.ms`. Admin: Admin requests will use `socket.timeout.ms` or explicitly set `rd_kafka_AdminOptions_set_operation_timeout()` value.
    ///     <br /><br />default: 60000
    ///     <br />importance: low
    /// </summary>
    public int? SocketTimeoutMs
    {
        get => ClientConfig.SocketTimeoutMs;
        init => ClientConfig.SocketTimeoutMs = value;
    }

    /// <summary>
    ///     Broker socket send buffer size. System default is used if 0.
    ///     <br /><br />default: 0
    ///     <br />importance: low
    /// </summary>
    public int? SocketSendBufferBytes
    {
        get => ClientConfig.SocketSendBufferBytes;
        init => ClientConfig.SocketSendBufferBytes = value;
    }

    /// <summary>
    ///     Broker socket receive buffer size. System default is used if 0.
    ///     <br /><br />default: 0
    ///     <br />importance: low
    /// </summary>
    public int? SocketReceiveBufferBytes
    {
        get => ClientConfig.SocketReceiveBufferBytes;
        init => ClientConfig.SocketReceiveBufferBytes = value;
    }

    /// <summary>
    ///     Enable TCP keep-alives (SO_KEEPALIVE) on broker sockets
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? SocketKeepaliveEnable
    {
        get => ClientConfig.SocketKeepaliveEnable;
        init => ClientConfig.SocketKeepaliveEnable = value;
    }

    /// <summary>
    ///     Disable the Nagle algorithm (TCP_NODELAY) on broker sockets.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? SocketNagleDisable
    {
        get => ClientConfig.SocketNagleDisable;
        init => ClientConfig.SocketNagleDisable = value;
    }

    /// <summary>
    ///     Disconnect from broker when this number of send failures (e.g., timed out requests) is reached. Disable with 0. WARNING: It is highly recommended to leave this setting at its default value of 1 to avoid the client and broker to become desynchronized in case of request timeouts. NOTE: The connection is automatically re-established.
    ///     <br /><br />default: 1
    ///     <br />importance: low
    /// </summary>
    public int? SocketMaxFails
    {
        get => ClientConfig.SocketMaxFails;
        init => ClientConfig.SocketMaxFails = value;
    }

    /// <summary>
    ///     How long to cache the broker address resolving results (milliseconds).
    ///     <br /><br />default: 1000
    ///     <br />importance: low
    /// </summary>
    public int? BrokerAddressTtl
    {
        get => ClientConfig.BrokerAddressTtl;
        init => ClientConfig.BrokerAddressTtl = value;
    }

    /// <summary>
    ///     Allowed broker IP address families: any, v4, v6
    ///     <br /><br />default: any
    ///     <br />importance: low
    /// </summary>
    public BrokerAddressFamily? BrokerAddressFamily
    {
        get => ClientConfig.BrokerAddressFamily;
        init => ClientConfig.BrokerAddressFamily = value;
    }

    /// <summary>
    ///     Close broker connections after the specified time of inactivity. Disable with 0. If this property is left at its default value some heuristics are performed to determine a suitable default value, this is currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info).
    ///     <br /><br />default: 0
    ///     <br />importance: medium
    /// </summary>
    public int? ConnectionsMaxIdleMs
    {
        get => ClientConfig.ConnectionsMaxIdleMs;
        init => ClientConfig.ConnectionsMaxIdleMs = value;
    }

    /// <summary>
    ///     The initial time to wait before reconnecting to a broker after the connection has been closed. The time is increased exponentially until `reconnect.backoff.max.ms` is reached. -25% to +50% jitter is applied to each reconnect backoff. A value of 0 disables the backoff and reconnects immediately.
    ///     <br /><br />default: 100
    ///     <br />importance: medium
    /// </summary>
    public int? ReconnectBackoffMs
    {
        get => ClientConfig.ReconnectBackoffMs;
        init => ClientConfig.ReconnectBackoffMs = value;
    }

    /// <summary>
    ///     The maximum time to wait before reconnecting to a broker after the connection has been closed.
    ///     <br /><br />default: 10000
    ///     <br />importance: medium
    /// </summary>
    public int? ReconnectBackoffMaxMs
    {
        get => ClientConfig.ReconnectBackoffMaxMs;
        init => ClientConfig.ReconnectBackoffMaxMs = value;
    }

    /// <summary>
    ///     librdkafka statistics emit interval. The application also needs to register a stats callback using `rd_kafka_conf_set_stats_cb()`. The granularity is 1000ms. A value of 0 disables statistics.
    ///     <br /><br />default: 0
    ///     <br />importance: high
    /// </summary>
    public int? StatisticsIntervalMs
    {
        get => ClientConfig.StatisticsIntervalMs;
        init => ClientConfig.StatisticsIntervalMs = value;
    }

    /// <summary>
    ///     Disable spontaneous log_cb from internal librdkafka threads, instead enqueue log messages on queue set with `rd_kafka_set_log_queue()` and serve log callbacks or events through the standard poll APIs. **NOTE**: Log messages will linger in a temporary queue until the log queue has been set.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? LogQueue
    {
        get => ClientConfig.LogQueue;
        init => ClientConfig.LogQueue = value;
    }

    /// <summary>
    ///     Print internal thread name in log messages (useful for debugging librdkafka internals)
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? LogThreadName
    {
        get => ClientConfig.LogThreadName;
        init => ClientConfig.LogThreadName = value;
    }

    /// <summary>
    ///     If enabled librdkafka will initialize the PRNG with srand(current_time.milliseconds) on the first invocation of rd_kafka_new() (required only if rand_r() is not available on your platform). If disabled the application must call srand() prior to calling rd_kafka_new().
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? EnableRandomSeed
    {
        get => ClientConfig.EnableRandomSeed;
        init => ClientConfig.EnableRandomSeed = value;
    }

    /// <summary>
    ///     Log broker disconnects. It might be useful to turn this off when interacting with 0.9 brokers with an aggressive `connection.max.idle.ms` value.
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? LogConnectionClose
    {
        get => ClientConfig.LogConnectionClose;
        init => ClientConfig.LogConnectionClose = value;
    }

    /// <summary>
    ///     Signal that librdkafka will use to quickly terminate on rd_kafka_destroy(). If this signal is not set then there will be a delay before rd_kafka_wait_destroyed() returns true as internal threads are timing out their system calls. If this signal is set however the delay will be minimal. The application should mask this signal as an internal signal handler is installed.
    ///     <br /><br />default: 0
    ///     <br />importance: low
    /// </summary>
    public int? InternalTerminationSignal
    {
        get => ClientConfig.InternalTerminationSignal;
        init => ClientConfig.InternalTerminationSignal = value;
    }

    /// <summary>
    ///     Request broker's supported API versions to adjust functionality to available protocol features. If set to false, or the ApiVersionRequest fails, the fallback version `broker.version.fallback` will be used. **NOTE**: Depends on broker version &gt;=0.10.0. If the request is not supported by (an older) broker the `broker.version.fallback` fallback is used.
    ///     <br /><br />default: true
    ///     <br />importance: high
    /// </summary>
    public bool? ApiVersionRequest
    {
        get => ClientConfig.ApiVersionRequest;
        init => ClientConfig.ApiVersionRequest = value;
    }

    /// <summary>
    ///     Timeout for broker API version requests.
    ///     <br /><br />default: 10000
    ///     <br />importance: low
    /// </summary>
    public int? ApiVersionRequestTimeoutMs
    {
        get => ClientConfig.ApiVersionRequestTimeoutMs;
        init => ClientConfig.ApiVersionRequestTimeoutMs = value;
    }

    /// <summary>
    ///     Dictates how long the `broker.version.fallback` fallback is used in the case the ApiVersionRequest fails. **NOTE**: The ApiVersionRequest is only issued when a new connection to the broker is made (such as after an upgrade).
    ///     <br /><br />default: 0
    ///     <br />importance: medium
    /// </summary>
    public int? ApiVersionFallbackMs
    {
        get => ClientConfig.ApiVersionFallbackMs;
        init => ClientConfig.ApiVersionFallbackMs = value;
    }

    /// <summary>
    ///     Older broker versions (before 0.10.0) provide no way for a client to query for supported protocol features (ApiVersionRequest, see `api.version.request`) making it impossible for the client to know what features it may use. As a workaround a user may set this property to the expected broker version and the client will automatically adjust its feature set accordingly if the ApiVersionRequest fails (or is disabled). The fallback broker version will be used for `api.version.fallback.ms`. Valid values are: 0.9.0, 0.8.2, 0.8.1, 0.8.0. Any other value &gt;= 0.10, such as 0.10.2.1, enables ApiVersionRequests.
    ///     <br /><br />default: 0.10.0
    ///     <br />importance: medium
    /// </summary>
    public string? BrokerVersionFallback
    {
        get => ClientConfig.BrokerVersionFallback;
        init => ClientConfig.BrokerVersionFallback = value;
    }

    /// <summary>
    ///     Protocol used to communicate with brokers.
    ///     <br /><br />default: plaintext
    ///     <br />importance: high
    /// </summary>
    public SecurityProtocol? SecurityProtocol
    {
        get => ClientConfig.SecurityProtocol;
        init => ClientConfig.SecurityProtocol = value;
    }

    /// <summary>
    ///     A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. See manual page for `ciphers(1)` and `SSL_CTX_set_cipher_list(3).
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslCipherSuites
    {
        get => ClientConfig.SslCipherSuites;
        init => ClientConfig.SslCipherSuites = value;
    }

    /// <summary>
    ///     The supported-curves extension in the TLS ClientHello message specifies the curves (standard/named, or 'explicit' GF(2^k) or GF(p)) the client is willing to have the server use. See manual page for `SSL_CTX_set1_curves_list(3)`. OpenSSL &gt;= 1.0.2 required.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslCurvesList
    {
        get => ClientConfig.SslCurvesList;
        init => ClientConfig.SslCurvesList = value;
    }

    /// <summary>
    ///     The client uses the TLS ClientHello signature_algorithms extension to indicate to the server which signature/hash algorithm pairs may be used in digital signatures. See manual page for `SSL_CTX_set1_sigalgs_list(3)`. OpenSSL &gt;= 1.0.2 required.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslSigalgsList
    {
        get => ClientConfig.SslSigalgsList;
        init => ClientConfig.SslSigalgsList = value;
    }

    /// <summary>
    ///     Path to client's private key (PEM) used for authentication.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslKeyLocation
    {
        get => ClientConfig.SslKeyLocation;
        init => ClientConfig.SslKeyLocation = value;
    }

    /// <summary>
    ///     Private key passphrase (for use with `ssl.key.location` and `set_ssl_cert()`)
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslKeyPassword
    {
        get => ClientConfig.SslKeyPassword;
        init => ClientConfig.SslKeyPassword = value;
    }

    /// <summary>
    ///     Client's private key string (PEM format) used for authentication.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslKeyPem
    {
        get => ClientConfig.SslKeyPem;
        init => ClientConfig.SslKeyPem = value;
    }

    /// <summary>
    ///     Path to client's public key (PEM) used for authentication.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslCertificateLocation
    {
        get => ClientConfig.SslCertificateLocation;
        init => ClientConfig.SslCertificateLocation = value;
    }

    /// <summary>
    ///     Client's public key string (PEM format) used for authentication.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslCertificatePem
    {
        get => ClientConfig.SslCertificatePem;
        init => ClientConfig.SslCertificatePem = value;
    }

    /// <summary>
    ///     File or directory path to CA certificate(s) for verifying the broker's key. Defaults: On Windows the system's CA certificates are automatically looked up in the Windows Root certificate store. On Mac OSX this configuration defaults to `probe`. It is recommended to install openssl using Homebrew, to provide CA certificates. On Linux install the distribution's ca-certificates package. If OpenSSL is statically linked or `ssl.ca.location` is set to `probe` a list of standard paths will be probed and the first one found will be used as the default CA certificate location path. If OpenSSL is dynamically linked the OpenSSL library's default path will be used (see `OPENSSLDIR` in `openssl version -a`).
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslCaLocation
    {
        get => ClientConfig.SslCaLocation;
        init => ClientConfig.SslCaLocation = value;
    }

    /// <summary>
    ///     CA certificate string (PEM format) for verifying the broker's key.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslCaPem
    {
        get => ClientConfig.SslCaPem;
        init => ClientConfig.SslCaPem = value;
    }

    /// <summary>
    ///     Comma-separated list of Windows Certificate stores to load CA certificates from. Certificates will be loaded in the same order as stores are specified. If no certificates can be loaded from any of the specified stores an error is logged and the OpenSSL library's default CA location is used instead. Store names are typically one or more of: MY, Root, Trust, CA.
    ///     <br /><br />default: Root
    ///     <br />importance: low
    /// </summary>
    public string? SslCaCertificateStores
    {
        get => ClientConfig.SslCaCertificateStores;
        init => ClientConfig.SslCaCertificateStores = value;
    }

    /// <summary>
    ///     Path to CRL for verifying broker's certificate validity.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslCrlLocation
    {
        get => ClientConfig.SslCrlLocation;
        init => ClientConfig.SslCrlLocation = value;
    }

    /// <summary>
    ///     Path to client's keystore (PKCS#12) used for authentication.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslKeystoreLocation
    {
        get => ClientConfig.SslKeystoreLocation;
        init => ClientConfig.SslKeystoreLocation = value;
    }

    /// <summary>
    ///     Client's keystore (PKCS#12) password.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslKeystorePassword
    {
        get => ClientConfig.SslKeystorePassword;
        init => ClientConfig.SslKeystorePassword = value;
    }

    /// <summary>
    ///     Path to OpenSSL engine library. OpenSSL &gt;= 1.1.0 required.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SslEngineLocation
    {
        get => ClientConfig.SslEngineLocation;
        init => ClientConfig.SslEngineLocation = value;
    }

    /// <summary>
    ///     OpenSSL engine id is the name used for loading engine.
    ///     <br /><br />default: dynamic
    ///     <br />importance: low
    /// </summary>
    public string? SslEngineId
    {
        get => ClientConfig.SslEngineId;
        init => ClientConfig.SslEngineId = value;
    }

    /// <summary>
    ///     Enable OpenSSL's builtin broker (server) certificate verification. This verification can be extended by the application by implementing a certificate_verify_cb.
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? EnableSslCertificateVerification
    {
        get => ClientConfig.EnableSslCertificateVerification;
        init => ClientConfig.EnableSslCertificateVerification = value;
    }

    /// <summary>
    ///     Endpoint identification algorithm to validate broker hostname using broker certificate. https - Server (broker) hostname verification as specified in RFC2818. none - No endpoint verification. OpenSSL &gt;= 1.0.2 required.
    ///     <br /><br />default: none
    ///     <br />importance: low
    /// </summary>
    public SslEndpointIdentificationAlgorithm? SslEndpointIdentificationAlgorithm
    {
        get => ClientConfig.SslEndpointIdentificationAlgorithm;
        init => ClientConfig.SslEndpointIdentificationAlgorithm = value;
    }

    /// <summary>
    ///     Kerberos principal name that Kafka runs as, not including /hostname@REALM
    ///     <br /><br />default: kafka
    ///     <br />importance: low
    /// </summary>
    public string? SaslKerberosServiceName
    {
        get => ClientConfig.SaslKerberosServiceName;
        init => ClientConfig.SaslKerberosServiceName = value;
    }

    /// <summary>
    ///     This client's Kerberos principal name. (Not supported on Windows, will use the logon user's principal).
    ///     <br /><br />default: kafkaclient
    ///     <br />importance: low
    /// </summary>
    public string? SaslKerberosPrincipal
    {
        get => ClientConfig.SaslKerberosPrincipal;
        init => ClientConfig.SaslKerberosPrincipal = value;
    }

    /// <summary>
    ///     Shell command to refresh or acquire the client's Kerberos ticket. This command is executed on client creation and every sasl.kerberos.min.time.before.relogin (0=disable). %{config.prop.name} is replaced by corresponding config object value.
    ///     <br /><br />default: kinit -R -t "%{sasl.kerberos.keytab}" -k %{sasl.kerberos.principal} || kinit -t "%{sasl.kerberos.keytab}" -k %{sasl.kerberos.principal}
    ///     <br />importance: low
    /// </summary>
    public string? SaslKerberosKinitCmd
    {
        get => ClientConfig.SaslKerberosKinitCmd;
        init => ClientConfig.SaslKerberosKinitCmd = value;
    }

    /// <summary>
    ///     Path to Kerberos keytab file. This configuration property is only used as a variable in `sasl.kerberos.kinit.cmd` as ` ... -t "%{sasl.kerberos.keytab}"`.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SaslKerberosKeytab
    {
        get => ClientConfig.SaslKerberosKeytab;
        init => ClientConfig.SaslKerberosKeytab = value;
    }

    /// <summary>
    ///     Minimum time in milliseconds between key refresh attempts. Disable automatic key refresh by setting this property to 0.
    ///     <br /><br />default: 60000
    ///     <br />importance: low
    /// </summary>
    public int? SaslKerberosMinTimeBeforeRelogin
    {
        get => ClientConfig.SaslKerberosMinTimeBeforeRelogin;
        init => ClientConfig.SaslKerberosMinTimeBeforeRelogin = value;
    }

    /// <summary>
    ///     SASL username for use with the PLAIN and SASL-SCRAM-.. mechanisms
    ///     <br /><br />default: ''
    ///     <br />importance: high
    /// </summary>
    public string? SaslUsername
    {
        get => ClientConfig.SaslUsername;
        init => ClientConfig.SaslUsername = value;
    }

    /// <summary>
    ///     SASL password for use with the PLAIN and SASL-SCRAM-.. mechanism
    ///     <br /><br />default: ''
    ///     <br />importance: high
    /// </summary>
    public string? SaslPassword
    {
        get => ClientConfig.SaslPassword;
        init => ClientConfig.SaslPassword = value;
    }

    /// <summary>
    ///     SASL/OAUTHBEARER configuration. The format is implementation-dependent and must be parsed accordingly. The default unsecured token implementation (see https://tools.ietf.org/html/rfc7515#appendix-A.5) recognizes space-separated name=value pairs with valid names including principalClaimName, principal, scopeClaimName, scope, and lifeSeconds. The default value for principalClaimName is "sub", the default value for scopeClaimName is "scope", and the default value for lifeSeconds is 3600. The scope value is CSV format with the default value being no/empty scope. For example: `principalClaimName=azp principal=admin scopeClaimName=roles scope=role1,role2 lifeSeconds=600`. In addition, SASL extensions can be communicated to the broker via `extension_NAME=value`. For example: `principal=admin extension_traceId=123`
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? SaslOauthbearerConfig
    {
        get => ClientConfig.SaslOauthbearerConfig;
        init => ClientConfig.SaslOauthbearerConfig = value;
    }

    /// <summary>
    ///     Enable the builtin unsecure JWT OAUTHBEARER token handler if no oauthbearer_refresh_cb has been set. This builtin handler should only be used for development or testing, and not in production.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? EnableSaslOauthbearerUnsecureJwt
    {
        get => ClientConfig.EnableSaslOauthbearerUnsecureJwt;
        init => ClientConfig.EnableSaslOauthbearerUnsecureJwt = value;
    }

    /// <summary>
    ///     List of plugin libraries to load (; separated). The library search path is platform dependent (see dlopen(3) for Unix and LoadLibrary() for Windows). If no filename extension is specified the platform-specific extension (such as .dll or .so) will be appended automatically.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? PluginLibraryPaths
    {
        get => ClientConfig.PluginLibraryPaths;
        init => ClientConfig.PluginLibraryPaths = value;
    }

    /// <summary>
    ///     A rack identifier for this client. This can be any string value which indicates where this client is physically located. It corresponds with the broker config `broker.rack`.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string? ClientRack
    {
        get => ClientConfig.ClientRack;
        init => ClientConfig.ClientRack = value;
    }

    /// <summary>
    ///     The maximum length of time (in milliseconds) before a cancellation request
    ///     is acted on. Low values may result in measurably higher CPU usage.
    ///     range: 1 &lt;= dotnet.cancellation.delay.max.ms &lt;= 10000
    ///     <br /><br />default: 100
    ///     <br />importance: low
    /// </summary>
    public int CancellationDelayMaxMs
    {
        init => ClientConfig.CancellationDelayMaxMs = value;
    }
}

/// <content>
///     The autogenerated part of the <see cref="KafkaConsumerConfiguration" /> class.
/// </content>
[SuppressMessage("", "SA1649", Justification = "Autogenerated all at once")]
[SuppressMessage("", "SA1402", Justification = "Autogenerated all at once")]
[SuppressMessage("", "CA1200", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1623", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1629", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "CA1044", Justification = "Accessors generated according to wrapped class")]
public partial record KafkaConsumerConfiguration
{
    /// <summary>
    ///     A comma separated list of fields that may be optionally set
    ///     in <see cref="T:Confluent.Kafka.ConsumeResult`2" />
    ///     objects returned by the
    ///     <see cref="M:Confluent.Kafka.Consumer`2.Consume(System.TimeSpan)" />
    ///     method. Disabling fields that you do not require will improve
    ///     throughput and reduce memory consumption. Allowed values:
    ///     headers, timestamp, topic, all, none
    ///     <br /><br />default: all
    ///     <br />importance: low
    /// </summary>
    public string? ConsumeResultFields
    {
        init => ClientConfig.ConsumeResultFields = value;
    }

    /// <summary>
    ///     Action to take when there is no initial offset in offset store or the desired offset is out of range: 'smallest','earliest' - automatically reset the offset to the smallest offset, 'largest','latest' - automatically reset the offset to the largest offset, 'error' - trigger an error (ERR__AUTO_OFFSET_RESET) which is retrieved by consuming messages and checking 'message-&gt;err'.
    ///     <br /><br />default: largest
    ///     <br />importance: high
    /// </summary>
    public AutoOffsetReset? AutoOffsetReset
    {
        get => ClientConfig.AutoOffsetReset;
        init => ClientConfig.AutoOffsetReset = value;
    }

    /// <summary>
    ///     Enable static group membership. Static group members are able to leave and rejoin a group within the configured `session.timeout.ms` without prompting a group rebalance. This should be used in combination with a larger `session.timeout.ms` to avoid group rebalances caused by transient unavailability (e.g. process restarts). Requires broker version &gt;= 2.3.0.
    ///     <br /><br />default: ''
    ///     <br />importance: medium
    /// </summary>
    public string? GroupInstanceId
    {
        get => ClientConfig.GroupInstanceId;
        init => ClientConfig.GroupInstanceId = value;
    }

    /// <summary>
    ///     The name of one or more partition assignment strategies. The elected group leader will use a strategy supported by all members of the group to assign partitions to group members. If there is more than one eligible strategy, preference is determined by the order of this list (strategies earlier in the list have higher priority). Cooperative and non-cooperative (eager) strategies must not be mixed. Available strategies: range, roundrobin, cooperative-sticky.
    ///     <br /><br />default: range,roundrobin
    ///     <br />importance: medium
    /// </summary>
    public PartitionAssignmentStrategy? PartitionAssignmentStrategy
    {
        get => ClientConfig.PartitionAssignmentStrategy;
        init => ClientConfig.PartitionAssignmentStrategy = value;
    }

    /// <summary>
    ///     Client group session and failure detection timeout. The consumer sends periodic heartbeats (heartbeat.interval.ms) to indicate its liveness to the broker. If no hearts are received by the broker for a group member within the session timeout, the broker will remove the consumer from the group and trigger a rebalance. The allowed range is configured with the **broker** configuration properties `group.min.session.timeout.ms` and `group.max.session.timeout.ms`. Also see `max.poll.interval.ms`.
    ///     <br /><br />default: 45000
    ///     <br />importance: high
    /// </summary>
    public int? SessionTimeoutMs
    {
        get => ClientConfig.SessionTimeoutMs;
        init => ClientConfig.SessionTimeoutMs = value;
    }

    /// <summary>
    ///     Group session keepalive heartbeat interval.
    ///     <br /><br />default: 3000
    ///     <br />importance: low
    /// </summary>
    public int? HeartbeatIntervalMs
    {
        get => ClientConfig.HeartbeatIntervalMs;
        init => ClientConfig.HeartbeatIntervalMs = value;
    }

    /// <summary>
    ///     Group protocol type. NOTE: Currently, the only supported group protocol type is `consumer`.
    ///     <br /><br />default: consumer
    ///     <br />importance: low
    /// </summary>
    public string? GroupProtocolType
    {
        get => ClientConfig.GroupProtocolType;
        init => ClientConfig.GroupProtocolType = value;
    }

    /// <summary>
    ///     How often to query for the current client group coordinator. If the currently assigned coordinator is down the configured query interval will be divided by ten to more quickly recover in case of coordinator reassignment.
    ///     <br /><br />default: 600000
    ///     <br />importance: low
    /// </summary>
    public int? CoordinatorQueryIntervalMs
    {
        get => ClientConfig.CoordinatorQueryIntervalMs;
        init => ClientConfig.CoordinatorQueryIntervalMs = value;
    }

    /// <summary>
    ///     Maximum allowed time between calls to consume messages (e.g., rd_kafka_consumer_poll()) for high-level consumers. If this interval is exceeded the consumer is considered failed and the group will rebalance in order to reassign the partitions to another consumer group member. Warning: Offset commits may be not possible at this point. Note: It is recommended to set `enable.auto.offset.store=false` for long-time processing applications and then explicitly store offsets (using offsets_store()) *after* message processing, to make sure offsets are not auto-committed prior to processing has finished. The interval is checked two times per second. See KIP-62 for more information.
    ///     <br /><br />default: 300000
    ///     <br />importance: high
    /// </summary>
    public int? MaxPollIntervalMs
    {
        get => ClientConfig.MaxPollIntervalMs;
        init => ClientConfig.MaxPollIntervalMs = value;
    }

    /// <summary>
    ///     The frequency in milliseconds that the consumer offsets are committed (written) to offset storage. (0 = disable). This setting is used by the high-level consumer.
    ///     <br /><br />default: 5000
    ///     <br />importance: medium
    /// </summary>
    public int? AutoCommitIntervalMs
    {
        get => ClientConfig.AutoCommitIntervalMs;
        init => ClientConfig.AutoCommitIntervalMs = value;
    }

    /// <summary>
    ///     Minimum number of messages per topic+partition librdkafka tries to maintain in the local consumer queue.
    ///     <br /><br />default: 100000
    ///     <br />importance: medium
    /// </summary>
    public int? QueuedMinMessages
    {
        get => ClientConfig.QueuedMinMessages;
        init => ClientConfig.QueuedMinMessages = value;
    }

    /// <summary>
    ///     Maximum number of kilobytes of queued pre-fetched messages in the local consumer queue. If using the high-level consumer this setting applies to the single consumer queue, regardless of the number of partitions. When using the legacy simple consumer or when separate partition queues are used this setting applies per partition. This value may be overshot by fetch.message.max.bytes. This property has higher priority than queued.min.messages.
    ///     <br /><br />default: 65536
    ///     <br />importance: medium
    /// </summary>
    public int? QueuedMaxMessagesKbytes
    {
        get => ClientConfig.QueuedMaxMessagesKbytes;
        init => ClientConfig.QueuedMaxMessagesKbytes = value;
    }

    /// <summary>
    ///     Maximum time the broker may wait to fill the Fetch response with fetch.min.bytes of messages.
    ///     <br /><br />default: 500
    ///     <br />importance: low
    /// </summary>
    public int? FetchWaitMaxMs
    {
        get => ClientConfig.FetchWaitMaxMs;
        init => ClientConfig.FetchWaitMaxMs = value;
    }

    /// <summary>
    ///     Initial maximum number of bytes per topic+partition to request when fetching messages from the broker. If the client encounters a message larger than this value it will gradually try to increase it until the entire message can be fetched.
    ///     <br /><br />default: 1048576
    ///     <br />importance: medium
    /// </summary>
    public int? MaxPartitionFetchBytes
    {
        get => ClientConfig.MaxPartitionFetchBytes;
        init => ClientConfig.MaxPartitionFetchBytes = value;
    }

    /// <summary>
    ///     Maximum amount of data the broker shall return for a Fetch request. Messages are fetched in batches by the consumer and if the first message batch in the first non-empty partition of the Fetch request is larger than this value, then the message batch will still be returned to ensure the consumer can make progress. The maximum message batch size accepted by the broker is defined via `message.max.bytes` (broker config) or `max.message.bytes` (broker topic config). `fetch.max.bytes` is automatically adjusted upwards to be at least `message.max.bytes` (consumer config).
    ///     <br /><br />default: 52428800
    ///     <br />importance: medium
    /// </summary>
    public int? FetchMaxBytes
    {
        get => ClientConfig.FetchMaxBytes;
        init => ClientConfig.FetchMaxBytes = value;
    }

    /// <summary>
    ///     Minimum number of bytes the broker responds with. If fetch.wait.max.ms expires the accumulated data will be sent to the client regardless of this setting.
    ///     <br /><br />default: 1
    ///     <br />importance: low
    /// </summary>
    public int? FetchMinBytes
    {
        get => ClientConfig.FetchMinBytes;
        init => ClientConfig.FetchMinBytes = value;
    }

    /// <summary>
    ///     How long to postpone the next fetch request for a topic+partition in case of a fetch error.
    ///     <br /><br />default: 500
    ///     <br />importance: medium
    /// </summary>
    public int? FetchErrorBackoffMs
    {
        get => ClientConfig.FetchErrorBackoffMs;
        init => ClientConfig.FetchErrorBackoffMs = value;
    }

    /// <summary>
    ///     Controls how to read messages written transactionally: `read_committed` - only return transactional messages which have been committed. `read_uncommitted` - return all messages, even transactional messages which have been aborted.
    ///     <br /><br />default: read_committed
    ///     <br />importance: high
    /// </summary>
    public IsolationLevel? IsolationLevel
    {
        get => ClientConfig.IsolationLevel;
        init => ClientConfig.IsolationLevel = value;
    }

    /// <summary>
    ///     Emit RD_KAFKA_RESP_ERR__PARTITION_EOF event whenever the consumer reaches the end of a partition.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? EnablePartitionEof
    {
        get => ClientConfig.EnablePartitionEof;
        init => ClientConfig.EnablePartitionEof = value;
    }

    /// <summary>
    ///     Verify CRC32 of consumed messages, ensuring no on-the-wire or on-disk corruption to the messages occurred. This check comes at slightly increased CPU usage.
    ///     <br /><br />default: false
    ///     <br />importance: medium
    /// </summary>
    public bool? CheckCrcs
    {
        get => ClientConfig.CheckCrcs;
        init => ClientConfig.CheckCrcs = value;
    }

    /// <summary>
    ///     Allow automatic topic creation on the broker when subscribing to or assigning non-existent topics. The broker must also be configured with `auto.create.topics.enable=true` for this configuraiton to take effect. Note: The default value (false) is different from the Java consumer (true). Requires broker version &gt;= 0.11.0.0, for older broker versions only the broker configuration applies.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? AllowAutoCreateTopics
    {
        get => ClientConfig.AllowAutoCreateTopics;
        init => ClientConfig.AllowAutoCreateTopics = value;
    }
}

/// <content>
///     The autogenerated part of the <see cref="KafkaProducerConfiguration" /> class.
/// </content>
[SuppressMessage("", "SA1649", Justification = "Autogenerated all at once")]
[SuppressMessage("", "SA1402", Justification = "Autogenerated all at once")]
[SuppressMessage("", "CA1200", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1623", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1629", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "CA1044", Justification = "Accessors generated according to wrapped class")]
public partial record KafkaProducerConfiguration
{
    /// <summary>
    ///     Specifies whether or not the producer should start a background poll
    ///     thread to receive delivery reports and event notifications. Generally,
    ///     this should be set to true. If set to false, you will need to call
    ///     the Poll function manually.
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? EnableBackgroundPoll
    {
        get => ClientConfig.EnableBackgroundPoll;
        init => ClientConfig.EnableBackgroundPoll = value;
    }

    /// <summary>
    ///     Specifies whether to enable notification of delivery reports. Typically
    ///     you should set this parameter to true. Set it to false for "fire and
    ///     forget" semantics and a small boost in performance.
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? EnableDeliveryReports
    {
        get => ClientConfig.EnableDeliveryReports;
        init => ClientConfig.EnableDeliveryReports = value;
    }

    /// <summary>
    ///     A comma separated list of fields that may be optionally set in delivery
    ///     reports. Disabling delivery report fields that you do not require will
    ///     improve maximum throughput and reduce memory usage. Allowed values:
    ///     key, value, timestamp, headers, status, all, none.
    ///     <br /><br />default: all
    ///     <br />importance: low
    /// </summary>
    /// <remarks>
    ///     Silverback overrides this value by default setting it to &quot;key,status&quot; as an optimization,
    ///     since the other fields aren't used.
    /// </remarks>
    public string? DeliveryReportFields
    {
        get => ClientConfig.DeliveryReportFields;
        init
        {
            if (value != null)
                ClientConfig.DeliveryReportFields = value;
        }
    }

    /// <summary>
    ///     The ack timeout of the producer request in milliseconds. This value is only enforced by the broker and relies on `request.required.acks` being != 0.
    ///     <br /><br />default: 30000
    ///     <br />importance: medium
    /// </summary>
    public int? RequestTimeoutMs
    {
        get => ClientConfig.RequestTimeoutMs;
        init => ClientConfig.RequestTimeoutMs = value;
    }

    /// <summary>
    ///     Local message timeout. This value is only enforced locally and limits the time a produced message waits for successful delivery. A time of 0 is infinite. This is the maximum time librdkafka may use to deliver a message (including retries). Delivery error occurs when either the retry count or the message timeout are exceeded. The message timeout is automatically adjusted to `transaction.timeout.ms` if `transactional.id` is configured.
    ///     <br /><br />default: 300000
    ///     <br />importance: high
    /// </summary>
    public int? MessageTimeoutMs
    {
        get => ClientConfig.MessageTimeoutMs;
        init => ClientConfig.MessageTimeoutMs = value;
    }

    /// <summary>
    ///     Partitioner: `random` - random distribution, `consistent` - CRC32 hash of key (Empty and NULL keys are mapped to single partition), `consistent_random` - CRC32 hash of key (Empty and NULL keys are randomly partitioned), `murmur2` - Java Producer compatible Murmur2 hash of key (NULL keys are mapped to single partition), `murmur2_random` - Java Producer compatible Murmur2 hash of key (NULL keys are randomly partitioned. This is functionally equivalent to the default partitioner in the Java Producer.), `fnv1a` - FNV-1a hash of key (NULL keys are mapped to single partition), `fnv1a_random` - FNV-1a hash of key (NULL keys are randomly partitioned).
    ///     <br /><br />default: consistent_random
    ///     <br />importance: high
    /// </summary>
    public Partitioner? Partitioner
    {
        get => ClientConfig.Partitioner;
        init => ClientConfig.Partitioner = value;
    }

    /// <summary>
    ///     Compression level parameter for algorithm selected by configuration property `compression.codec`. Higher values will result in better compression at the cost of more CPU usage. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level.
    ///     <br /><br />default: -1
    ///     <br />importance: medium
    /// </summary>
    public int? CompressionLevel
    {
        get => ClientConfig.CompressionLevel;
        init => ClientConfig.CompressionLevel = value;
    }

    /// <summary>
    ///     Enables the transactional producer. The transactional.id is used to identify the same transactional producer instance across process restarts. It allows the producer to guarantee that transactions corresponding to earlier instances of the same producer have been finalized prior to starting any new transactions, and that any zombie instances are fenced off. If no transactional.id is provided, then the producer is limited to idempotent delivery (if enable.idempotence is set). Requires broker version &gt;= 0.11.0.
    ///     <br /><br />default: ''
    ///     <br />importance: high
    /// </summary>
    public string? TransactionalId
    {
        get => ClientConfig.TransactionalId;
        init => ClientConfig.TransactionalId = value;
    }

    /// <summary>
    ///     The maximum amount of time in milliseconds that the transaction coordinator will wait for a transaction status update from the producer before proactively aborting the ongoing transaction. If this value is larger than the `transaction.max.timeout.ms` setting in the broker, the init_transactions() call will fail with ERR_INVALID_TRANSACTION_TIMEOUT. The transaction timeout automatically adjusts `message.timeout.ms` and `socket.timeout.ms`, unless explicitly configured in which case they must not exceed the transaction timeout (`socket.timeout.ms` must be at least 100ms lower than `transaction.timeout.ms`). This is also the default timeout value if no timeout (-1) is supplied to the transactional API methods.
    ///     <br /><br />default: 60000
    ///     <br />importance: medium
    /// </summary>
    public int? TransactionTimeoutMs
    {
        get => ClientConfig.TransactionTimeoutMs;
        init => ClientConfig.TransactionTimeoutMs = value;
    }

    /// <summary>
    ///     When set to `true`, the producer will ensure that messages are successfully produced exactly once and in the original produce order. The following configuration properties are adjusted automatically (if not modified by the user) when idempotence is enabled: `max.in.flight.requests.per.connection=5` (must be less than or equal to 5), `retries=INT32_MAX` (must be greater than 0), `acks=all`, `queuing.strategy=fifo`. Producer instantation will fail if user-supplied configuration is incompatible.
    ///     <br /><br />default: false
    ///     <br />importance: high
    /// </summary>
    public bool? EnableIdempotence
    {
        get => ClientConfig.EnableIdempotence;
        init => ClientConfig.EnableIdempotence = value;
    }

    /// <summary>
    ///     **EXPERIMENTAL**: subject to change or removal. When set to `true`, any error that could result in a gap in the produced message series when a batch of messages fails, will raise a fatal error (ERR__GAPLESS_GUARANTEE) and stop the producer. Messages failing due to `message.timeout.ms` are not covered by this guarantee. Requires `enable.idempotence=true`.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? EnableGaplessGuarantee
    {
        get => ClientConfig.EnableGaplessGuarantee;
        init => ClientConfig.EnableGaplessGuarantee = value;
    }

    /// <summary>
    ///     Maximum number of messages allowed on the producer queue. This queue is shared by all topics and partitions.
    ///     <br /><br />default: 100000
    ///     <br />importance: high
    /// </summary>
    public int? QueueBufferingMaxMessages
    {
        get => ClientConfig.QueueBufferingMaxMessages;
        init => ClientConfig.QueueBufferingMaxMessages = value;
    }

    /// <summary>
    ///     Maximum total message size sum allowed on the producer queue. This queue is shared by all topics and partitions. This property has higher priority than queue.buffering.max.messages.
    ///     <br /><br />default: 1048576
    ///     <br />importance: high
    /// </summary>
    public int? QueueBufferingMaxKbytes
    {
        get => ClientConfig.QueueBufferingMaxKbytes;
        init => ClientConfig.QueueBufferingMaxKbytes = value;
    }

    /// <summary>
    ///     Delay in milliseconds to wait for messages in the producer queue to accumulate before constructing message batches (MessageSets) to transmit to brokers. A higher value allows larger and more effective (less overhead, improved compression) batches of messages to accumulate at the expense of increased message delivery latency.
    ///     <br /><br />default: 5
    ///     <br />importance: high
    /// </summary>
    public double? LingerMs
    {
        get => ClientConfig.LingerMs;
        init => ClientConfig.LingerMs = value;
    }

    /// <summary>
    ///     How many times to retry sending a failing Message. **Note:** retrying may cause reordering unless `enable.idempotence` is set to true.
    ///     <br /><br />default: 2147483647
    ///     <br />importance: high
    /// </summary>
    public int? MessageSendMaxRetries
    {
        get => ClientConfig.MessageSendMaxRetries;
        init => ClientConfig.MessageSendMaxRetries = value;
    }

    /// <summary>
    ///     The backoff time in milliseconds before retrying a protocol request.
    ///     <br /><br />default: 100
    ///     <br />importance: medium
    /// </summary>
    public int? RetryBackoffMs
    {
        get => ClientConfig.RetryBackoffMs;
        init => ClientConfig.RetryBackoffMs = value;
    }

    /// <summary>
    ///     The threshold of outstanding not yet transmitted broker requests needed to backpressure the producer's message accumulator. If the number of not yet transmitted requests equals or exceeds this number, produce request creation that would have otherwise been triggered (for example, in accordance with linger.ms) will be delayed. A lower number yields larger and more effective batches. A higher value can improve latency when using compression on slow machines.
    ///     <br /><br />default: 1
    ///     <br />importance: low
    /// </summary>
    public int? QueueBufferingBackpressureThreshold
    {
        get => ClientConfig.QueueBufferingBackpressureThreshold;
        init => ClientConfig.QueueBufferingBackpressureThreshold = value;
    }

    /// <summary>
    ///     compression codec to use for compressing message sets. This is the default value for all topics, may be overridden by the topic configuration property `compression.codec`.
    ///     <br /><br />default: none
    ///     <br />importance: medium
    /// </summary>
    public CompressionType? CompressionType
    {
        get => ClientConfig.CompressionType;
        init => ClientConfig.CompressionType = value;
    }

    /// <summary>
    ///     Maximum number of messages batched in one MessageSet. The total MessageSet size is also limited by batch.size and message.max.bytes.
    ///     <br /><br />default: 10000
    ///     <br />importance: medium
    /// </summary>
    public int? BatchNumMessages
    {
        get => ClientConfig.BatchNumMessages;
        init => ClientConfig.BatchNumMessages = value;
    }

    /// <summary>
    ///     Maximum size (in bytes) of all messages batched in one MessageSet, including protocol framing overhead. This limit is applied after the first message has been added to the batch, regardless of the first message's size, this is to ensure that messages that exceed batch.size are produced. The total MessageSet size is also limited by batch.num.messages and message.max.bytes.
    ///     <br /><br />default: 1000000
    ///     <br />importance: medium
    /// </summary>
    public int? BatchSize
    {
        get => ClientConfig.BatchSize;
        init => ClientConfig.BatchSize = value;
    }

    /// <summary>
    ///     Delay in milliseconds to wait to assign new sticky partitions for each topic. By default, set to double the time of linger.ms. To disable sticky behavior, set to 0. This behavior affects messages with the key NULL in all cases, and messages with key lengths of zero when the consistent_random partitioner is in use. These messages would otherwise be assigned randomly. A higher value allows for more effective batching of these messages.
    ///     <br /><br />default: 10
    ///     <br />importance: low
    /// </summary>
    public int? StickyPartitioningLingerMs
    {
        get => ClientConfig.StickyPartitioningLingerMs;
        init => ClientConfig.StickyPartitioningLingerMs = value;
    }
}

/// <summary>
///     Builds the <see cref="KafkaProducerConfiguration" /> or <see cref="KafkaConsumerConfiguration" />.
/// </summary>
[SuppressMessage("", "SA1649", Justification = "Autogenerated all at once")]
[SuppressMessage("", "SA1402", Justification = "Autogenerated all at once")]
[SuppressMessage("", "CA1200", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1623", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1629", Justification = "Summary copied from wrapped class")]
[SuppressMessage("StyleCop.CSharp.DocumentationRules", "SA1625:Element documentation should not be copied and pasted", Justification = "Summary copied from wrapped class")]
[SuppressMessage("StyleCop.CSharp.OrderingRules", "SA1201:Elements should appear in the correct order", Justification = "Autogenerated")]
internal interface IKafkaClientConfigurationBuilder
{
    /// <summary>
    ///     SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512. **NOTE**: Despite the name, you may not configure more than one mechanism.
    /// </summary>
    /// <param name="saslMechanism">
    ///     SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512. **NOTE**: Despite the name, you may not configure more than one mechanism.
    /// </param>
    void WithSaslMechanism(SaslMechanism? saslMechanism);

    /// <summary>
    ///     This field indicates the number of acknowledgements the leader broker must receive from ISR brokers
    ///     before responding to the request: Zero=Broker does not send any response/ack to client, One=The
    ///     leader will write the record to its local log but will respond without awaiting full acknowledgement
    ///     from all followers. All=Broker will block until message is committed by all in sync replicas (ISRs).
    ///     If there are less than min.insync.replicas (broker configuration) in the ISR set the produce request
    ///     will fail.
    /// </summary>
    /// <param name="acks">
    ///     This field indicates the number of acknowledgements the leader broker must receive from ISR brokers
    ///     before responding to the request: Zero=Broker does not send any response/ack to client, One=The
    ///     leader will write the record to its local log but will respond without awaiting full acknowledgement
    ///     from all followers. All=Broker will block until message is committed by all in sync replicas (ISRs).
    ///     If there are less than min.insync.replicas (broker configuration) in the ISR set the produce request
    ///     will fail.
    /// </param>
    void WithAcks(Acks? acks);

    /// <summary>
    ///     Client identifier.
    /// </summary>
    /// <param name="clientId">
    ///     Client identifier.
    /// </param>
    void WithClientId(string? clientId);

    /// <summary>
    ///     Initial list of brokers as a CSV list of broker host or host:port. The application may also use `rd_kafka_brokers_add()` to add brokers during runtime.
    /// </summary>
    /// <param name="bootstrapServers">
    ///     Initial list of brokers as a CSV list of broker host or host:port. The application may also use `rd_kafka_brokers_add()` to add brokers during runtime.
    /// </param>
    void WithBootstrapServers(string? bootstrapServers);

    /// <summary>
    ///     Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic's `max.message.bytes` limit (see Apache Kafka documentation).
    /// </summary>
    /// <param name="messageMaxBytes">
    ///     Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic's `max.message.bytes` limit (see Apache Kafka documentation).
    /// </param>
    void WithMessageMaxBytes(int? messageMaxBytes);

    /// <summary>
    ///     Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference (zero-copy) at the expense of larger iovecs.
    /// </summary>
    /// <param name="messageCopyMaxBytes">
    ///     Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference (zero-copy) at the expense of larger iovecs.
    /// </param>
    void WithMessageCopyMaxBytes(int? messageCopyMaxBytes);

    /// <summary>
    ///     Maximum Kafka protocol response message size. This serves as a safety precaution to avoid memory exhaustion in case of protocol hickups. This value must be at least `fetch.max.bytes`  + 512 to allow for protocol overhead; the value is adjusted automatically unless the configuration property is explicitly set.
    /// </summary>
    /// <param name="receiveMessageMaxBytes">
    ///     Maximum Kafka protocol response message size. This serves as a safety precaution to avoid memory exhaustion in case of protocol hickups. This value must be at least `fetch.max.bytes`  + 512 to allow for protocol overhead; the value is adjusted automatically unless the configuration property is explicitly set.
    /// </param>
    void WithReceiveMessageMaxBytes(int? receiveMessageMaxBytes);

    /// <summary>
    ///     Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one.
    /// </summary>
    /// <param name="maxInFlight">
    ///     Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one.
    /// </param>
    void WithMaxInFlight(int? maxInFlight);

    /// <summary>
    ///     Period of time in milliseconds at which topic and broker metadata is refreshed in order to proactively discover any new brokers, topics, partitions or partition leader changes. Use -1 to disable the intervalled refresh (not recommended). If there are no locally referenced topics (no topic objects created, no messages produced, no subscription or no assignment) then only the broker list will be refreshed every interval but no more often than every 10s.
    /// </summary>
    /// <param name="topicMetadataRefreshIntervalMs">
    ///     Period of time in milliseconds at which topic and broker metadata is refreshed in order to proactively discover any new brokers, topics, partitions or partition leader changes. Use -1 to disable the intervalled refresh (not recommended). If there are no locally referenced topics (no topic objects created, no messages produced, no subscription or no assignment) then only the broker list will be refreshed every interval but no more often than every 10s.
    /// </param>
    void WithTopicMetadataRefreshIntervalMs(int? topicMetadataRefreshIntervalMs);

    /// <summary>
    ///     Metadata cache max age. Defaults to topic.metadata.refresh.interval.ms * 3
    /// </summary>
    /// <param name="metadataMaxAgeMs">
    ///     Metadata cache max age. Defaults to topic.metadata.refresh.interval.ms * 3
    /// </param>
    void WithMetadataMaxAgeMs(int? metadataMaxAgeMs);

    /// <summary>
    ///     When a topic loses its leader a new metadata request will be enqueued with this initial interval, exponentially increasing until the topic metadata has been refreshed. This is used to recover quickly from transitioning leader brokers.
    /// </summary>
    /// <param name="topicMetadataRefreshFastIntervalMs">
    ///     When a topic loses its leader a new metadata request will be enqueued with this initial interval, exponentially increasing until the topic metadata has been refreshed. This is used to recover quickly from transitioning leader brokers.
    /// </param>
    void WithTopicMetadataRefreshFastIntervalMs(int? topicMetadataRefreshFastIntervalMs);

    /// <summary>
    ///     Sparse metadata requests (consumes less network bandwidth)
    /// </summary>
    /// <param name="topicMetadataRefreshSparse">
    ///     Sparse metadata requests (consumes less network bandwidth)
    /// </param>
    void WithTopicMetadataRefreshSparse(bool? topicMetadataRefreshSparse);

    /// <summary>
    ///     Apache Kafka topic creation is asynchronous and it takes some time for a new topic to propagate throughout the cluster to all brokers. If a client requests topic metadata after manual topic creation but before the topic has been fully propagated to the broker the client is requesting metadata from, the topic will seem to be non-existent and the client will mark the topic as such, failing queued produced messages with `ERR__UNKNOWN_TOPIC`. This setting delays marking a topic as non-existent until the configured propagation max time has passed. The maximum propagation time is calculated from the time the topic is first referenced in the client, e.g., on produce().
    /// </summary>
    /// <param name="topicMetadataPropagationMaxMs">
    ///     Apache Kafka topic creation is asynchronous and it takes some time for a new topic to propagate throughout the cluster to all brokers. If a client requests topic metadata after manual topic creation but before the topic has been fully propagated to the broker the client is requesting metadata from, the topic will seem to be non-existent and the client will mark the topic as such, failing queued produced messages with `ERR__UNKNOWN_TOPIC`. This setting delays marking a topic as non-existent until the configured propagation max time has passed. The maximum propagation time is calculated from the time the topic is first referenced in the client, e.g., on produce().
    /// </param>
    void WithTopicMetadataPropagationMaxMs(int? topicMetadataPropagationMaxMs);

    /// <summary>
    ///     Topic blacklist, a comma-separated list of regular expressions for matching topic names that should be ignored in broker metadata information as if the topics did not exist.
    /// </summary>
    /// <param name="topicBlacklist">
    ///     Topic blacklist, a comma-separated list of regular expressions for matching topic names that should be ignored in broker metadata information as if the topics did not exist.
    /// </param>
    void WithTopicBlacklist(string? topicBlacklist);

    /// <summary>
    ///     A comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg. Consumer: consumer,cgrp,topic,fetch
    /// </summary>
    /// <param name="debug">
    ///     A comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg. Consumer: consumer,cgrp,topic,fetch
    /// </param>
    void WithDebug(string? debug);

    /// <summary>
    ///     Default timeout for network requests. Producer: ProduceRequests will use the lesser value of `socket.timeout.ms` and remaining `message.timeout.ms` for the first message in the batch. Consumer: FetchRequests will use `fetch.wait.max.ms` + `socket.timeout.ms`. Admin: Admin requests will use `socket.timeout.ms` or explicitly set `rd_kafka_AdminOptions_set_operation_timeout()` value.
    /// </summary>
    /// <param name="socketTimeoutMs">
    ///     Default timeout for network requests. Producer: ProduceRequests will use the lesser value of `socket.timeout.ms` and remaining `message.timeout.ms` for the first message in the batch. Consumer: FetchRequests will use `fetch.wait.max.ms` + `socket.timeout.ms`. Admin: Admin requests will use `socket.timeout.ms` or explicitly set `rd_kafka_AdminOptions_set_operation_timeout()` value.
    /// </param>
    void WithSocketTimeoutMs(int? socketTimeoutMs);

    /// <summary>
    ///     Broker socket send buffer size. System default is used if 0.
    /// </summary>
    /// <param name="socketSendBufferBytes">
    ///     Broker socket send buffer size. System default is used if 0.
    /// </param>
    void WithSocketSendBufferBytes(int? socketSendBufferBytes);

    /// <summary>
    ///     Broker socket receive buffer size. System default is used if 0.
    /// </summary>
    /// <param name="socketReceiveBufferBytes">
    ///     Broker socket receive buffer size. System default is used if 0.
    /// </param>
    void WithSocketReceiveBufferBytes(int? socketReceiveBufferBytes);

    /// <summary>
    ///     Enable TCP keep-alives (SO_KEEPALIVE) on broker sockets
    /// </summary>
    /// <param name="socketKeepaliveEnable">
    ///     Enable TCP keep-alives (SO_KEEPALIVE) on broker sockets
    /// </param>
    void WithSocketKeepaliveEnable(bool? socketKeepaliveEnable);

    /// <summary>
    ///     Disable the Nagle algorithm (TCP_NODELAY) on broker sockets.
    /// </summary>
    /// <param name="socketNagleDisable">
    ///     Disable the Nagle algorithm (TCP_NODELAY) on broker sockets.
    /// </param>
    void WithSocketNagleDisable(bool? socketNagleDisable);

    /// <summary>
    ///     Disconnect from broker when this number of send failures (e.g., timed out requests) is reached. Disable with 0. WARNING: It is highly recommended to leave this setting at its default value of 1 to avoid the client and broker to become desynchronized in case of request timeouts. NOTE: The connection is automatically re-established.
    /// </summary>
    /// <param name="socketMaxFails">
    ///     Disconnect from broker when this number of send failures (e.g., timed out requests) is reached. Disable with 0. WARNING: It is highly recommended to leave this setting at its default value of 1 to avoid the client and broker to become desynchronized in case of request timeouts. NOTE: The connection is automatically re-established.
    /// </param>
    void WithSocketMaxFails(int? socketMaxFails);

    /// <summary>
    ///     How long to cache the broker address resolving results (milliseconds).
    /// </summary>
    /// <param name="brokerAddressTtl">
    ///     How long to cache the broker address resolving results (milliseconds).
    /// </param>
    void WithBrokerAddressTtl(int? brokerAddressTtl);

    /// <summary>
    ///     Allowed broker IP address families: any, v4, v6
    /// </summary>
    /// <param name="brokerAddressFamily">
    ///     Allowed broker IP address families: any, v4, v6
    /// </param>
    void WithBrokerAddressFamily(BrokerAddressFamily? brokerAddressFamily);

    /// <summary>
    ///     Close broker connections after the specified time of inactivity. Disable with 0. If this property is left at its default value some heuristics are performed to determine a suitable default value, this is currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info).
    /// </summary>
    /// <param name="connectionsMaxIdleMs">
    ///     Close broker connections after the specified time of inactivity. Disable with 0. If this property is left at its default value some heuristics are performed to determine a suitable default value, this is currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info).
    /// </param>
    void WithConnectionsMaxIdleMs(int? connectionsMaxIdleMs);

    /// <summary>
    ///     The initial time to wait before reconnecting to a broker after the connection has been closed. The time is increased exponentially until `reconnect.backoff.max.ms` is reached. -25% to +50% jitter is applied to each reconnect backoff. A value of 0 disables the backoff and reconnects immediately.
    /// </summary>
    /// <param name="reconnectBackoffMs">
    ///     The initial time to wait before reconnecting to a broker after the connection has been closed. The time is increased exponentially until `reconnect.backoff.max.ms` is reached. -25% to +50% jitter is applied to each reconnect backoff. A value of 0 disables the backoff and reconnects immediately.
    /// </param>
    void WithReconnectBackoffMs(int? reconnectBackoffMs);

    /// <summary>
    ///     The maximum time to wait before reconnecting to a broker after the connection has been closed.
    /// </summary>
    /// <param name="reconnectBackoffMaxMs">
    ///     The maximum time to wait before reconnecting to a broker after the connection has been closed.
    /// </param>
    void WithReconnectBackoffMaxMs(int? reconnectBackoffMaxMs);

    /// <summary>
    ///     librdkafka statistics emit interval. The application also needs to register a stats callback using `rd_kafka_conf_set_stats_cb()`. The granularity is 1000ms. A value of 0 disables statistics.
    /// </summary>
    /// <param name="statisticsIntervalMs">
    ///     librdkafka statistics emit interval. The application also needs to register a stats callback using `rd_kafka_conf_set_stats_cb()`. The granularity is 1000ms. A value of 0 disables statistics.
    /// </param>
    void WithStatisticsIntervalMs(int? statisticsIntervalMs);

    /// <summary>
    ///     Disable spontaneous log_cb from internal librdkafka threads, instead enqueue log messages on queue set with `rd_kafka_set_log_queue()` and serve log callbacks or events through the standard poll APIs. **NOTE**: Log messages will linger in a temporary queue until the log queue has been set.
    /// </summary>
    /// <param name="logQueue">
    ///     Disable spontaneous log_cb from internal librdkafka threads, instead enqueue log messages on queue set with `rd_kafka_set_log_queue()` and serve log callbacks or events through the standard poll APIs. **NOTE**: Log messages will linger in a temporary queue until the log queue has been set.
    /// </param>
    void WithLogQueue(bool? logQueue);

    /// <summary>
    ///     Print internal thread name in log messages (useful for debugging librdkafka internals)
    /// </summary>
    /// <param name="logThreadName">
    ///     Print internal thread name in log messages (useful for debugging librdkafka internals)
    /// </param>
    void WithLogThreadName(bool? logThreadName);

    /// <summary>
    ///     If enabled librdkafka will initialize the PRNG with srand(current_time.milliseconds) on the first invocation of rd_kafka_new() (required only if rand_r() is not available on your platform). If disabled the application must call srand() prior to calling rd_kafka_new().
    /// </summary>
    /// <param name="enableRandomSeed">
    ///     If enabled librdkafka will initialize the PRNG with srand(current_time.milliseconds) on the first invocation of rd_kafka_new() (required only if rand_r() is not available on your platform). If disabled the application must call srand() prior to calling rd_kafka_new().
    /// </param>
    void WithEnableRandomSeed(bool? enableRandomSeed);

    /// <summary>
    ///     Log broker disconnects. It might be useful to turn this off when interacting with 0.9 brokers with an aggressive `connection.max.idle.ms` value.
    /// </summary>
    /// <param name="logConnectionClose">
    ///     Log broker disconnects. It might be useful to turn this off when interacting with 0.9 brokers with an aggressive `connection.max.idle.ms` value.
    /// </param>
    void WithLogConnectionClose(bool? logConnectionClose);

    /// <summary>
    ///     Signal that librdkafka will use to quickly terminate on rd_kafka_destroy(). If this signal is not set then there will be a delay before rd_kafka_wait_destroyed() returns true as internal threads are timing out their system calls. If this signal is set however the delay will be minimal. The application should mask this signal as an internal signal handler is installed.
    /// </summary>
    /// <param name="internalTerminationSignal">
    ///     Signal that librdkafka will use to quickly terminate on rd_kafka_destroy(). If this signal is not set then there will be a delay before rd_kafka_wait_destroyed() returns true as internal threads are timing out their system calls. If this signal is set however the delay will be minimal. The application should mask this signal as an internal signal handler is installed.
    /// </param>
    void WithInternalTerminationSignal(int? internalTerminationSignal);

    /// <summary>
    ///     Request broker's supported API versions to adjust functionality to available protocol features. If set to false, or the ApiVersionRequest fails, the fallback version `broker.version.fallback` will be used. **NOTE**: Depends on broker version &gt;=0.10.0. If the request is not supported by (an older) broker the `broker.version.fallback` fallback is used.
    /// </summary>
    /// <param name="apiVersionRequest">
    ///     Request broker's supported API versions to adjust functionality to available protocol features. If set to false, or the ApiVersionRequest fails, the fallback version `broker.version.fallback` will be used. **NOTE**: Depends on broker version &gt;=0.10.0. If the request is not supported by (an older) broker the `broker.version.fallback` fallback is used.
    /// </param>
    void WithApiVersionRequest(bool? apiVersionRequest);

    /// <summary>
    ///     Timeout for broker API version requests.
    /// </summary>
    /// <param name="apiVersionRequestTimeoutMs">
    ///     Timeout for broker API version requests.
    /// </param>
    void WithApiVersionRequestTimeoutMs(int? apiVersionRequestTimeoutMs);

    /// <summary>
    ///     Dictates how long the `broker.version.fallback` fallback is used in the case the ApiVersionRequest fails. **NOTE**: The ApiVersionRequest is only issued when a new connection to the broker is made (such as after an upgrade).
    /// </summary>
    /// <param name="apiVersionFallbackMs">
    ///     Dictates how long the `broker.version.fallback` fallback is used in the case the ApiVersionRequest fails. **NOTE**: The ApiVersionRequest is only issued when a new connection to the broker is made (such as after an upgrade).
    /// </param>
    void WithApiVersionFallbackMs(int? apiVersionFallbackMs);

    /// <summary>
    ///     Older broker versions (before 0.10.0) provide no way for a client to query for supported protocol features (ApiVersionRequest, see `api.version.request`) making it impossible for the client to know what features it may use. As a workaround a user may set this property to the expected broker version and the client will automatically adjust its feature set accordingly if the ApiVersionRequest fails (or is disabled). The fallback broker version will be used for `api.version.fallback.ms`. Valid values are: 0.9.0, 0.8.2, 0.8.1, 0.8.0. Any other value &gt;= 0.10, such as 0.10.2.1, enables ApiVersionRequests.
    /// </summary>
    /// <param name="brokerVersionFallback">
    ///     Older broker versions (before 0.10.0) provide no way for a client to query for supported protocol features (ApiVersionRequest, see `api.version.request`) making it impossible for the client to know what features it may use. As a workaround a user may set this property to the expected broker version and the client will automatically adjust its feature set accordingly if the ApiVersionRequest fails (or is disabled). The fallback broker version will be used for `api.version.fallback.ms`. Valid values are: 0.9.0, 0.8.2, 0.8.1, 0.8.0. Any other value &gt;= 0.10, such as 0.10.2.1, enables ApiVersionRequests.
    /// </param>
    void WithBrokerVersionFallback(string? brokerVersionFallback);

    /// <summary>
    ///     Protocol used to communicate with brokers.
    /// </summary>
    /// <param name="securityProtocol">
    ///     Protocol used to communicate with brokers.
    /// </param>
    void WithSecurityProtocol(SecurityProtocol? securityProtocol);

    /// <summary>
    ///     A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. See manual page for `ciphers(1)` and `SSL_CTX_set_cipher_list(3).
    /// </summary>
    /// <param name="sslCipherSuites">
    ///     A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. See manual page for `ciphers(1)` and `SSL_CTX_set_cipher_list(3).
    /// </param>
    void WithSslCipherSuites(string? sslCipherSuites);

    /// <summary>
    ///     The supported-curves extension in the TLS ClientHello message specifies the curves (standard/named, or 'explicit' GF(2^k) or GF(p)) the client is willing to have the server use. See manual page for `SSL_CTX_set1_curves_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </summary>
    /// <param name="sslCurvesList">
    ///     The supported-curves extension in the TLS ClientHello message specifies the curves (standard/named, or 'explicit' GF(2^k) or GF(p)) the client is willing to have the server use. See manual page for `SSL_CTX_set1_curves_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </param>
    void WithSslCurvesList(string? sslCurvesList);

    /// <summary>
    ///     The client uses the TLS ClientHello signature_algorithms extension to indicate to the server which signature/hash algorithm pairs may be used in digital signatures. See manual page for `SSL_CTX_set1_sigalgs_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </summary>
    /// <param name="sslSigalgsList">
    ///     The client uses the TLS ClientHello signature_algorithms extension to indicate to the server which signature/hash algorithm pairs may be used in digital signatures. See manual page for `SSL_CTX_set1_sigalgs_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </param>
    void WithSslSigalgsList(string? sslSigalgsList);

    /// <summary>
    ///     Path to client's private key (PEM) used for authentication.
    /// </summary>
    /// <param name="sslKeyLocation">
    ///     Path to client's private key (PEM) used for authentication.
    /// </param>
    void WithSslKeyLocation(string? sslKeyLocation);

    /// <summary>
    ///     Private key passphrase (for use with `ssl.key.location` and `set_ssl_cert()`)
    /// </summary>
    /// <param name="sslKeyPassword">
    ///     Private key passphrase (for use with `ssl.key.location` and `set_ssl_cert()`)
    /// </param>
    void WithSslKeyPassword(string? sslKeyPassword);

    /// <summary>
    ///     Client's private key string (PEM format) used for authentication.
    /// </summary>
    /// <param name="sslKeyPem">
    ///     Client's private key string (PEM format) used for authentication.
    /// </param>
    void WithSslKeyPem(string? sslKeyPem);

    /// <summary>
    ///     Path to client's public key (PEM) used for authentication.
    /// </summary>
    /// <param name="sslCertificateLocation">
    ///     Path to client's public key (PEM) used for authentication.
    /// </param>
    void WithSslCertificateLocation(string? sslCertificateLocation);

    /// <summary>
    ///     Client's public key string (PEM format) used for authentication.
    /// </summary>
    /// <param name="sslCertificatePem">
    ///     Client's public key string (PEM format) used for authentication.
    /// </param>
    void WithSslCertificatePem(string? sslCertificatePem);

    /// <summary>
    ///     File or directory path to CA certificate(s) for verifying the broker's key. Defaults: On Windows the system's CA certificates are automatically looked up in the Windows Root certificate store. On Mac OSX this configuration defaults to `probe`. It is recommended to install openssl using Homebrew, to provide CA certificates. On Linux install the distribution's ca-certificates package. If OpenSSL is statically linked or `ssl.ca.location` is set to `probe` a list of standard paths will be probed and the first one found will be used as the default CA certificate location path. If OpenSSL is dynamically linked the OpenSSL library's default path will be used (see `OPENSSLDIR` in `openssl version -a`).
    /// </summary>
    /// <param name="sslCaLocation">
    ///     File or directory path to CA certificate(s) for verifying the broker's key. Defaults: On Windows the system's CA certificates are automatically looked up in the Windows Root certificate store. On Mac OSX this configuration defaults to `probe`. It is recommended to install openssl using Homebrew, to provide CA certificates. On Linux install the distribution's ca-certificates package. If OpenSSL is statically linked or `ssl.ca.location` is set to `probe` a list of standard paths will be probed and the first one found will be used as the default CA certificate location path. If OpenSSL is dynamically linked the OpenSSL library's default path will be used (see `OPENSSLDIR` in `openssl version -a`).
    /// </param>
    void WithSslCaLocation(string? sslCaLocation);

    /// <summary>
    ///     CA certificate string (PEM format) for verifying the broker's key.
    /// </summary>
    /// <param name="sslCaPem">
    ///     CA certificate string (PEM format) for verifying the broker's key.
    /// </param>
    void WithSslCaPem(string? sslCaPem);

    /// <summary>
    ///     Comma-separated list of Windows Certificate stores to load CA certificates from. Certificates will be loaded in the same order as stores are specified. If no certificates can be loaded from any of the specified stores an error is logged and the OpenSSL library's default CA location is used instead. Store names are typically one or more of: MY, Root, Trust, CA.
    /// </summary>
    /// <param name="sslCaCertificateStores">
    ///     Comma-separated list of Windows Certificate stores to load CA certificates from. Certificates will be loaded in the same order as stores are specified. If no certificates can be loaded from any of the specified stores an error is logged and the OpenSSL library's default CA location is used instead. Store names are typically one or more of: MY, Root, Trust, CA.
    /// </param>
    void WithSslCaCertificateStores(string? sslCaCertificateStores);

    /// <summary>
    ///     Path to CRL for verifying broker's certificate validity.
    /// </summary>
    /// <param name="sslCrlLocation">
    ///     Path to CRL for verifying broker's certificate validity.
    /// </param>
    void WithSslCrlLocation(string? sslCrlLocation);

    /// <summary>
    ///     Path to client's keystore (PKCS#12) used for authentication.
    /// </summary>
    /// <param name="sslKeystoreLocation">
    ///     Path to client's keystore (PKCS#12) used for authentication.
    /// </param>
    void WithSslKeystoreLocation(string? sslKeystoreLocation);

    /// <summary>
    ///     Client's keystore (PKCS#12) password.
    /// </summary>
    /// <param name="sslKeystorePassword">
    ///     Client's keystore (PKCS#12) password.
    /// </param>
    void WithSslKeystorePassword(string? sslKeystorePassword);

    /// <summary>
    ///     Path to OpenSSL engine library. OpenSSL &gt;= 1.1.0 required.
    /// </summary>
    /// <param name="sslEngineLocation">
    ///     Path to OpenSSL engine library. OpenSSL &gt;= 1.1.0 required.
    /// </param>
    void WithSslEngineLocation(string? sslEngineLocation);

    /// <summary>
    ///     OpenSSL engine id is the name used for loading engine.
    /// </summary>
    /// <param name="sslEngineId">
    ///     OpenSSL engine id is the name used for loading engine.
    /// </param>
    void WithSslEngineId(string? sslEngineId);

    /// <summary>
    ///     Enable OpenSSL's builtin broker (server) certificate verification. This verification can be extended by the application by implementing a certificate_verify_cb.
    /// </summary>
    /// <param name="enableSslCertificateVerification">
    ///     Enable OpenSSL's builtin broker (server) certificate verification. This verification can be extended by the application by implementing a certificate_verify_cb.
    /// </param>
    void WithEnableSslCertificateVerification(bool? enableSslCertificateVerification);

    /// <summary>
    ///     Endpoint identification algorithm to validate broker hostname using broker certificate. https - Server (broker) hostname verification as specified in RFC2818. none - No endpoint verification. OpenSSL &gt;= 1.0.2 required.
    /// </summary>
    /// <param name="sslEndpointIdentificationAlgorithm">
    ///     Endpoint identification algorithm to validate broker hostname using broker certificate. https - Server (broker) hostname verification as specified in RFC2818. none - No endpoint verification. OpenSSL &gt;= 1.0.2 required.
    /// </param>
    void WithSslEndpointIdentificationAlgorithm(SslEndpointIdentificationAlgorithm? sslEndpointIdentificationAlgorithm);

    /// <summary>
    ///     Kerberos principal name that Kafka runs as, not including /hostname@REALM
    /// </summary>
    /// <param name="saslKerberosServiceName">
    ///     Kerberos principal name that Kafka runs as, not including /hostname@REALM
    /// </param>
    void WithSaslKerberosServiceName(string? saslKerberosServiceName);

    /// <summary>
    ///     This client's Kerberos principal name. (Not supported on Windows, will use the logon user's principal).
    /// </summary>
    /// <param name="saslKerberosPrincipal">
    ///     This client's Kerberos principal name. (Not supported on Windows, will use the logon user's principal).
    /// </param>
    void WithSaslKerberosPrincipal(string? saslKerberosPrincipal);

    /// <summary>
    ///     Shell command to refresh or acquire the client's Kerberos ticket. This command is executed on client creation and every sasl.kerberos.min.time.before.relogin (0=disable). %{config.prop.name} is replaced by corresponding config object value.
    /// </summary>
    /// <param name="saslKerberosKinitCmd">
    ///     Shell command to refresh or acquire the client's Kerberos ticket. This command is executed on client creation and every sasl.kerberos.min.time.before.relogin (0=disable). %{config.prop.name} is replaced by corresponding config object value.
    /// </param>
    void WithSaslKerberosKinitCmd(string? saslKerberosKinitCmd);

    /// <summary>
    ///     Path to Kerberos keytab file. This configuration property is only used as a variable in `sasl.kerberos.kinit.cmd` as ` ... -t "%{sasl.kerberos.keytab}"`.
    /// </summary>
    /// <param name="saslKerberosKeytab">
    ///     Path to Kerberos keytab file. This configuration property is only used as a variable in `sasl.kerberos.kinit.cmd` as ` ... -t "%{sasl.kerberos.keytab}"`.
    /// </param>
    void WithSaslKerberosKeytab(string? saslKerberosKeytab);

    /// <summary>
    ///     Minimum time in milliseconds between key refresh attempts. Disable automatic key refresh by setting this property to 0.
    /// </summary>
    /// <param name="saslKerberosMinTimeBeforeRelogin">
    ///     Minimum time in milliseconds between key refresh attempts. Disable automatic key refresh by setting this property to 0.
    /// </param>
    void WithSaslKerberosMinTimeBeforeRelogin(int? saslKerberosMinTimeBeforeRelogin);

    /// <summary>
    ///     SASL username for use with the PLAIN and SASL-SCRAM-.. mechanisms
    /// </summary>
    /// <param name="saslUsername">
    ///     SASL username for use with the PLAIN and SASL-SCRAM-.. mechanisms
    /// </param>
    void WithSaslUsername(string? saslUsername);

    /// <summary>
    ///     SASL password for use with the PLAIN and SASL-SCRAM-.. mechanism
    /// </summary>
    /// <param name="saslPassword">
    ///     SASL password for use with the PLAIN and SASL-SCRAM-.. mechanism
    /// </param>
    void WithSaslPassword(string? saslPassword);

    /// <summary>
    ///     SASL/OAUTHBEARER configuration. The format is implementation-dependent and must be parsed accordingly. The default unsecured token implementation (see https://tools.ietf.org/html/rfc7515#appendix-A.5) recognizes space-separated name=value pairs with valid names including principalClaimName, principal, scopeClaimName, scope, and lifeSeconds. The default value for principalClaimName is "sub", the default value for scopeClaimName is "scope", and the default value for lifeSeconds is 3600. The scope value is CSV format with the default value being no/empty scope. For example: `principalClaimName=azp principal=admin scopeClaimName=roles scope=role1,role2 lifeSeconds=600`. In addition, SASL extensions can be communicated to the broker via `extension_NAME=value`. For example: `principal=admin extension_traceId=123`
    /// </summary>
    /// <param name="saslOauthbearerConfig">
    ///     SASL/OAUTHBEARER configuration. The format is implementation-dependent and must be parsed accordingly. The default unsecured token implementation (see https://tools.ietf.org/html/rfc7515#appendix-A.5) recognizes space-separated name=value pairs with valid names including principalClaimName, principal, scopeClaimName, scope, and lifeSeconds. The default value for principalClaimName is "sub", the default value for scopeClaimName is "scope", and the default value for lifeSeconds is 3600. The scope value is CSV format with the default value being no/empty scope. For example: `principalClaimName=azp principal=admin scopeClaimName=roles scope=role1,role2 lifeSeconds=600`. In addition, SASL extensions can be communicated to the broker via `extension_NAME=value`. For example: `principal=admin extension_traceId=123`
    /// </param>
    void WithSaslOauthbearerConfig(string? saslOauthbearerConfig);

    /// <summary>
    ///     Enable the builtin unsecure JWT OAUTHBEARER token handler if no oauthbearer_refresh_cb has been set. This builtin handler should only be used for development or testing, and not in production.
    /// </summary>
    /// <param name="enableSaslOauthbearerUnsecureJwt">
    ///     Enable the builtin unsecure JWT OAUTHBEARER token handler if no oauthbearer_refresh_cb has been set. This builtin handler should only be used for development or testing, and not in production.
    /// </param>
    void WithEnableSaslOauthbearerUnsecureJwt(bool? enableSaslOauthbearerUnsecureJwt);

    /// <summary>
    ///     List of plugin libraries to load (; separated). The library search path is platform dependent (see dlopen(3) for Unix and LoadLibrary() for Windows). If no filename extension is specified the platform-specific extension (such as .dll or .so) will be appended automatically.
    /// </summary>
    /// <param name="pluginLibraryPaths">
    ///     List of plugin libraries to load (; separated). The library search path is platform dependent (see dlopen(3) for Unix and LoadLibrary() for Windows). If no filename extension is specified the platform-specific extension (such as .dll or .so) will be appended automatically.
    /// </param>
    void WithPluginLibraryPaths(string? pluginLibraryPaths);

    /// <summary>
    ///     A rack identifier for this client. This can be any string value which indicates where this client is physically located. It corresponds with the broker config `broker.rack`.
    /// </summary>
    /// <param name="clientRack">
    ///     A rack identifier for this client. This can be any string value which indicates where this client is physically located. It corresponds with the broker config `broker.rack`.
    /// </param>
    void WithClientRack(string? clientRack);

    /// <summary>
    ///     The maximum length of time (in milliseconds) before a cancellation request
    ///     is acted on. Low values may result in measurably higher CPU usage.
    ///     range: 1 &lt;= dotnet.cancellation.delay.max.ms &lt;= 10000
    /// </summary>
    /// <param name="cancellationDelayMaxMs">
    ///     The maximum length of time (in milliseconds) before a cancellation request
    ///     is acted on. Low values may result in measurably higher CPU usage.
    ///     range: 1 &lt;= dotnet.cancellation.delay.max.ms &lt;= 10000
    /// </param>
    void WithCancellationDelayMaxMs(int cancellationDelayMaxMs);
}

/// <content>
///     The autogenerated part of the <see cref="KafkaClientsConfigurationBuilder" /> class.
/// </content>
[SuppressMessage("", "SA1649", Justification = "Autogenerated all at once")]
[SuppressMessage("", "SA1402", Justification = "Autogenerated all at once")]
[SuppressMessage("", "CA1200", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1623", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1629", Justification = "Summary copied from wrapped class")]
[SuppressMessage("StyleCop.CSharp.DocumentationRules", "SA1625:Element documentation should not be copied and pasted", Justification = "Summary copied from wrapped class")]
public partial class KafkaClientsConfigurationBuilder
{
    /// <summary>
    ///     SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512. **NOTE**: Despite the name, you may not configure more than one mechanism.
    /// </summary>
    /// <param name="saslMechanism">
    ///     SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512. **NOTE**: Despite the name, you may not configure more than one mechanism.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSaslMechanism(SaslMechanism? saslMechanism)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSaslMechanism(saslMechanism));
        return this;
    }

    /// <summary>
    ///     This field indicates the number of acknowledgements the leader broker must receive from ISR brokers
    ///     before responding to the request: Zero=Broker does not send any response/ack to client, One=The
    ///     leader will write the record to its local log but will respond without awaiting full acknowledgement
    ///     from all followers. All=Broker will block until message is committed by all in sync replicas (ISRs).
    ///     If there are less than min.insync.replicas (broker configuration) in the ISR set the produce request
    ///     will fail.
    /// </summary>
    /// <param name="acks">
    ///     This field indicates the number of acknowledgements the leader broker must receive from ISR brokers
    ///     before responding to the request: Zero=Broker does not send any response/ack to client, One=The
    ///     leader will write the record to its local log but will respond without awaiting full acknowledgement
    ///     from all followers. All=Broker will block until message is committed by all in sync replicas (ISRs).
    ///     If there are less than min.insync.replicas (broker configuration) in the ISR set the produce request
    ///     will fail.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithAcks(Acks? acks)
    {
        _sharedConfigurationActions.Add(builder => builder.WithAcks(acks));
        return this;
    }

    /// <summary>
    ///     Initial list of brokers as a CSV list of broker host or host:port. The application may also use `rd_kafka_brokers_add()` to add brokers during runtime.
    /// </summary>
    /// <param name="bootstrapServers">
    ///     Initial list of brokers as a CSV list of broker host or host:port. The application may also use `rd_kafka_brokers_add()` to add brokers during runtime.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithBootstrapServers(string? bootstrapServers)
    {
        _sharedConfigurationActions.Add(builder => builder.WithBootstrapServers(bootstrapServers));
        return this;
    }

    /// <summary>
    ///     Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic's `max.message.bytes` limit (see Apache Kafka documentation).
    /// </summary>
    /// <param name="messageMaxBytes">
    ///     Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic's `max.message.bytes` limit (see Apache Kafka documentation).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithMessageMaxBytes(int? messageMaxBytes)
    {
        _sharedConfigurationActions.Add(builder => builder.WithMessageMaxBytes(messageMaxBytes));
        return this;
    }

    /// <summary>
    ///     Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference (zero-copy) at the expense of larger iovecs.
    /// </summary>
    /// <param name="messageCopyMaxBytes">
    ///     Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference (zero-copy) at the expense of larger iovecs.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithMessageCopyMaxBytes(int? messageCopyMaxBytes)
    {
        _sharedConfigurationActions.Add(builder => builder.WithMessageCopyMaxBytes(messageCopyMaxBytes));
        return this;
    }

    /// <summary>
    ///     Maximum Kafka protocol response message size. This serves as a safety precaution to avoid memory exhaustion in case of protocol hickups. This value must be at least `fetch.max.bytes`  + 512 to allow for protocol overhead; the value is adjusted automatically unless the configuration property is explicitly set.
    /// </summary>
    /// <param name="receiveMessageMaxBytes">
    ///     Maximum Kafka protocol response message size. This serves as a safety precaution to avoid memory exhaustion in case of protocol hickups. This value must be at least `fetch.max.bytes`  + 512 to allow for protocol overhead; the value is adjusted automatically unless the configuration property is explicitly set.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithReceiveMessageMaxBytes(int? receiveMessageMaxBytes)
    {
        _sharedConfigurationActions.Add(builder => builder.WithReceiveMessageMaxBytes(receiveMessageMaxBytes));
        return this;
    }

    /// <summary>
    ///     Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one.
    /// </summary>
    /// <param name="maxInFlight">
    ///     Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithMaxInFlight(int? maxInFlight)
    {
        _sharedConfigurationActions.Add(builder => builder.WithMaxInFlight(maxInFlight));
        return this;
    }

    /// <summary>
    ///     Period of time in milliseconds at which topic and broker metadata is refreshed in order to proactively discover any new brokers, topics, partitions or partition leader changes. Use -1 to disable the intervalled refresh (not recommended). If there are no locally referenced topics (no topic objects created, no messages produced, no subscription or no assignment) then only the broker list will be refreshed every interval but no more often than every 10s.
    /// </summary>
    /// <param name="topicMetadataRefreshIntervalMs">
    ///     Period of time in milliseconds at which topic and broker metadata is refreshed in order to proactively discover any new brokers, topics, partitions or partition leader changes. Use -1 to disable the intervalled refresh (not recommended). If there are no locally referenced topics (no topic objects created, no messages produced, no subscription or no assignment) then only the broker list will be refreshed every interval but no more often than every 10s.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithTopicMetadataRefreshIntervalMs(int? topicMetadataRefreshIntervalMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithTopicMetadataRefreshIntervalMs(topicMetadataRefreshIntervalMs));
        return this;
    }

    /// <summary>
    ///     Metadata cache max age. Defaults to topic.metadata.refresh.interval.ms * 3
    /// </summary>
    /// <param name="metadataMaxAgeMs">
    ///     Metadata cache max age. Defaults to topic.metadata.refresh.interval.ms * 3
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithMetadataMaxAgeMs(int? metadataMaxAgeMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithMetadataMaxAgeMs(metadataMaxAgeMs));
        return this;
    }

    /// <summary>
    ///     When a topic loses its leader a new metadata request will be enqueued with this initial interval, exponentially increasing until the topic metadata has been refreshed. This is used to recover quickly from transitioning leader brokers.
    /// </summary>
    /// <param name="topicMetadataRefreshFastIntervalMs">
    ///     When a topic loses its leader a new metadata request will be enqueued with this initial interval, exponentially increasing until the topic metadata has been refreshed. This is used to recover quickly from transitioning leader brokers.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithTopicMetadataRefreshFastIntervalMs(int? topicMetadataRefreshFastIntervalMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithTopicMetadataRefreshFastIntervalMs(topicMetadataRefreshFastIntervalMs));
        return this;
    }

    /// <summary>
    ///     Sparse metadata requests (consumes less network bandwidth)
    /// </summary>
    /// <param name="topicMetadataRefreshSparse">
    ///     Sparse metadata requests (consumes less network bandwidth)
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithTopicMetadataRefreshSparse(bool? topicMetadataRefreshSparse)
    {
        _sharedConfigurationActions.Add(builder => builder.WithTopicMetadataRefreshSparse(topicMetadataRefreshSparse));
        return this;
    }

    /// <summary>
    ///     Apache Kafka topic creation is asynchronous and it takes some time for a new topic to propagate throughout the cluster to all brokers. If a client requests topic metadata after manual topic creation but before the topic has been fully propagated to the broker the client is requesting metadata from, the topic will seem to be non-existent and the client will mark the topic as such, failing queued produced messages with `ERR__UNKNOWN_TOPIC`. This setting delays marking a topic as non-existent until the configured propagation max time has passed. The maximum propagation time is calculated from the time the topic is first referenced in the client, e.g., on produce().
    /// </summary>
    /// <param name="topicMetadataPropagationMaxMs">
    ///     Apache Kafka topic creation is asynchronous and it takes some time for a new topic to propagate throughout the cluster to all brokers. If a client requests topic metadata after manual topic creation but before the topic has been fully propagated to the broker the client is requesting metadata from, the topic will seem to be non-existent and the client will mark the topic as such, failing queued produced messages with `ERR__UNKNOWN_TOPIC`. This setting delays marking a topic as non-existent until the configured propagation max time has passed. The maximum propagation time is calculated from the time the topic is first referenced in the client, e.g., on produce().
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithTopicMetadataPropagationMaxMs(int? topicMetadataPropagationMaxMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithTopicMetadataPropagationMaxMs(topicMetadataPropagationMaxMs));
        return this;
    }

    /// <summary>
    ///     Topic blacklist, a comma-separated list of regular expressions for matching topic names that should be ignored in broker metadata information as if the topics did not exist.
    /// </summary>
    /// <param name="topicBlacklist">
    ///     Topic blacklist, a comma-separated list of regular expressions for matching topic names that should be ignored in broker metadata information as if the topics did not exist.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithTopicBlacklist(string? topicBlacklist)
    {
        _sharedConfigurationActions.Add(builder => builder.WithTopicBlacklist(topicBlacklist));
        return this;
    }

    /// <summary>
    ///     A comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg. Consumer: consumer,cgrp,topic,fetch
    /// </summary>
    /// <param name="debug">
    ///     A comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg. Consumer: consumer,cgrp,topic,fetch
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithDebug(string? debug)
    {
        _sharedConfigurationActions.Add(builder => builder.WithDebug(debug));
        return this;
    }

    /// <summary>
    ///     Default timeout for network requests. Producer: ProduceRequests will use the lesser value of `socket.timeout.ms` and remaining `message.timeout.ms` for the first message in the batch. Consumer: FetchRequests will use `fetch.wait.max.ms` + `socket.timeout.ms`. Admin: Admin requests will use `socket.timeout.ms` or explicitly set `rd_kafka_AdminOptions_set_operation_timeout()` value.
    /// </summary>
    /// <param name="socketTimeoutMs">
    ///     Default timeout for network requests. Producer: ProduceRequests will use the lesser value of `socket.timeout.ms` and remaining `message.timeout.ms` for the first message in the batch. Consumer: FetchRequests will use `fetch.wait.max.ms` + `socket.timeout.ms`. Admin: Admin requests will use `socket.timeout.ms` or explicitly set `rd_kafka_AdminOptions_set_operation_timeout()` value.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSocketTimeoutMs(int? socketTimeoutMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSocketTimeoutMs(socketTimeoutMs));
        return this;
    }

    /// <summary>
    ///     Broker socket send buffer size. System default is used if 0.
    /// </summary>
    /// <param name="socketSendBufferBytes">
    ///     Broker socket send buffer size. System default is used if 0.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSocketSendBufferBytes(int? socketSendBufferBytes)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSocketSendBufferBytes(socketSendBufferBytes));
        return this;
    }

    /// <summary>
    ///     Broker socket receive buffer size. System default is used if 0.
    /// </summary>
    /// <param name="socketReceiveBufferBytes">
    ///     Broker socket receive buffer size. System default is used if 0.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSocketReceiveBufferBytes(int? socketReceiveBufferBytes)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSocketReceiveBufferBytes(socketReceiveBufferBytes));
        return this;
    }

    /// <summary>
    ///     Enable TCP keep-alives (SO_KEEPALIVE) on broker sockets
    /// </summary>
    /// <param name="socketKeepaliveEnable">
    ///     Enable TCP keep-alives (SO_KEEPALIVE) on broker sockets
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSocketKeepaliveEnable(bool? socketKeepaliveEnable)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSocketKeepaliveEnable(socketKeepaliveEnable));
        return this;
    }

    /// <summary>
    ///     Disable the Nagle algorithm (TCP_NODELAY) on broker sockets.
    /// </summary>
    /// <param name="socketNagleDisable">
    ///     Disable the Nagle algorithm (TCP_NODELAY) on broker sockets.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSocketNagleDisable(bool? socketNagleDisable)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSocketNagleDisable(socketNagleDisable));
        return this;
    }

    /// <summary>
    ///     Disconnect from broker when this number of send failures (e.g., timed out requests) is reached. Disable with 0. WARNING: It is highly recommended to leave this setting at its default value of 1 to avoid the client and broker to become desynchronized in case of request timeouts. NOTE: The connection is automatically re-established.
    /// </summary>
    /// <param name="socketMaxFails">
    ///     Disconnect from broker when this number of send failures (e.g., timed out requests) is reached. Disable with 0. WARNING: It is highly recommended to leave this setting at its default value of 1 to avoid the client and broker to become desynchronized in case of request timeouts. NOTE: The connection is automatically re-established.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSocketMaxFails(int? socketMaxFails)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSocketMaxFails(socketMaxFails));
        return this;
    }

    /// <summary>
    ///     How long to cache the broker address resolving results (milliseconds).
    /// </summary>
    /// <param name="brokerAddressTtl">
    ///     How long to cache the broker address resolving results (milliseconds).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithBrokerAddressTtl(int? brokerAddressTtl)
    {
        _sharedConfigurationActions.Add(builder => builder.WithBrokerAddressTtl(brokerAddressTtl));
        return this;
    }

    /// <summary>
    ///     Allowed broker IP address families: any, v4, v6
    /// </summary>
    /// <param name="brokerAddressFamily">
    ///     Allowed broker IP address families: any, v4, v6
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithBrokerAddressFamily(BrokerAddressFamily? brokerAddressFamily)
    {
        _sharedConfigurationActions.Add(builder => builder.WithBrokerAddressFamily(brokerAddressFamily));
        return this;
    }

    /// <summary>
    ///     Close broker connections after the specified time of inactivity. Disable with 0. If this property is left at its default value some heuristics are performed to determine a suitable default value, this is currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info).
    /// </summary>
    /// <param name="connectionsMaxIdleMs">
    ///     Close broker connections after the specified time of inactivity. Disable with 0. If this property is left at its default value some heuristics are performed to determine a suitable default value, this is currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithConnectionsMaxIdleMs(int? connectionsMaxIdleMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithConnectionsMaxIdleMs(connectionsMaxIdleMs));
        return this;
    }

    /// <summary>
    ///     The initial time to wait before reconnecting to a broker after the connection has been closed. The time is increased exponentially until `reconnect.backoff.max.ms` is reached. -25% to +50% jitter is applied to each reconnect backoff. A value of 0 disables the backoff and reconnects immediately.
    /// </summary>
    /// <param name="reconnectBackoffMs">
    ///     The initial time to wait before reconnecting to a broker after the connection has been closed. The time is increased exponentially until `reconnect.backoff.max.ms` is reached. -25% to +50% jitter is applied to each reconnect backoff. A value of 0 disables the backoff and reconnects immediately.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithReconnectBackoffMs(int? reconnectBackoffMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithReconnectBackoffMs(reconnectBackoffMs));
        return this;
    }

    /// <summary>
    ///     The maximum time to wait before reconnecting to a broker after the connection has been closed.
    /// </summary>
    /// <param name="reconnectBackoffMaxMs">
    ///     The maximum time to wait before reconnecting to a broker after the connection has been closed.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithReconnectBackoffMaxMs(int? reconnectBackoffMaxMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithReconnectBackoffMaxMs(reconnectBackoffMaxMs));
        return this;
    }

    /// <summary>
    ///     librdkafka statistics emit interval. The application also needs to register a stats callback using `rd_kafka_conf_set_stats_cb()`. The granularity is 1000ms. A value of 0 disables statistics.
    /// </summary>
    /// <param name="statisticsIntervalMs">
    ///     librdkafka statistics emit interval. The application also needs to register a stats callback using `rd_kafka_conf_set_stats_cb()`. The granularity is 1000ms. A value of 0 disables statistics.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithStatisticsIntervalMs(int? statisticsIntervalMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithStatisticsIntervalMs(statisticsIntervalMs));
        return this;
    }

    /// <summary>
    ///     Disable spontaneous log_cb from internal librdkafka threads, instead enqueue log messages on queue set with `rd_kafka_set_log_queue()` and serve log callbacks or events through the standard poll APIs. **NOTE**: Log messages will linger in a temporary queue until the log queue has been set.
    /// </summary>
    /// <param name="logQueue">
    ///     Disable spontaneous log_cb from internal librdkafka threads, instead enqueue log messages on queue set with `rd_kafka_set_log_queue()` and serve log callbacks or events through the standard poll APIs. **NOTE**: Log messages will linger in a temporary queue until the log queue has been set.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithLogQueue(bool? logQueue)
    {
        _sharedConfigurationActions.Add(builder => builder.WithLogQueue(logQueue));
        return this;
    }

    /// <summary>
    ///     Print internal thread name in log messages (useful for debugging librdkafka internals)
    /// </summary>
    /// <param name="logThreadName">
    ///     Print internal thread name in log messages (useful for debugging librdkafka internals)
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithLogThreadName(bool? logThreadName)
    {
        _sharedConfigurationActions.Add(builder => builder.WithLogThreadName(logThreadName));
        return this;
    }

    /// <summary>
    ///     If enabled librdkafka will initialize the PRNG with srand(current_time.milliseconds) on the first invocation of rd_kafka_new() (required only if rand_r() is not available on your platform). If disabled the application must call srand() prior to calling rd_kafka_new().
    /// </summary>
    /// <param name="enableRandomSeed">
    ///     If enabled librdkafka will initialize the PRNG with srand(current_time.milliseconds) on the first invocation of rd_kafka_new() (required only if rand_r() is not available on your platform). If disabled the application must call srand() prior to calling rd_kafka_new().
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithEnableRandomSeed(bool? enableRandomSeed)
    {
        _sharedConfigurationActions.Add(builder => builder.WithEnableRandomSeed(enableRandomSeed));
        return this;
    }

    /// <summary>
    ///     Log broker disconnects. It might be useful to turn this off when interacting with 0.9 brokers with an aggressive `connection.max.idle.ms` value.
    /// </summary>
    /// <param name="logConnectionClose">
    ///     Log broker disconnects. It might be useful to turn this off when interacting with 0.9 brokers with an aggressive `connection.max.idle.ms` value.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithLogConnectionClose(bool? logConnectionClose)
    {
        _sharedConfigurationActions.Add(builder => builder.WithLogConnectionClose(logConnectionClose));
        return this;
    }

    /// <summary>
    ///     Signal that librdkafka will use to quickly terminate on rd_kafka_destroy(). If this signal is not set then there will be a delay before rd_kafka_wait_destroyed() returns true as internal threads are timing out their system calls. If this signal is set however the delay will be minimal. The application should mask this signal as an internal signal handler is installed.
    /// </summary>
    /// <param name="internalTerminationSignal">
    ///     Signal that librdkafka will use to quickly terminate on rd_kafka_destroy(). If this signal is not set then there will be a delay before rd_kafka_wait_destroyed() returns true as internal threads are timing out their system calls. If this signal is set however the delay will be minimal. The application should mask this signal as an internal signal handler is installed.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithInternalTerminationSignal(int? internalTerminationSignal)
    {
        _sharedConfigurationActions.Add(builder => builder.WithInternalTerminationSignal(internalTerminationSignal));
        return this;
    }

    /// <summary>
    ///     Request broker's supported API versions to adjust functionality to available protocol features. If set to false, or the ApiVersionRequest fails, the fallback version `broker.version.fallback` will be used. **NOTE**: Depends on broker version &gt;=0.10.0. If the request is not supported by (an older) broker the `broker.version.fallback` fallback is used.
    /// </summary>
    /// <param name="apiVersionRequest">
    ///     Request broker's supported API versions to adjust functionality to available protocol features. If set to false, or the ApiVersionRequest fails, the fallback version `broker.version.fallback` will be used. **NOTE**: Depends on broker version &gt;=0.10.0. If the request is not supported by (an older) broker the `broker.version.fallback` fallback is used.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithApiVersionRequest(bool? apiVersionRequest)
    {
        _sharedConfigurationActions.Add(builder => builder.WithApiVersionRequest(apiVersionRequest));
        return this;
    }

    /// <summary>
    ///     Timeout for broker API version requests.
    /// </summary>
    /// <param name="apiVersionRequestTimeoutMs">
    ///     Timeout for broker API version requests.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithApiVersionRequestTimeoutMs(int? apiVersionRequestTimeoutMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithApiVersionRequestTimeoutMs(apiVersionRequestTimeoutMs));
        return this;
    }

    /// <summary>
    ///     Dictates how long the `broker.version.fallback` fallback is used in the case the ApiVersionRequest fails. **NOTE**: The ApiVersionRequest is only issued when a new connection to the broker is made (such as after an upgrade).
    /// </summary>
    /// <param name="apiVersionFallbackMs">
    ///     Dictates how long the `broker.version.fallback` fallback is used in the case the ApiVersionRequest fails. **NOTE**: The ApiVersionRequest is only issued when a new connection to the broker is made (such as after an upgrade).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithApiVersionFallbackMs(int? apiVersionFallbackMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithApiVersionFallbackMs(apiVersionFallbackMs));
        return this;
    }

    /// <summary>
    ///     Older broker versions (before 0.10.0) provide no way for a client to query for supported protocol features (ApiVersionRequest, see `api.version.request`) making it impossible for the client to know what features it may use. As a workaround a user may set this property to the expected broker version and the client will automatically adjust its feature set accordingly if the ApiVersionRequest fails (or is disabled). The fallback broker version will be used for `api.version.fallback.ms`. Valid values are: 0.9.0, 0.8.2, 0.8.1, 0.8.0. Any other value &gt;= 0.10, such as 0.10.2.1, enables ApiVersionRequests.
    /// </summary>
    /// <param name="brokerVersionFallback">
    ///     Older broker versions (before 0.10.0) provide no way for a client to query for supported protocol features (ApiVersionRequest, see `api.version.request`) making it impossible for the client to know what features it may use. As a workaround a user may set this property to the expected broker version and the client will automatically adjust its feature set accordingly if the ApiVersionRequest fails (or is disabled). The fallback broker version will be used for `api.version.fallback.ms`. Valid values are: 0.9.0, 0.8.2, 0.8.1, 0.8.0. Any other value &gt;= 0.10, such as 0.10.2.1, enables ApiVersionRequests.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithBrokerVersionFallback(string? brokerVersionFallback)
    {
        _sharedConfigurationActions.Add(builder => builder.WithBrokerVersionFallback(brokerVersionFallback));
        return this;
    }

    /// <summary>
    ///     Protocol used to communicate with brokers.
    /// </summary>
    /// <param name="securityProtocol">
    ///     Protocol used to communicate with brokers.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSecurityProtocol(SecurityProtocol? securityProtocol)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSecurityProtocol(securityProtocol));
        return this;
    }

    /// <summary>
    ///     A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. See manual page for `ciphers(1)` and `SSL_CTX_set_cipher_list(3).
    /// </summary>
    /// <param name="sslCipherSuites">
    ///     A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. See manual page for `ciphers(1)` and `SSL_CTX_set_cipher_list(3).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslCipherSuites(string? sslCipherSuites)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslCipherSuites(sslCipherSuites));
        return this;
    }

    /// <summary>
    ///     The supported-curves extension in the TLS ClientHello message specifies the curves (standard/named, or 'explicit' GF(2^k) or GF(p)) the client is willing to have the server use. See manual page for `SSL_CTX_set1_curves_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </summary>
    /// <param name="sslCurvesList">
    ///     The supported-curves extension in the TLS ClientHello message specifies the curves (standard/named, or 'explicit' GF(2^k) or GF(p)) the client is willing to have the server use. See manual page for `SSL_CTX_set1_curves_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslCurvesList(string? sslCurvesList)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslCurvesList(sslCurvesList));
        return this;
    }

    /// <summary>
    ///     The client uses the TLS ClientHello signature_algorithms extension to indicate to the server which signature/hash algorithm pairs may be used in digital signatures. See manual page for `SSL_CTX_set1_sigalgs_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </summary>
    /// <param name="sslSigalgsList">
    ///     The client uses the TLS ClientHello signature_algorithms extension to indicate to the server which signature/hash algorithm pairs may be used in digital signatures. See manual page for `SSL_CTX_set1_sigalgs_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslSigalgsList(string? sslSigalgsList)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslSigalgsList(sslSigalgsList));
        return this;
    }

    /// <summary>
    ///     Path to client's private key (PEM) used for authentication.
    /// </summary>
    /// <param name="sslKeyLocation">
    ///     Path to client's private key (PEM) used for authentication.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslKeyLocation(string? sslKeyLocation)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslKeyLocation(sslKeyLocation));
        return this;
    }

    /// <summary>
    ///     Private key passphrase (for use with `ssl.key.location` and `set_ssl_cert()`)
    /// </summary>
    /// <param name="sslKeyPassword">
    ///     Private key passphrase (for use with `ssl.key.location` and `set_ssl_cert()`)
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslKeyPassword(string? sslKeyPassword)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslKeyPassword(sslKeyPassword));
        return this;
    }

    /// <summary>
    ///     Client's private key string (PEM format) used for authentication.
    /// </summary>
    /// <param name="sslKeyPem">
    ///     Client's private key string (PEM format) used for authentication.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslKeyPem(string? sslKeyPem)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslKeyPem(sslKeyPem));
        return this;
    }

    /// <summary>
    ///     Path to client's public key (PEM) used for authentication.
    /// </summary>
    /// <param name="sslCertificateLocation">
    ///     Path to client's public key (PEM) used for authentication.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslCertificateLocation(string? sslCertificateLocation)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslCertificateLocation(sslCertificateLocation));
        return this;
    }

    /// <summary>
    ///     Client's public key string (PEM format) used for authentication.
    /// </summary>
    /// <param name="sslCertificatePem">
    ///     Client's public key string (PEM format) used for authentication.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslCertificatePem(string? sslCertificatePem)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslCertificatePem(sslCertificatePem));
        return this;
    }

    /// <summary>
    ///     File or directory path to CA certificate(s) for verifying the broker's key. Defaults: On Windows the system's CA certificates are automatically looked up in the Windows Root certificate store. On Mac OSX this configuration defaults to `probe`. It is recommended to install openssl using Homebrew, to provide CA certificates. On Linux install the distribution's ca-certificates package. If OpenSSL is statically linked or `ssl.ca.location` is set to `probe` a list of standard paths will be probed and the first one found will be used as the default CA certificate location path. If OpenSSL is dynamically linked the OpenSSL library's default path will be used (see `OPENSSLDIR` in `openssl version -a`).
    /// </summary>
    /// <param name="sslCaLocation">
    ///     File or directory path to CA certificate(s) for verifying the broker's key. Defaults: On Windows the system's CA certificates are automatically looked up in the Windows Root certificate store. On Mac OSX this configuration defaults to `probe`. It is recommended to install openssl using Homebrew, to provide CA certificates. On Linux install the distribution's ca-certificates package. If OpenSSL is statically linked or `ssl.ca.location` is set to `probe` a list of standard paths will be probed and the first one found will be used as the default CA certificate location path. If OpenSSL is dynamically linked the OpenSSL library's default path will be used (see `OPENSSLDIR` in `openssl version -a`).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslCaLocation(string? sslCaLocation)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslCaLocation(sslCaLocation));
        return this;
    }

    /// <summary>
    ///     CA certificate string (PEM format) for verifying the broker's key.
    /// </summary>
    /// <param name="sslCaPem">
    ///     CA certificate string (PEM format) for verifying the broker's key.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslCaPem(string? sslCaPem)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslCaPem(sslCaPem));
        return this;
    }

    /// <summary>
    ///     Comma-separated list of Windows Certificate stores to load CA certificates from. Certificates will be loaded in the same order as stores are specified. If no certificates can be loaded from any of the specified stores an error is logged and the OpenSSL library's default CA location is used instead. Store names are typically one or more of: MY, Root, Trust, CA.
    /// </summary>
    /// <param name="sslCaCertificateStores">
    ///     Comma-separated list of Windows Certificate stores to load CA certificates from. Certificates will be loaded in the same order as stores are specified. If no certificates can be loaded from any of the specified stores an error is logged and the OpenSSL library's default CA location is used instead. Store names are typically one or more of: MY, Root, Trust, CA.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslCaCertificateStores(string? sslCaCertificateStores)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslCaCertificateStores(sslCaCertificateStores));
        return this;
    }

    /// <summary>
    ///     Path to CRL for verifying broker's certificate validity.
    /// </summary>
    /// <param name="sslCrlLocation">
    ///     Path to CRL for verifying broker's certificate validity.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslCrlLocation(string? sslCrlLocation)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslCrlLocation(sslCrlLocation));
        return this;
    }

    /// <summary>
    ///     Path to client's keystore (PKCS#12) used for authentication.
    /// </summary>
    /// <param name="sslKeystoreLocation">
    ///     Path to client's keystore (PKCS#12) used for authentication.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslKeystoreLocation(string? sslKeystoreLocation)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslKeystoreLocation(sslKeystoreLocation));
        return this;
    }

    /// <summary>
    ///     Client's keystore (PKCS#12) password.
    /// </summary>
    /// <param name="sslKeystorePassword">
    ///     Client's keystore (PKCS#12) password.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslKeystorePassword(string? sslKeystorePassword)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslKeystorePassword(sslKeystorePassword));
        return this;
    }

    /// <summary>
    ///     Path to OpenSSL engine library. OpenSSL &gt;= 1.1.0 required.
    /// </summary>
    /// <param name="sslEngineLocation">
    ///     Path to OpenSSL engine library. OpenSSL &gt;= 1.1.0 required.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslEngineLocation(string? sslEngineLocation)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslEngineLocation(sslEngineLocation));
        return this;
    }

    /// <summary>
    ///     OpenSSL engine id is the name used for loading engine.
    /// </summary>
    /// <param name="sslEngineId">
    ///     OpenSSL engine id is the name used for loading engine.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslEngineId(string? sslEngineId)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslEngineId(sslEngineId));
        return this;
    }

    /// <summary>
    ///     Enable OpenSSL's builtin broker (server) certificate verification. This verification can be extended by the application by implementing a certificate_verify_cb.
    /// </summary>
    /// <param name="enableSslCertificateVerification">
    ///     Enable OpenSSL's builtin broker (server) certificate verification. This verification can be extended by the application by implementing a certificate_verify_cb.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithEnableSslCertificateVerification(bool? enableSslCertificateVerification)
    {
        _sharedConfigurationActions.Add(builder => builder.WithEnableSslCertificateVerification(enableSslCertificateVerification));
        return this;
    }

    /// <summary>
    ///     Endpoint identification algorithm to validate broker hostname using broker certificate. https - Server (broker) hostname verification as specified in RFC2818. none - No endpoint verification. OpenSSL &gt;= 1.0.2 required.
    /// </summary>
    /// <param name="sslEndpointIdentificationAlgorithm">
    ///     Endpoint identification algorithm to validate broker hostname using broker certificate. https - Server (broker) hostname verification as specified in RFC2818. none - No endpoint verification. OpenSSL &gt;= 1.0.2 required.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSslEndpointIdentificationAlgorithm(SslEndpointIdentificationAlgorithm? sslEndpointIdentificationAlgorithm)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSslEndpointIdentificationAlgorithm(sslEndpointIdentificationAlgorithm));
        return this;
    }

    /// <summary>
    ///     Kerberos principal name that Kafka runs as, not including /hostname@REALM
    /// </summary>
    /// <param name="saslKerberosServiceName">
    ///     Kerberos principal name that Kafka runs as, not including /hostname@REALM
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSaslKerberosServiceName(string? saslKerberosServiceName)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSaslKerberosServiceName(saslKerberosServiceName));
        return this;
    }

    /// <summary>
    ///     This client's Kerberos principal name. (Not supported on Windows, will use the logon user's principal).
    /// </summary>
    /// <param name="saslKerberosPrincipal">
    ///     This client's Kerberos principal name. (Not supported on Windows, will use the logon user's principal).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSaslKerberosPrincipal(string? saslKerberosPrincipal)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSaslKerberosPrincipal(saslKerberosPrincipal));
        return this;
    }

    /// <summary>
    ///     Shell command to refresh or acquire the client's Kerberos ticket. This command is executed on client creation and every sasl.kerberos.min.time.before.relogin (0=disable). %{config.prop.name} is replaced by corresponding config object value.
    /// </summary>
    /// <param name="saslKerberosKinitCmd">
    ///     Shell command to refresh or acquire the client's Kerberos ticket. This command is executed on client creation and every sasl.kerberos.min.time.before.relogin (0=disable). %{config.prop.name} is replaced by corresponding config object value.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSaslKerberosKinitCmd(string? saslKerberosKinitCmd)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSaslKerberosKinitCmd(saslKerberosKinitCmd));
        return this;
    }

    /// <summary>
    ///     Path to Kerberos keytab file. This configuration property is only used as a variable in `sasl.kerberos.kinit.cmd` as ` ... -t "%{sasl.kerberos.keytab}"`.
    /// </summary>
    /// <param name="saslKerberosKeytab">
    ///     Path to Kerberos keytab file. This configuration property is only used as a variable in `sasl.kerberos.kinit.cmd` as ` ... -t "%{sasl.kerberos.keytab}"`.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSaslKerberosKeytab(string? saslKerberosKeytab)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSaslKerberosKeytab(saslKerberosKeytab));
        return this;
    }

    /// <summary>
    ///     Minimum time in milliseconds between key refresh attempts. Disable automatic key refresh by setting this property to 0.
    /// </summary>
    /// <param name="saslKerberosMinTimeBeforeRelogin">
    ///     Minimum time in milliseconds between key refresh attempts. Disable automatic key refresh by setting this property to 0.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSaslKerberosMinTimeBeforeRelogin(int? saslKerberosMinTimeBeforeRelogin)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSaslKerberosMinTimeBeforeRelogin(saslKerberosMinTimeBeforeRelogin));
        return this;
    }

    /// <summary>
    ///     SASL username for use with the PLAIN and SASL-SCRAM-.. mechanisms
    /// </summary>
    /// <param name="saslUsername">
    ///     SASL username for use with the PLAIN and SASL-SCRAM-.. mechanisms
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSaslUsername(string? saslUsername)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSaslUsername(saslUsername));
        return this;
    }

    /// <summary>
    ///     SASL password for use with the PLAIN and SASL-SCRAM-.. mechanism
    /// </summary>
    /// <param name="saslPassword">
    ///     SASL password for use with the PLAIN and SASL-SCRAM-.. mechanism
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSaslPassword(string? saslPassword)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSaslPassword(saslPassword));
        return this;
    }

    /// <summary>
    ///     SASL/OAUTHBEARER configuration. The format is implementation-dependent and must be parsed accordingly. The default unsecured token implementation (see https://tools.ietf.org/html/rfc7515#appendix-A.5) recognizes space-separated name=value pairs with valid names including principalClaimName, principal, scopeClaimName, scope, and lifeSeconds. The default value for principalClaimName is "sub", the default value for scopeClaimName is "scope", and the default value for lifeSeconds is 3600. The scope value is CSV format with the default value being no/empty scope. For example: `principalClaimName=azp principal=admin scopeClaimName=roles scope=role1,role2 lifeSeconds=600`. In addition, SASL extensions can be communicated to the broker via `extension_NAME=value`. For example: `principal=admin extension_traceId=123`
    /// </summary>
    /// <param name="saslOauthbearerConfig">
    ///     SASL/OAUTHBEARER configuration. The format is implementation-dependent and must be parsed accordingly. The default unsecured token implementation (see https://tools.ietf.org/html/rfc7515#appendix-A.5) recognizes space-separated name=value pairs with valid names including principalClaimName, principal, scopeClaimName, scope, and lifeSeconds. The default value for principalClaimName is "sub", the default value for scopeClaimName is "scope", and the default value for lifeSeconds is 3600. The scope value is CSV format with the default value being no/empty scope. For example: `principalClaimName=azp principal=admin scopeClaimName=roles scope=role1,role2 lifeSeconds=600`. In addition, SASL extensions can be communicated to the broker via `extension_NAME=value`. For example: `principal=admin extension_traceId=123`
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithSaslOauthbearerConfig(string? saslOauthbearerConfig)
    {
        _sharedConfigurationActions.Add(builder => builder.WithSaslOauthbearerConfig(saslOauthbearerConfig));
        return this;
    }

    /// <summary>
    ///     Enable the builtin unsecure JWT OAUTHBEARER token handler if no oauthbearer_refresh_cb has been set. This builtin handler should only be used for development or testing, and not in production.
    /// </summary>
    /// <param name="enableSaslOauthbearerUnsecureJwt">
    ///     Enable the builtin unsecure JWT OAUTHBEARER token handler if no oauthbearer_refresh_cb has been set. This builtin handler should only be used for development or testing, and not in production.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithEnableSaslOauthbearerUnsecureJwt(bool? enableSaslOauthbearerUnsecureJwt)
    {
        _sharedConfigurationActions.Add(builder => builder.WithEnableSaslOauthbearerUnsecureJwt(enableSaslOauthbearerUnsecureJwt));
        return this;
    }

    /// <summary>
    ///     List of plugin libraries to load (; separated). The library search path is platform dependent (see dlopen(3) for Unix and LoadLibrary() for Windows). If no filename extension is specified the platform-specific extension (such as .dll or .so) will be appended automatically.
    /// </summary>
    /// <param name="pluginLibraryPaths">
    ///     List of plugin libraries to load (; separated). The library search path is platform dependent (see dlopen(3) for Unix and LoadLibrary() for Windows). If no filename extension is specified the platform-specific extension (such as .dll or .so) will be appended automatically.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithPluginLibraryPaths(string? pluginLibraryPaths)
    {
        _sharedConfigurationActions.Add(builder => builder.WithPluginLibraryPaths(pluginLibraryPaths));
        return this;
    }

    /// <summary>
    ///     A rack identifier for this client. This can be any string value which indicates where this client is physically located. It corresponds with the broker config `broker.rack`.
    /// </summary>
    /// <param name="clientRack">
    ///     A rack identifier for this client. This can be any string value which indicates where this client is physically located. It corresponds with the broker config `broker.rack`.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithClientRack(string? clientRack)
    {
        _sharedConfigurationActions.Add(builder => builder.WithClientRack(clientRack));
        return this;
    }

    /// <summary>
    ///     The maximum length of time (in milliseconds) before a cancellation request
    ///     is acted on. Low values may result in measurably higher CPU usage.
    ///     range: 1 &lt;= dotnet.cancellation.delay.max.ms &lt;= 10000
    /// </summary>
    /// <param name="cancellationDelayMaxMs">
    ///     The maximum length of time (in milliseconds) before a cancellation request
    ///     is acted on. Low values may result in measurably higher CPU usage.
    ///     range: 1 &lt;= dotnet.cancellation.delay.max.ms &lt;= 10000
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaClientsConfigurationBuilder WithCancellationDelayMaxMs(int cancellationDelayMaxMs)
    {
        _sharedConfigurationActions.Add(builder => builder.WithCancellationDelayMaxMs(cancellationDelayMaxMs));
        return this;
    }
}

/// <content>
///     The autogenerated part of the <see cref="KafkaClientConfigurationBuilder{TClientConfig,TBuilder}" /> class.
/// </content>
[SuppressMessage("", "SA1649", Justification = "Autogenerated all at once")]
[SuppressMessage("", "SA1402", Justification = "Autogenerated all at once")]
[SuppressMessage("", "CA1200", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1623", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1629", Justification = "Summary copied from wrapped class")]
[SuppressMessage("StyleCop.CSharp.DocumentationRules", "SA1625:Element documentation should not be copied and pasted", Justification = "Summary copied from wrapped class")]
public partial class KafkaClientConfigurationBuilder<TClientConfig, TBuilder> : IKafkaClientConfigurationBuilder
{
    /// <summary>
    ///     SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512. **NOTE**: Despite the name, you may not configure more than one mechanism.
    /// </summary>
    /// <param name="saslMechanism">
    ///     SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512. **NOTE**: Despite the name, you may not configure more than one mechanism.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSaslMechanism(SaslMechanism? saslMechanism)
    {
        ClientConfig.SaslMechanism = saslMechanism;
        return This;
    }

    /// <summary>
    ///     This field indicates the number of acknowledgements the leader broker must receive from ISR brokers
    ///     before responding to the request: Zero=Broker does not send any response/ack to client, One=The
    ///     leader will write the record to its local log but will respond without awaiting full acknowledgement
    ///     from all followers. All=Broker will block until message is committed by all in sync replicas (ISRs).
    ///     If there are less than min.insync.replicas (broker configuration) in the ISR set the produce request
    ///     will fail.
    /// </summary>
    /// <param name="acks">
    ///     This field indicates the number of acknowledgements the leader broker must receive from ISR brokers
    ///     before responding to the request: Zero=Broker does not send any response/ack to client, One=The
    ///     leader will write the record to its local log but will respond without awaiting full acknowledgement
    ///     from all followers. All=Broker will block until message is committed by all in sync replicas (ISRs).
    ///     If there are less than min.insync.replicas (broker configuration) in the ISR set the produce request
    ///     will fail.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithAcks(Acks? acks)
    {
        ClientConfig.Acks = acks;
        return This;
    }

    /// <summary>
    ///     Client identifier.
    /// </summary>
    /// <param name="clientId">
    ///     Client identifier.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithClientId(string? clientId)
    {
        ClientConfig.ClientId = clientId;
        return This;
    }

    /// <summary>
    ///     Initial list of brokers as a CSV list of broker host or host:port. The application may also use `rd_kafka_brokers_add()` to add brokers during runtime.
    /// </summary>
    /// <param name="bootstrapServers">
    ///     Initial list of brokers as a CSV list of broker host or host:port. The application may also use `rd_kafka_brokers_add()` to add brokers during runtime.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithBootstrapServers(string? bootstrapServers)
    {
        ClientConfig.BootstrapServers = bootstrapServers;
        return This;
    }

    /// <summary>
    ///     Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic's `max.message.bytes` limit (see Apache Kafka documentation).
    /// </summary>
    /// <param name="messageMaxBytes">
    ///     Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic's `max.message.bytes` limit (see Apache Kafka documentation).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithMessageMaxBytes(int? messageMaxBytes)
    {
        ClientConfig.MessageMaxBytes = messageMaxBytes;
        return This;
    }

    /// <summary>
    ///     Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference (zero-copy) at the expense of larger iovecs.
    /// </summary>
    /// <param name="messageCopyMaxBytes">
    ///     Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference (zero-copy) at the expense of larger iovecs.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithMessageCopyMaxBytes(int? messageCopyMaxBytes)
    {
        ClientConfig.MessageCopyMaxBytes = messageCopyMaxBytes;
        return This;
    }

    /// <summary>
    ///     Maximum Kafka protocol response message size. This serves as a safety precaution to avoid memory exhaustion in case of protocol hickups. This value must be at least `fetch.max.bytes`  + 512 to allow for protocol overhead; the value is adjusted automatically unless the configuration property is explicitly set.
    /// </summary>
    /// <param name="receiveMessageMaxBytes">
    ///     Maximum Kafka protocol response message size. This serves as a safety precaution to avoid memory exhaustion in case of protocol hickups. This value must be at least `fetch.max.bytes`  + 512 to allow for protocol overhead; the value is adjusted automatically unless the configuration property is explicitly set.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithReceiveMessageMaxBytes(int? receiveMessageMaxBytes)
    {
        ClientConfig.ReceiveMessageMaxBytes = receiveMessageMaxBytes;
        return This;
    }

    /// <summary>
    ///     Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one.
    /// </summary>
    /// <param name="maxInFlight">
    ///     Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithMaxInFlight(int? maxInFlight)
    {
        ClientConfig.MaxInFlight = maxInFlight;
        return This;
    }

    /// <summary>
    ///     Period of time in milliseconds at which topic and broker metadata is refreshed in order to proactively discover any new brokers, topics, partitions or partition leader changes. Use -1 to disable the intervalled refresh (not recommended). If there are no locally referenced topics (no topic objects created, no messages produced, no subscription or no assignment) then only the broker list will be refreshed every interval but no more often than every 10s.
    /// </summary>
    /// <param name="topicMetadataRefreshIntervalMs">
    ///     Period of time in milliseconds at which topic and broker metadata is refreshed in order to proactively discover any new brokers, topics, partitions or partition leader changes. Use -1 to disable the intervalled refresh (not recommended). If there are no locally referenced topics (no topic objects created, no messages produced, no subscription or no assignment) then only the broker list will be refreshed every interval but no more often than every 10s.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithTopicMetadataRefreshIntervalMs(int? topicMetadataRefreshIntervalMs)
    {
        ClientConfig.TopicMetadataRefreshIntervalMs = topicMetadataRefreshIntervalMs;
        return This;
    }

    /// <summary>
    ///     Metadata cache max age. Defaults to topic.metadata.refresh.interval.ms * 3
    /// </summary>
    /// <param name="metadataMaxAgeMs">
    ///     Metadata cache max age. Defaults to topic.metadata.refresh.interval.ms * 3
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithMetadataMaxAgeMs(int? metadataMaxAgeMs)
    {
        ClientConfig.MetadataMaxAgeMs = metadataMaxAgeMs;
        return This;
    }

    /// <summary>
    ///     When a topic loses its leader a new metadata request will be enqueued with this initial interval, exponentially increasing until the topic metadata has been refreshed. This is used to recover quickly from transitioning leader brokers.
    /// </summary>
    /// <param name="topicMetadataRefreshFastIntervalMs">
    ///     When a topic loses its leader a new metadata request will be enqueued with this initial interval, exponentially increasing until the topic metadata has been refreshed. This is used to recover quickly from transitioning leader brokers.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithTopicMetadataRefreshFastIntervalMs(int? topicMetadataRefreshFastIntervalMs)
    {
        ClientConfig.TopicMetadataRefreshFastIntervalMs = topicMetadataRefreshFastIntervalMs;
        return This;
    }

    /// <summary>
    ///     Sparse metadata requests (consumes less network bandwidth)
    /// </summary>
    /// <param name="topicMetadataRefreshSparse">
    ///     Sparse metadata requests (consumes less network bandwidth)
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithTopicMetadataRefreshSparse(bool? topicMetadataRefreshSparse)
    {
        ClientConfig.TopicMetadataRefreshSparse = topicMetadataRefreshSparse;
        return This;
    }

    /// <summary>
    ///     Apache Kafka topic creation is asynchronous and it takes some time for a new topic to propagate throughout the cluster to all brokers. If a client requests topic metadata after manual topic creation but before the topic has been fully propagated to the broker the client is requesting metadata from, the topic will seem to be non-existent and the client will mark the topic as such, failing queued produced messages with `ERR__UNKNOWN_TOPIC`. This setting delays marking a topic as non-existent until the configured propagation max time has passed. The maximum propagation time is calculated from the time the topic is first referenced in the client, e.g., on produce().
    /// </summary>
    /// <param name="topicMetadataPropagationMaxMs">
    ///     Apache Kafka topic creation is asynchronous and it takes some time for a new topic to propagate throughout the cluster to all brokers. If a client requests topic metadata after manual topic creation but before the topic has been fully propagated to the broker the client is requesting metadata from, the topic will seem to be non-existent and the client will mark the topic as such, failing queued produced messages with `ERR__UNKNOWN_TOPIC`. This setting delays marking a topic as non-existent until the configured propagation max time has passed. The maximum propagation time is calculated from the time the topic is first referenced in the client, e.g., on produce().
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithTopicMetadataPropagationMaxMs(int? topicMetadataPropagationMaxMs)
    {
        ClientConfig.TopicMetadataPropagationMaxMs = topicMetadataPropagationMaxMs;
        return This;
    }

    /// <summary>
    ///     Topic blacklist, a comma-separated list of regular expressions for matching topic names that should be ignored in broker metadata information as if the topics did not exist.
    /// </summary>
    /// <param name="topicBlacklist">
    ///     Topic blacklist, a comma-separated list of regular expressions for matching topic names that should be ignored in broker metadata information as if the topics did not exist.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithTopicBlacklist(string? topicBlacklist)
    {
        ClientConfig.TopicBlacklist = topicBlacklist;
        return This;
    }

    /// <summary>
    ///     A comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg. Consumer: consumer,cgrp,topic,fetch
    /// </summary>
    /// <param name="debug">
    ///     A comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg. Consumer: consumer,cgrp,topic,fetch
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithDebug(string? debug)
    {
        ClientConfig.Debug = debug;
        return This;
    }

    /// <summary>
    ///     Default timeout for network requests. Producer: ProduceRequests will use the lesser value of `socket.timeout.ms` and remaining `message.timeout.ms` for the first message in the batch. Consumer: FetchRequests will use `fetch.wait.max.ms` + `socket.timeout.ms`. Admin: Admin requests will use `socket.timeout.ms` or explicitly set `rd_kafka_AdminOptions_set_operation_timeout()` value.
    /// </summary>
    /// <param name="socketTimeoutMs">
    ///     Default timeout for network requests. Producer: ProduceRequests will use the lesser value of `socket.timeout.ms` and remaining `message.timeout.ms` for the first message in the batch. Consumer: FetchRequests will use `fetch.wait.max.ms` + `socket.timeout.ms`. Admin: Admin requests will use `socket.timeout.ms` or explicitly set `rd_kafka_AdminOptions_set_operation_timeout()` value.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSocketTimeoutMs(int? socketTimeoutMs)
    {
        ClientConfig.SocketTimeoutMs = socketTimeoutMs;
        return This;
    }

    /// <summary>
    ///     Broker socket send buffer size. System default is used if 0.
    /// </summary>
    /// <param name="socketSendBufferBytes">
    ///     Broker socket send buffer size. System default is used if 0.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSocketSendBufferBytes(int? socketSendBufferBytes)
    {
        ClientConfig.SocketSendBufferBytes = socketSendBufferBytes;
        return This;
    }

    /// <summary>
    ///     Broker socket receive buffer size. System default is used if 0.
    /// </summary>
    /// <param name="socketReceiveBufferBytes">
    ///     Broker socket receive buffer size. System default is used if 0.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSocketReceiveBufferBytes(int? socketReceiveBufferBytes)
    {
        ClientConfig.SocketReceiveBufferBytes = socketReceiveBufferBytes;
        return This;
    }

    /// <summary>
    ///     Enable TCP keep-alives (SO_KEEPALIVE) on broker sockets
    /// </summary>
    /// <param name="socketKeepaliveEnable">
    ///     Enable TCP keep-alives (SO_KEEPALIVE) on broker sockets
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSocketKeepaliveEnable(bool? socketKeepaliveEnable)
    {
        ClientConfig.SocketKeepaliveEnable = socketKeepaliveEnable;
        return This;
    }

    /// <summary>
    ///     Disable the Nagle algorithm (TCP_NODELAY) on broker sockets.
    /// </summary>
    /// <param name="socketNagleDisable">
    ///     Disable the Nagle algorithm (TCP_NODELAY) on broker sockets.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSocketNagleDisable(bool? socketNagleDisable)
    {
        ClientConfig.SocketNagleDisable = socketNagleDisable;
        return This;
    }

    /// <summary>
    ///     Disconnect from broker when this number of send failures (e.g., timed out requests) is reached. Disable with 0. WARNING: It is highly recommended to leave this setting at its default value of 1 to avoid the client and broker to become desynchronized in case of request timeouts. NOTE: The connection is automatically re-established.
    /// </summary>
    /// <param name="socketMaxFails">
    ///     Disconnect from broker when this number of send failures (e.g., timed out requests) is reached. Disable with 0. WARNING: It is highly recommended to leave this setting at its default value of 1 to avoid the client and broker to become desynchronized in case of request timeouts. NOTE: The connection is automatically re-established.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSocketMaxFails(int? socketMaxFails)
    {
        ClientConfig.SocketMaxFails = socketMaxFails;
        return This;
    }

    /// <summary>
    ///     How long to cache the broker address resolving results (milliseconds).
    /// </summary>
    /// <param name="brokerAddressTtl">
    ///     How long to cache the broker address resolving results (milliseconds).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithBrokerAddressTtl(int? brokerAddressTtl)
    {
        ClientConfig.BrokerAddressTtl = brokerAddressTtl;
        return This;
    }

    /// <summary>
    ///     Allowed broker IP address families: any, v4, v6
    /// </summary>
    /// <param name="brokerAddressFamily">
    ///     Allowed broker IP address families: any, v4, v6
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithBrokerAddressFamily(BrokerAddressFamily? brokerAddressFamily)
    {
        ClientConfig.BrokerAddressFamily = brokerAddressFamily;
        return This;
    }

    /// <summary>
    ///     Close broker connections after the specified time of inactivity. Disable with 0. If this property is left at its default value some heuristics are performed to determine a suitable default value, this is currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info).
    /// </summary>
    /// <param name="connectionsMaxIdleMs">
    ///     Close broker connections after the specified time of inactivity. Disable with 0. If this property is left at its default value some heuristics are performed to determine a suitable default value, this is currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithConnectionsMaxIdleMs(int? connectionsMaxIdleMs)
    {
        ClientConfig.ConnectionsMaxIdleMs = connectionsMaxIdleMs;
        return This;
    }

    /// <summary>
    ///     The initial time to wait before reconnecting to a broker after the connection has been closed. The time is increased exponentially until `reconnect.backoff.max.ms` is reached. -25% to +50% jitter is applied to each reconnect backoff. A value of 0 disables the backoff and reconnects immediately.
    /// </summary>
    /// <param name="reconnectBackoffMs">
    ///     The initial time to wait before reconnecting to a broker after the connection has been closed. The time is increased exponentially until `reconnect.backoff.max.ms` is reached. -25% to +50% jitter is applied to each reconnect backoff. A value of 0 disables the backoff and reconnects immediately.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithReconnectBackoffMs(int? reconnectBackoffMs)
    {
        ClientConfig.ReconnectBackoffMs = reconnectBackoffMs;
        return This;
    }

    /// <summary>
    ///     The maximum time to wait before reconnecting to a broker after the connection has been closed.
    /// </summary>
    /// <param name="reconnectBackoffMaxMs">
    ///     The maximum time to wait before reconnecting to a broker after the connection has been closed.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithReconnectBackoffMaxMs(int? reconnectBackoffMaxMs)
    {
        ClientConfig.ReconnectBackoffMaxMs = reconnectBackoffMaxMs;
        return This;
    }

    /// <summary>
    ///     librdkafka statistics emit interval. The application also needs to register a stats callback using `rd_kafka_conf_set_stats_cb()`. The granularity is 1000ms. A value of 0 disables statistics.
    /// </summary>
    /// <param name="statisticsIntervalMs">
    ///     librdkafka statistics emit interval. The application also needs to register a stats callback using `rd_kafka_conf_set_stats_cb()`. The granularity is 1000ms. A value of 0 disables statistics.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithStatisticsIntervalMs(int? statisticsIntervalMs)
    {
        ClientConfig.StatisticsIntervalMs = statisticsIntervalMs;
        return This;
    }

    /// <summary>
    ///     Disable spontaneous log_cb from internal librdkafka threads, instead enqueue log messages on queue set with `rd_kafka_set_log_queue()` and serve log callbacks or events through the standard poll APIs. **NOTE**: Log messages will linger in a temporary queue until the log queue has been set.
    /// </summary>
    /// <param name="logQueue">
    ///     Disable spontaneous log_cb from internal librdkafka threads, instead enqueue log messages on queue set with `rd_kafka_set_log_queue()` and serve log callbacks or events through the standard poll APIs. **NOTE**: Log messages will linger in a temporary queue until the log queue has been set.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithLogQueue(bool? logQueue)
    {
        ClientConfig.LogQueue = logQueue;
        return This;
    }

    /// <summary>
    ///     Print internal thread name in log messages (useful for debugging librdkafka internals)
    /// </summary>
    /// <param name="logThreadName">
    ///     Print internal thread name in log messages (useful for debugging librdkafka internals)
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithLogThreadName(bool? logThreadName)
    {
        ClientConfig.LogThreadName = logThreadName;
        return This;
    }

    /// <summary>
    ///     If enabled librdkafka will initialize the PRNG with srand(current_time.milliseconds) on the first invocation of rd_kafka_new() (required only if rand_r() is not available on your platform). If disabled the application must call srand() prior to calling rd_kafka_new().
    /// </summary>
    /// <param name="enableRandomSeed">
    ///     If enabled librdkafka will initialize the PRNG with srand(current_time.milliseconds) on the first invocation of rd_kafka_new() (required only if rand_r() is not available on your platform). If disabled the application must call srand() prior to calling rd_kafka_new().
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithEnableRandomSeed(bool? enableRandomSeed)
    {
        ClientConfig.EnableRandomSeed = enableRandomSeed;
        return This;
    }

    /// <summary>
    ///     Log broker disconnects. It might be useful to turn this off when interacting with 0.9 brokers with an aggressive `connection.max.idle.ms` value.
    /// </summary>
    /// <param name="logConnectionClose">
    ///     Log broker disconnects. It might be useful to turn this off when interacting with 0.9 brokers with an aggressive `connection.max.idle.ms` value.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithLogConnectionClose(bool? logConnectionClose)
    {
        ClientConfig.LogConnectionClose = logConnectionClose;
        return This;
    }

    /// <summary>
    ///     Signal that librdkafka will use to quickly terminate on rd_kafka_destroy(). If this signal is not set then there will be a delay before rd_kafka_wait_destroyed() returns true as internal threads are timing out their system calls. If this signal is set however the delay will be minimal. The application should mask this signal as an internal signal handler is installed.
    /// </summary>
    /// <param name="internalTerminationSignal">
    ///     Signal that librdkafka will use to quickly terminate on rd_kafka_destroy(). If this signal is not set then there will be a delay before rd_kafka_wait_destroyed() returns true as internal threads are timing out their system calls. If this signal is set however the delay will be minimal. The application should mask this signal as an internal signal handler is installed.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithInternalTerminationSignal(int? internalTerminationSignal)
    {
        ClientConfig.InternalTerminationSignal = internalTerminationSignal;
        return This;
    }

    /// <summary>
    ///     Request broker's supported API versions to adjust functionality to available protocol features. If set to false, or the ApiVersionRequest fails, the fallback version `broker.version.fallback` will be used. **NOTE**: Depends on broker version &gt;=0.10.0. If the request is not supported by (an older) broker the `broker.version.fallback` fallback is used.
    /// </summary>
    /// <param name="apiVersionRequest">
    ///     Request broker's supported API versions to adjust functionality to available protocol features. If set to false, or the ApiVersionRequest fails, the fallback version `broker.version.fallback` will be used. **NOTE**: Depends on broker version &gt;=0.10.0. If the request is not supported by (an older) broker the `broker.version.fallback` fallback is used.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithApiVersionRequest(bool? apiVersionRequest)
    {
        ClientConfig.ApiVersionRequest = apiVersionRequest;
        return This;
    }

    /// <summary>
    ///     Timeout for broker API version requests.
    /// </summary>
    /// <param name="apiVersionRequestTimeoutMs">
    ///     Timeout for broker API version requests.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithApiVersionRequestTimeoutMs(int? apiVersionRequestTimeoutMs)
    {
        ClientConfig.ApiVersionRequestTimeoutMs = apiVersionRequestTimeoutMs;
        return This;
    }

    /// <summary>
    ///     Dictates how long the `broker.version.fallback` fallback is used in the case the ApiVersionRequest fails. **NOTE**: The ApiVersionRequest is only issued when a new connection to the broker is made (such as after an upgrade).
    /// </summary>
    /// <param name="apiVersionFallbackMs">
    ///     Dictates how long the `broker.version.fallback` fallback is used in the case the ApiVersionRequest fails. **NOTE**: The ApiVersionRequest is only issued when a new connection to the broker is made (such as after an upgrade).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithApiVersionFallbackMs(int? apiVersionFallbackMs)
    {
        ClientConfig.ApiVersionFallbackMs = apiVersionFallbackMs;
        return This;
    }

    /// <summary>
    ///     Older broker versions (before 0.10.0) provide no way for a client to query for supported protocol features (ApiVersionRequest, see `api.version.request`) making it impossible for the client to know what features it may use. As a workaround a user may set this property to the expected broker version and the client will automatically adjust its feature set accordingly if the ApiVersionRequest fails (or is disabled). The fallback broker version will be used for `api.version.fallback.ms`. Valid values are: 0.9.0, 0.8.2, 0.8.1, 0.8.0. Any other value &gt;= 0.10, such as 0.10.2.1, enables ApiVersionRequests.
    /// </summary>
    /// <param name="brokerVersionFallback">
    ///     Older broker versions (before 0.10.0) provide no way for a client to query for supported protocol features (ApiVersionRequest, see `api.version.request`) making it impossible for the client to know what features it may use. As a workaround a user may set this property to the expected broker version and the client will automatically adjust its feature set accordingly if the ApiVersionRequest fails (or is disabled). The fallback broker version will be used for `api.version.fallback.ms`. Valid values are: 0.9.0, 0.8.2, 0.8.1, 0.8.0. Any other value &gt;= 0.10, such as 0.10.2.1, enables ApiVersionRequests.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithBrokerVersionFallback(string? brokerVersionFallback)
    {
        ClientConfig.BrokerVersionFallback = brokerVersionFallback;
        return This;
    }

    /// <summary>
    ///     Protocol used to communicate with brokers.
    /// </summary>
    /// <param name="securityProtocol">
    ///     Protocol used to communicate with brokers.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSecurityProtocol(SecurityProtocol? securityProtocol)
    {
        ClientConfig.SecurityProtocol = securityProtocol;
        return This;
    }

    /// <summary>
    ///     A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. See manual page for `ciphers(1)` and `SSL_CTX_set_cipher_list(3).
    /// </summary>
    /// <param name="sslCipherSuites">
    ///     A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. See manual page for `ciphers(1)` and `SSL_CTX_set_cipher_list(3).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslCipherSuites(string? sslCipherSuites)
    {
        ClientConfig.SslCipherSuites = sslCipherSuites;
        return This;
    }

    /// <summary>
    ///     The supported-curves extension in the TLS ClientHello message specifies the curves (standard/named, or 'explicit' GF(2^k) or GF(p)) the client is willing to have the server use. See manual page for `SSL_CTX_set1_curves_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </summary>
    /// <param name="sslCurvesList">
    ///     The supported-curves extension in the TLS ClientHello message specifies the curves (standard/named, or 'explicit' GF(2^k) or GF(p)) the client is willing to have the server use. See manual page for `SSL_CTX_set1_curves_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslCurvesList(string? sslCurvesList)
    {
        ClientConfig.SslCurvesList = sslCurvesList;
        return This;
    }

    /// <summary>
    ///     The client uses the TLS ClientHello signature_algorithms extension to indicate to the server which signature/hash algorithm pairs may be used in digital signatures. See manual page for `SSL_CTX_set1_sigalgs_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </summary>
    /// <param name="sslSigalgsList">
    ///     The client uses the TLS ClientHello signature_algorithms extension to indicate to the server which signature/hash algorithm pairs may be used in digital signatures. See manual page for `SSL_CTX_set1_sigalgs_list(3)`. OpenSSL &gt;= 1.0.2 required.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslSigalgsList(string? sslSigalgsList)
    {
        ClientConfig.SslSigalgsList = sslSigalgsList;
        return This;
    }

    /// <summary>
    ///     Path to client's private key (PEM) used for authentication.
    /// </summary>
    /// <param name="sslKeyLocation">
    ///     Path to client's private key (PEM) used for authentication.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslKeyLocation(string? sslKeyLocation)
    {
        ClientConfig.SslKeyLocation = sslKeyLocation;
        return This;
    }

    /// <summary>
    ///     Private key passphrase (for use with `ssl.key.location` and `set_ssl_cert()`)
    /// </summary>
    /// <param name="sslKeyPassword">
    ///     Private key passphrase (for use with `ssl.key.location` and `set_ssl_cert()`)
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslKeyPassword(string? sslKeyPassword)
    {
        ClientConfig.SslKeyPassword = sslKeyPassword;
        return This;
    }

    /// <summary>
    ///     Client's private key string (PEM format) used for authentication.
    /// </summary>
    /// <param name="sslKeyPem">
    ///     Client's private key string (PEM format) used for authentication.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslKeyPem(string? sslKeyPem)
    {
        ClientConfig.SslKeyPem = sslKeyPem;
        return This;
    }

    /// <summary>
    ///     Path to client's public key (PEM) used for authentication.
    /// </summary>
    /// <param name="sslCertificateLocation">
    ///     Path to client's public key (PEM) used for authentication.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslCertificateLocation(string? sslCertificateLocation)
    {
        ClientConfig.SslCertificateLocation = sslCertificateLocation;
        return This;
    }

    /// <summary>
    ///     Client's public key string (PEM format) used for authentication.
    /// </summary>
    /// <param name="sslCertificatePem">
    ///     Client's public key string (PEM format) used for authentication.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslCertificatePem(string? sslCertificatePem)
    {
        ClientConfig.SslCertificatePem = sslCertificatePem;
        return This;
    }

    /// <summary>
    ///     File or directory path to CA certificate(s) for verifying the broker's key. Defaults: On Windows the system's CA certificates are automatically looked up in the Windows Root certificate store. On Mac OSX this configuration defaults to `probe`. It is recommended to install openssl using Homebrew, to provide CA certificates. On Linux install the distribution's ca-certificates package. If OpenSSL is statically linked or `ssl.ca.location` is set to `probe` a list of standard paths will be probed and the first one found will be used as the default CA certificate location path. If OpenSSL is dynamically linked the OpenSSL library's default path will be used (see `OPENSSLDIR` in `openssl version -a`).
    /// </summary>
    /// <param name="sslCaLocation">
    ///     File or directory path to CA certificate(s) for verifying the broker's key. Defaults: On Windows the system's CA certificates are automatically looked up in the Windows Root certificate store. On Mac OSX this configuration defaults to `probe`. It is recommended to install openssl using Homebrew, to provide CA certificates. On Linux install the distribution's ca-certificates package. If OpenSSL is statically linked or `ssl.ca.location` is set to `probe` a list of standard paths will be probed and the first one found will be used as the default CA certificate location path. If OpenSSL is dynamically linked the OpenSSL library's default path will be used (see `OPENSSLDIR` in `openssl version -a`).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslCaLocation(string? sslCaLocation)
    {
        ClientConfig.SslCaLocation = sslCaLocation;
        return This;
    }

    /// <summary>
    ///     CA certificate string (PEM format) for verifying the broker's key.
    /// </summary>
    /// <param name="sslCaPem">
    ///     CA certificate string (PEM format) for verifying the broker's key.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslCaPem(string? sslCaPem)
    {
        ClientConfig.SslCaPem = sslCaPem;
        return This;
    }

    /// <summary>
    ///     Comma-separated list of Windows Certificate stores to load CA certificates from. Certificates will be loaded in the same order as stores are specified. If no certificates can be loaded from any of the specified stores an error is logged and the OpenSSL library's default CA location is used instead. Store names are typically one or more of: MY, Root, Trust, CA.
    /// </summary>
    /// <param name="sslCaCertificateStores">
    ///     Comma-separated list of Windows Certificate stores to load CA certificates from. Certificates will be loaded in the same order as stores are specified. If no certificates can be loaded from any of the specified stores an error is logged and the OpenSSL library's default CA location is used instead. Store names are typically one or more of: MY, Root, Trust, CA.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslCaCertificateStores(string? sslCaCertificateStores)
    {
        ClientConfig.SslCaCertificateStores = sslCaCertificateStores;
        return This;
    }

    /// <summary>
    ///     Path to CRL for verifying broker's certificate validity.
    /// </summary>
    /// <param name="sslCrlLocation">
    ///     Path to CRL for verifying broker's certificate validity.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslCrlLocation(string? sslCrlLocation)
    {
        ClientConfig.SslCrlLocation = sslCrlLocation;
        return This;
    }

    /// <summary>
    ///     Path to client's keystore (PKCS#12) used for authentication.
    /// </summary>
    /// <param name="sslKeystoreLocation">
    ///     Path to client's keystore (PKCS#12) used for authentication.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslKeystoreLocation(string? sslKeystoreLocation)
    {
        ClientConfig.SslKeystoreLocation = sslKeystoreLocation;
        return This;
    }

    /// <summary>
    ///     Client's keystore (PKCS#12) password.
    /// </summary>
    /// <param name="sslKeystorePassword">
    ///     Client's keystore (PKCS#12) password.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslKeystorePassword(string? sslKeystorePassword)
    {
        ClientConfig.SslKeystorePassword = sslKeystorePassword;
        return This;
    }

    /// <summary>
    ///     Path to OpenSSL engine library. OpenSSL &gt;= 1.1.0 required.
    /// </summary>
    /// <param name="sslEngineLocation">
    ///     Path to OpenSSL engine library. OpenSSL &gt;= 1.1.0 required.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslEngineLocation(string? sslEngineLocation)
    {
        ClientConfig.SslEngineLocation = sslEngineLocation;
        return This;
    }

    /// <summary>
    ///     OpenSSL engine id is the name used for loading engine.
    /// </summary>
    /// <param name="sslEngineId">
    ///     OpenSSL engine id is the name used for loading engine.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslEngineId(string? sslEngineId)
    {
        ClientConfig.SslEngineId = sslEngineId;
        return This;
    }

    /// <summary>
    ///     Enable OpenSSL's builtin broker (server) certificate verification. This verification can be extended by the application by implementing a certificate_verify_cb.
    /// </summary>
    /// <param name="enableSslCertificateVerification">
    ///     Enable OpenSSL's builtin broker (server) certificate verification. This verification can be extended by the application by implementing a certificate_verify_cb.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithEnableSslCertificateVerification(bool? enableSslCertificateVerification)
    {
        ClientConfig.EnableSslCertificateVerification = enableSslCertificateVerification;
        return This;
    }

    /// <summary>
    ///     Endpoint identification algorithm to validate broker hostname using broker certificate. https - Server (broker) hostname verification as specified in RFC2818. none - No endpoint verification. OpenSSL &gt;= 1.0.2 required.
    /// </summary>
    /// <param name="sslEndpointIdentificationAlgorithm">
    ///     Endpoint identification algorithm to validate broker hostname using broker certificate. https - Server (broker) hostname verification as specified in RFC2818. none - No endpoint verification. OpenSSL &gt;= 1.0.2 required.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSslEndpointIdentificationAlgorithm(SslEndpointIdentificationAlgorithm? sslEndpointIdentificationAlgorithm)
    {
        ClientConfig.SslEndpointIdentificationAlgorithm = sslEndpointIdentificationAlgorithm;
        return This;
    }

    /// <summary>
    ///     Kerberos principal name that Kafka runs as, not including /hostname@REALM
    /// </summary>
    /// <param name="saslKerberosServiceName">
    ///     Kerberos principal name that Kafka runs as, not including /hostname@REALM
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSaslKerberosServiceName(string? saslKerberosServiceName)
    {
        ClientConfig.SaslKerberosServiceName = saslKerberosServiceName;
        return This;
    }

    /// <summary>
    ///     This client's Kerberos principal name. (Not supported on Windows, will use the logon user's principal).
    /// </summary>
    /// <param name="saslKerberosPrincipal">
    ///     This client's Kerberos principal name. (Not supported on Windows, will use the logon user's principal).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSaslKerberosPrincipal(string? saslKerberosPrincipal)
    {
        ClientConfig.SaslKerberosPrincipal = saslKerberosPrincipal;
        return This;
    }

    /// <summary>
    ///     Shell command to refresh or acquire the client's Kerberos ticket. This command is executed on client creation and every sasl.kerberos.min.time.before.relogin (0=disable). %{config.prop.name} is replaced by corresponding config object value.
    /// </summary>
    /// <param name="saslKerberosKinitCmd">
    ///     Shell command to refresh or acquire the client's Kerberos ticket. This command is executed on client creation and every sasl.kerberos.min.time.before.relogin (0=disable). %{config.prop.name} is replaced by corresponding config object value.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSaslKerberosKinitCmd(string? saslKerberosKinitCmd)
    {
        ClientConfig.SaslKerberosKinitCmd = saslKerberosKinitCmd;
        return This;
    }

    /// <summary>
    ///     Path to Kerberos keytab file. This configuration property is only used as a variable in `sasl.kerberos.kinit.cmd` as ` ... -t "%{sasl.kerberos.keytab}"`.
    /// </summary>
    /// <param name="saslKerberosKeytab">
    ///     Path to Kerberos keytab file. This configuration property is only used as a variable in `sasl.kerberos.kinit.cmd` as ` ... -t "%{sasl.kerberos.keytab}"`.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSaslKerberosKeytab(string? saslKerberosKeytab)
    {
        ClientConfig.SaslKerberosKeytab = saslKerberosKeytab;
        return This;
    }

    /// <summary>
    ///     Minimum time in milliseconds between key refresh attempts. Disable automatic key refresh by setting this property to 0.
    /// </summary>
    /// <param name="saslKerberosMinTimeBeforeRelogin">
    ///     Minimum time in milliseconds between key refresh attempts. Disable automatic key refresh by setting this property to 0.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSaslKerberosMinTimeBeforeRelogin(int? saslKerberosMinTimeBeforeRelogin)
    {
        ClientConfig.SaslKerberosMinTimeBeforeRelogin = saslKerberosMinTimeBeforeRelogin;
        return This;
    }

    /// <summary>
    ///     SASL username for use with the PLAIN and SASL-SCRAM-.. mechanisms
    /// </summary>
    /// <param name="saslUsername">
    ///     SASL username for use with the PLAIN and SASL-SCRAM-.. mechanisms
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSaslUsername(string? saslUsername)
    {
        ClientConfig.SaslUsername = saslUsername;
        return This;
    }

    /// <summary>
    ///     SASL password for use with the PLAIN and SASL-SCRAM-.. mechanism
    /// </summary>
    /// <param name="saslPassword">
    ///     SASL password for use with the PLAIN and SASL-SCRAM-.. mechanism
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSaslPassword(string? saslPassword)
    {
        ClientConfig.SaslPassword = saslPassword;
        return This;
    }

    /// <summary>
    ///     SASL/OAUTHBEARER configuration. The format is implementation-dependent and must be parsed accordingly. The default unsecured token implementation (see https://tools.ietf.org/html/rfc7515#appendix-A.5) recognizes space-separated name=value pairs with valid names including principalClaimName, principal, scopeClaimName, scope, and lifeSeconds. The default value for principalClaimName is "sub", the default value for scopeClaimName is "scope", and the default value for lifeSeconds is 3600. The scope value is CSV format with the default value being no/empty scope. For example: `principalClaimName=azp principal=admin scopeClaimName=roles scope=role1,role2 lifeSeconds=600`. In addition, SASL extensions can be communicated to the broker via `extension_NAME=value`. For example: `principal=admin extension_traceId=123`
    /// </summary>
    /// <param name="saslOauthbearerConfig">
    ///     SASL/OAUTHBEARER configuration. The format is implementation-dependent and must be parsed accordingly. The default unsecured token implementation (see https://tools.ietf.org/html/rfc7515#appendix-A.5) recognizes space-separated name=value pairs with valid names including principalClaimName, principal, scopeClaimName, scope, and lifeSeconds. The default value for principalClaimName is "sub", the default value for scopeClaimName is "scope", and the default value for lifeSeconds is 3600. The scope value is CSV format with the default value being no/empty scope. For example: `principalClaimName=azp principal=admin scopeClaimName=roles scope=role1,role2 lifeSeconds=600`. In addition, SASL extensions can be communicated to the broker via `extension_NAME=value`. For example: `principal=admin extension_traceId=123`
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithSaslOauthbearerConfig(string? saslOauthbearerConfig)
    {
        ClientConfig.SaslOauthbearerConfig = saslOauthbearerConfig;
        return This;
    }

    /// <summary>
    ///     Enable the builtin unsecure JWT OAUTHBEARER token handler if no oauthbearer_refresh_cb has been set. This builtin handler should only be used for development or testing, and not in production.
    /// </summary>
    /// <param name="enableSaslOauthbearerUnsecureJwt">
    ///     Enable the builtin unsecure JWT OAUTHBEARER token handler if no oauthbearer_refresh_cb has been set. This builtin handler should only be used for development or testing, and not in production.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithEnableSaslOauthbearerUnsecureJwt(bool? enableSaslOauthbearerUnsecureJwt)
    {
        ClientConfig.EnableSaslOauthbearerUnsecureJwt = enableSaslOauthbearerUnsecureJwt;
        return This;
    }

    /// <summary>
    ///     List of plugin libraries to load (; separated). The library search path is platform dependent (see dlopen(3) for Unix and LoadLibrary() for Windows). If no filename extension is specified the platform-specific extension (such as .dll or .so) will be appended automatically.
    /// </summary>
    /// <param name="pluginLibraryPaths">
    ///     List of plugin libraries to load (; separated). The library search path is platform dependent (see dlopen(3) for Unix and LoadLibrary() for Windows). If no filename extension is specified the platform-specific extension (such as .dll or .so) will be appended automatically.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithPluginLibraryPaths(string? pluginLibraryPaths)
    {
        ClientConfig.PluginLibraryPaths = pluginLibraryPaths;
        return This;
    }

    /// <summary>
    ///     A rack identifier for this client. This can be any string value which indicates where this client is physically located. It corresponds with the broker config `broker.rack`.
    /// </summary>
    /// <param name="clientRack">
    ///     A rack identifier for this client. This can be any string value which indicates where this client is physically located. It corresponds with the broker config `broker.rack`.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithClientRack(string? clientRack)
    {
        ClientConfig.ClientRack = clientRack;
        return This;
    }

    /// <summary>
    ///     The maximum length of time (in milliseconds) before a cancellation request
    ///     is acted on. Low values may result in measurably higher CPU usage.
    ///     range: 1 &lt;= dotnet.cancellation.delay.max.ms &lt;= 10000
    /// </summary>
    /// <param name="cancellationDelayMaxMs">
    ///     The maximum length of time (in milliseconds) before a cancellation request
    ///     is acted on. Low values may result in measurably higher CPU usage.
    ///     range: 1 &lt;= dotnet.cancellation.delay.max.ms &lt;= 10000
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public TBuilder WithCancellationDelayMaxMs(int cancellationDelayMaxMs)
    {
        ClientConfig.CancellationDelayMaxMs = cancellationDelayMaxMs;
        return This;
    }

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSaslMechanism" />
    void IKafkaClientConfigurationBuilder.WithSaslMechanism(SaslMechanism? saslMechanism) => WithSaslMechanism(saslMechanism);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithAcks" />
    void IKafkaClientConfigurationBuilder.WithAcks(Acks? acks) => WithAcks(acks);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithClientId" />
    void IKafkaClientConfigurationBuilder.WithClientId(string? clientId) => WithClientId(clientId);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithBootstrapServers" />
    void IKafkaClientConfigurationBuilder.WithBootstrapServers(string? bootstrapServers) => WithBootstrapServers(bootstrapServers);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithMessageMaxBytes" />
    void IKafkaClientConfigurationBuilder.WithMessageMaxBytes(int? messageMaxBytes) => WithMessageMaxBytes(messageMaxBytes);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithMessageCopyMaxBytes" />
    void IKafkaClientConfigurationBuilder.WithMessageCopyMaxBytes(int? messageCopyMaxBytes) => WithMessageCopyMaxBytes(messageCopyMaxBytes);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithReceiveMessageMaxBytes" />
    void IKafkaClientConfigurationBuilder.WithReceiveMessageMaxBytes(int? receiveMessageMaxBytes) => WithReceiveMessageMaxBytes(receiveMessageMaxBytes);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithMaxInFlight" />
    void IKafkaClientConfigurationBuilder.WithMaxInFlight(int? maxInFlight) => WithMaxInFlight(maxInFlight);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithTopicMetadataRefreshIntervalMs" />
    void IKafkaClientConfigurationBuilder.WithTopicMetadataRefreshIntervalMs(int? topicMetadataRefreshIntervalMs) => WithTopicMetadataRefreshIntervalMs(topicMetadataRefreshIntervalMs);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithMetadataMaxAgeMs" />
    void IKafkaClientConfigurationBuilder.WithMetadataMaxAgeMs(int? metadataMaxAgeMs) => WithMetadataMaxAgeMs(metadataMaxAgeMs);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithTopicMetadataRefreshFastIntervalMs" />
    void IKafkaClientConfigurationBuilder.WithTopicMetadataRefreshFastIntervalMs(int? topicMetadataRefreshFastIntervalMs) => WithTopicMetadataRefreshFastIntervalMs(topicMetadataRefreshFastIntervalMs);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithTopicMetadataRefreshSparse" />
    void IKafkaClientConfigurationBuilder.WithTopicMetadataRefreshSparse(bool? topicMetadataRefreshSparse) => WithTopicMetadataRefreshSparse(topicMetadataRefreshSparse);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithTopicMetadataPropagationMaxMs" />
    void IKafkaClientConfigurationBuilder.WithTopicMetadataPropagationMaxMs(int? topicMetadataPropagationMaxMs) => WithTopicMetadataPropagationMaxMs(topicMetadataPropagationMaxMs);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithTopicBlacklist" />
    void IKafkaClientConfigurationBuilder.WithTopicBlacklist(string? topicBlacklist) => WithTopicBlacklist(topicBlacklist);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithDebug" />
    void IKafkaClientConfigurationBuilder.WithDebug(string? debug) => WithDebug(debug);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSocketTimeoutMs" />
    void IKafkaClientConfigurationBuilder.WithSocketTimeoutMs(int? socketTimeoutMs) => WithSocketTimeoutMs(socketTimeoutMs);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSocketSendBufferBytes" />
    void IKafkaClientConfigurationBuilder.WithSocketSendBufferBytes(int? socketSendBufferBytes) => WithSocketSendBufferBytes(socketSendBufferBytes);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSocketReceiveBufferBytes" />
    void IKafkaClientConfigurationBuilder.WithSocketReceiveBufferBytes(int? socketReceiveBufferBytes) => WithSocketReceiveBufferBytes(socketReceiveBufferBytes);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSocketKeepaliveEnable" />
    void IKafkaClientConfigurationBuilder.WithSocketKeepaliveEnable(bool? socketKeepaliveEnable) => WithSocketKeepaliveEnable(socketKeepaliveEnable);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSocketNagleDisable" />
    void IKafkaClientConfigurationBuilder.WithSocketNagleDisable(bool? socketNagleDisable) => WithSocketNagleDisable(socketNagleDisable);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSocketMaxFails" />
    void IKafkaClientConfigurationBuilder.WithSocketMaxFails(int? socketMaxFails) => WithSocketMaxFails(socketMaxFails);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithBrokerAddressTtl" />
    void IKafkaClientConfigurationBuilder.WithBrokerAddressTtl(int? brokerAddressTtl) => WithBrokerAddressTtl(brokerAddressTtl);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithBrokerAddressFamily" />
    void IKafkaClientConfigurationBuilder.WithBrokerAddressFamily(BrokerAddressFamily? brokerAddressFamily) => WithBrokerAddressFamily(brokerAddressFamily);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithConnectionsMaxIdleMs" />
    void IKafkaClientConfigurationBuilder.WithConnectionsMaxIdleMs(int? connectionsMaxIdleMs) => WithConnectionsMaxIdleMs(connectionsMaxIdleMs);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithReconnectBackoffMs" />
    void IKafkaClientConfigurationBuilder.WithReconnectBackoffMs(int? reconnectBackoffMs) => WithReconnectBackoffMs(reconnectBackoffMs);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithReconnectBackoffMaxMs" />
    void IKafkaClientConfigurationBuilder.WithReconnectBackoffMaxMs(int? reconnectBackoffMaxMs) => WithReconnectBackoffMaxMs(reconnectBackoffMaxMs);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithStatisticsIntervalMs" />
    void IKafkaClientConfigurationBuilder.WithStatisticsIntervalMs(int? statisticsIntervalMs) => WithStatisticsIntervalMs(statisticsIntervalMs);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithLogQueue" />
    void IKafkaClientConfigurationBuilder.WithLogQueue(bool? logQueue) => WithLogQueue(logQueue);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithLogThreadName" />
    void IKafkaClientConfigurationBuilder.WithLogThreadName(bool? logThreadName) => WithLogThreadName(logThreadName);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithEnableRandomSeed" />
    void IKafkaClientConfigurationBuilder.WithEnableRandomSeed(bool? enableRandomSeed) => WithEnableRandomSeed(enableRandomSeed);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithLogConnectionClose" />
    void IKafkaClientConfigurationBuilder.WithLogConnectionClose(bool? logConnectionClose) => WithLogConnectionClose(logConnectionClose);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithInternalTerminationSignal" />
    void IKafkaClientConfigurationBuilder.WithInternalTerminationSignal(int? internalTerminationSignal) => WithInternalTerminationSignal(internalTerminationSignal);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithApiVersionRequest" />
    void IKafkaClientConfigurationBuilder.WithApiVersionRequest(bool? apiVersionRequest) => WithApiVersionRequest(apiVersionRequest);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithApiVersionRequestTimeoutMs" />
    void IKafkaClientConfigurationBuilder.WithApiVersionRequestTimeoutMs(int? apiVersionRequestTimeoutMs) => WithApiVersionRequestTimeoutMs(apiVersionRequestTimeoutMs);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithApiVersionFallbackMs" />
    void IKafkaClientConfigurationBuilder.WithApiVersionFallbackMs(int? apiVersionFallbackMs) => WithApiVersionFallbackMs(apiVersionFallbackMs);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithBrokerVersionFallback" />
    void IKafkaClientConfigurationBuilder.WithBrokerVersionFallback(string? brokerVersionFallback) => WithBrokerVersionFallback(brokerVersionFallback);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSecurityProtocol" />
    void IKafkaClientConfigurationBuilder.WithSecurityProtocol(SecurityProtocol? securityProtocol) => WithSecurityProtocol(securityProtocol);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslCipherSuites" />
    void IKafkaClientConfigurationBuilder.WithSslCipherSuites(string? sslCipherSuites) => WithSslCipherSuites(sslCipherSuites);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslCurvesList" />
    void IKafkaClientConfigurationBuilder.WithSslCurvesList(string? sslCurvesList) => WithSslCurvesList(sslCurvesList);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslSigalgsList" />
    void IKafkaClientConfigurationBuilder.WithSslSigalgsList(string? sslSigalgsList) => WithSslSigalgsList(sslSigalgsList);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslKeyLocation" />
    void IKafkaClientConfigurationBuilder.WithSslKeyLocation(string? sslKeyLocation) => WithSslKeyLocation(sslKeyLocation);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslKeyPassword" />
    void IKafkaClientConfigurationBuilder.WithSslKeyPassword(string? sslKeyPassword) => WithSslKeyPassword(sslKeyPassword);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslKeyPem" />
    void IKafkaClientConfigurationBuilder.WithSslKeyPem(string? sslKeyPem) => WithSslKeyPem(sslKeyPem);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslCertificateLocation" />
    void IKafkaClientConfigurationBuilder.WithSslCertificateLocation(string? sslCertificateLocation) => WithSslCertificateLocation(sslCertificateLocation);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslCertificatePem" />
    void IKafkaClientConfigurationBuilder.WithSslCertificatePem(string? sslCertificatePem) => WithSslCertificatePem(sslCertificatePem);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslCaLocation" />
    void IKafkaClientConfigurationBuilder.WithSslCaLocation(string? sslCaLocation) => WithSslCaLocation(sslCaLocation);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslCaPem" />
    void IKafkaClientConfigurationBuilder.WithSslCaPem(string? sslCaPem) => WithSslCaPem(sslCaPem);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslCaCertificateStores" />
    void IKafkaClientConfigurationBuilder.WithSslCaCertificateStores(string? sslCaCertificateStores) => WithSslCaCertificateStores(sslCaCertificateStores);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslCrlLocation" />
    void IKafkaClientConfigurationBuilder.WithSslCrlLocation(string? sslCrlLocation) => WithSslCrlLocation(sslCrlLocation);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslKeystoreLocation" />
    void IKafkaClientConfigurationBuilder.WithSslKeystoreLocation(string? sslKeystoreLocation) => WithSslKeystoreLocation(sslKeystoreLocation);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslKeystorePassword" />
    void IKafkaClientConfigurationBuilder.WithSslKeystorePassword(string? sslKeystorePassword) => WithSslKeystorePassword(sslKeystorePassword);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslEngineLocation" />
    void IKafkaClientConfigurationBuilder.WithSslEngineLocation(string? sslEngineLocation) => WithSslEngineLocation(sslEngineLocation);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslEngineId" />
    void IKafkaClientConfigurationBuilder.WithSslEngineId(string? sslEngineId) => WithSslEngineId(sslEngineId);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithEnableSslCertificateVerification" />
    void IKafkaClientConfigurationBuilder.WithEnableSslCertificateVerification(bool? enableSslCertificateVerification) => WithEnableSslCertificateVerification(enableSslCertificateVerification);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSslEndpointIdentificationAlgorithm" />
    void IKafkaClientConfigurationBuilder.WithSslEndpointIdentificationAlgorithm(SslEndpointIdentificationAlgorithm? sslEndpointIdentificationAlgorithm) => WithSslEndpointIdentificationAlgorithm(sslEndpointIdentificationAlgorithm);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSaslKerberosServiceName" />
    void IKafkaClientConfigurationBuilder.WithSaslKerberosServiceName(string? saslKerberosServiceName) => WithSaslKerberosServiceName(saslKerberosServiceName);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSaslKerberosPrincipal" />
    void IKafkaClientConfigurationBuilder.WithSaslKerberosPrincipal(string? saslKerberosPrincipal) => WithSaslKerberosPrincipal(saslKerberosPrincipal);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSaslKerberosKinitCmd" />
    void IKafkaClientConfigurationBuilder.WithSaslKerberosKinitCmd(string? saslKerberosKinitCmd) => WithSaslKerberosKinitCmd(saslKerberosKinitCmd);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSaslKerberosKeytab" />
    void IKafkaClientConfigurationBuilder.WithSaslKerberosKeytab(string? saslKerberosKeytab) => WithSaslKerberosKeytab(saslKerberosKeytab);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSaslKerberosMinTimeBeforeRelogin" />
    void IKafkaClientConfigurationBuilder.WithSaslKerberosMinTimeBeforeRelogin(int? saslKerberosMinTimeBeforeRelogin) => WithSaslKerberosMinTimeBeforeRelogin(saslKerberosMinTimeBeforeRelogin);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSaslUsername" />
    void IKafkaClientConfigurationBuilder.WithSaslUsername(string? saslUsername) => WithSaslUsername(saslUsername);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSaslPassword" />
    void IKafkaClientConfigurationBuilder.WithSaslPassword(string? saslPassword) => WithSaslPassword(saslPassword);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithSaslOauthbearerConfig" />
    void IKafkaClientConfigurationBuilder.WithSaslOauthbearerConfig(string? saslOauthbearerConfig) => WithSaslOauthbearerConfig(saslOauthbearerConfig);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithEnableSaslOauthbearerUnsecureJwt" />
    void IKafkaClientConfigurationBuilder.WithEnableSaslOauthbearerUnsecureJwt(bool? enableSaslOauthbearerUnsecureJwt) => WithEnableSaslOauthbearerUnsecureJwt(enableSaslOauthbearerUnsecureJwt);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithPluginLibraryPaths" />
    void IKafkaClientConfigurationBuilder.WithPluginLibraryPaths(string? pluginLibraryPaths) => WithPluginLibraryPaths(pluginLibraryPaths);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithClientRack" />
    void IKafkaClientConfigurationBuilder.WithClientRack(string? clientRack) => WithClientRack(clientRack);

    /// <inheritdoc cref="IKafkaClientConfigurationBuilder.WithCancellationDelayMaxMs" />
    void IKafkaClientConfigurationBuilder.WithCancellationDelayMaxMs(int cancellationDelayMaxMs) => WithCancellationDelayMaxMs(cancellationDelayMaxMs);
}

/// <content>
///     The autogenerated part of the <see cref="KafkaConsumerConfigurationBuilder" /> class.
/// </content>
[SuppressMessage("", "SA1649", Justification = "Autogenerated all at once")]
[SuppressMessage("", "SA1402", Justification = "Autogenerated all at once")]
[SuppressMessage("", "CA1200", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1623", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1629", Justification = "Summary copied from wrapped class")]
[SuppressMessage("StyleCop.CSharp.DocumentationRules", "SA1625:Element documentation should not be copied and pasted", Justification = "Summary copied from wrapped class")]
public partial class KafkaConsumerConfigurationBuilder
{
    /// <summary>
    ///     A comma separated list of fields that may be optionally set
    ///     in <see cref="T:Confluent.Kafka.ConsumeResult`2" />
    ///     objects returned by the
    ///     <see cref="M:Confluent.Kafka.Consumer`2.Consume(System.TimeSpan)" />
    ///     method. Disabling fields that you do not require will improve
    ///     throughput and reduce memory consumption. Allowed values:
    ///     headers, timestamp, topic, all, none
    /// </summary>
    /// <param name="consumeResultFields">
    ///     A comma separated list of fields that may be optionally set
    ///     in <see cref="T:Confluent.Kafka.ConsumeResult`2" />
    ///     objects returned by the
    ///     <see cref="M:Confluent.Kafka.Consumer`2.Consume(System.TimeSpan)" />
    ///     method. Disabling fields that you do not require will improve
    ///     throughput and reduce memory consumption. Allowed values:
    ///     headers, timestamp, topic, all, none
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithConsumeResultFields(string? consumeResultFields)
    {
        ClientConfig.ConsumeResultFields = consumeResultFields;
        return This;
    }

    /// <summary>
    ///     Action to take when there is no initial offset in offset store or the desired offset is out of range: 'smallest','earliest' - automatically reset the offset to the smallest offset, 'largest','latest' - automatically reset the offset to the largest offset, 'error' - trigger an error (ERR__AUTO_OFFSET_RESET) which is retrieved by consuming messages and checking 'message-&gt;err'.
    /// </summary>
    /// <param name="autoOffsetReset">
    ///     Action to take when there is no initial offset in offset store or the desired offset is out of range: 'smallest','earliest' - automatically reset the offset to the smallest offset, 'largest','latest' - automatically reset the offset to the largest offset, 'error' - trigger an error (ERR__AUTO_OFFSET_RESET) which is retrieved by consuming messages and checking 'message-&gt;err'.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithAutoOffsetReset(AutoOffsetReset? autoOffsetReset)
    {
        ClientConfig.AutoOffsetReset = autoOffsetReset;
        return This;
    }

    /// <summary>
    ///     Enable static group membership. Static group members are able to leave and rejoin a group within the configured `session.timeout.ms` without prompting a group rebalance. This should be used in combination with a larger `session.timeout.ms` to avoid group rebalances caused by transient unavailability (e.g. process restarts). Requires broker version &gt;= 2.3.0.
    /// </summary>
    /// <param name="groupInstanceId">
    ///     Enable static group membership. Static group members are able to leave and rejoin a group within the configured `session.timeout.ms` without prompting a group rebalance. This should be used in combination with a larger `session.timeout.ms` to avoid group rebalances caused by transient unavailability (e.g. process restarts). Requires broker version &gt;= 2.3.0.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithGroupInstanceId(string? groupInstanceId)
    {
        ClientConfig.GroupInstanceId = groupInstanceId;
        return This;
    }

    /// <summary>
    ///     The name of one or more partition assignment strategies. The elected group leader will use a strategy supported by all members of the group to assign partitions to group members. If there is more than one eligible strategy, preference is determined by the order of this list (strategies earlier in the list have higher priority). Cooperative and non-cooperative (eager) strategies must not be mixed. Available strategies: range, roundrobin, cooperative-sticky.
    /// </summary>
    /// <param name="partitionAssignmentStrategy">
    ///     The name of one or more partition assignment strategies. The elected group leader will use a strategy supported by all members of the group to assign partitions to group members. If there is more than one eligible strategy, preference is determined by the order of this list (strategies earlier in the list have higher priority). Cooperative and non-cooperative (eager) strategies must not be mixed. Available strategies: range, roundrobin, cooperative-sticky.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithPartitionAssignmentStrategy(PartitionAssignmentStrategy? partitionAssignmentStrategy)
    {
        ClientConfig.PartitionAssignmentStrategy = partitionAssignmentStrategy;
        return This;
    }

    /// <summary>
    ///     Client group session and failure detection timeout. The consumer sends periodic heartbeats (heartbeat.interval.ms) to indicate its liveness to the broker. If no hearts are received by the broker for a group member within the session timeout, the broker will remove the consumer from the group and trigger a rebalance. The allowed range is configured with the **broker** configuration properties `group.min.session.timeout.ms` and `group.max.session.timeout.ms`. Also see `max.poll.interval.ms`.
    /// </summary>
    /// <param name="sessionTimeoutMs">
    ///     Client group session and failure detection timeout. The consumer sends periodic heartbeats (heartbeat.interval.ms) to indicate its liveness to the broker. If no hearts are received by the broker for a group member within the session timeout, the broker will remove the consumer from the group and trigger a rebalance. The allowed range is configured with the **broker** configuration properties `group.min.session.timeout.ms` and `group.max.session.timeout.ms`. Also see `max.poll.interval.ms`.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithSessionTimeoutMs(int? sessionTimeoutMs)
    {
        ClientConfig.SessionTimeoutMs = sessionTimeoutMs;
        return This;
    }

    /// <summary>
    ///     Group session keepalive heartbeat interval.
    /// </summary>
    /// <param name="heartbeatIntervalMs">
    ///     Group session keepalive heartbeat interval.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithHeartbeatIntervalMs(int? heartbeatIntervalMs)
    {
        ClientConfig.HeartbeatIntervalMs = heartbeatIntervalMs;
        return This;
    }

    /// <summary>
    ///     Group protocol type. NOTE: Currently, the only supported group protocol type is `consumer`.
    /// </summary>
    /// <param name="groupProtocolType">
    ///     Group protocol type. NOTE: Currently, the only supported group protocol type is `consumer`.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithGroupProtocolType(string? groupProtocolType)
    {
        ClientConfig.GroupProtocolType = groupProtocolType;
        return This;
    }

    /// <summary>
    ///     How often to query for the current client group coordinator. If the currently assigned coordinator is down the configured query interval will be divided by ten to more quickly recover in case of coordinator reassignment.
    /// </summary>
    /// <param name="coordinatorQueryIntervalMs">
    ///     How often to query for the current client group coordinator. If the currently assigned coordinator is down the configured query interval will be divided by ten to more quickly recover in case of coordinator reassignment.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithCoordinatorQueryIntervalMs(int? coordinatorQueryIntervalMs)
    {
        ClientConfig.CoordinatorQueryIntervalMs = coordinatorQueryIntervalMs;
        return This;
    }

    /// <summary>
    ///     Maximum allowed time between calls to consume messages (e.g., rd_kafka_consumer_poll()) for high-level consumers. If this interval is exceeded the consumer is considered failed and the group will rebalance in order to reassign the partitions to another consumer group member. Warning: Offset commits may be not possible at this point. Note: It is recommended to set `enable.auto.offset.store=false` for long-time processing applications and then explicitly store offsets (using offsets_store()) *after* message processing, to make sure offsets are not auto-committed prior to processing has finished. The interval is checked two times per second. See KIP-62 for more information.
    /// </summary>
    /// <param name="maxPollIntervalMs">
    ///     Maximum allowed time between calls to consume messages (e.g., rd_kafka_consumer_poll()) for high-level consumers. If this interval is exceeded the consumer is considered failed and the group will rebalance in order to reassign the partitions to another consumer group member. Warning: Offset commits may be not possible at this point. Note: It is recommended to set `enable.auto.offset.store=false` for long-time processing applications and then explicitly store offsets (using offsets_store()) *after* message processing, to make sure offsets are not auto-committed prior to processing has finished. The interval is checked two times per second. See KIP-62 for more information.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithMaxPollIntervalMs(int? maxPollIntervalMs)
    {
        ClientConfig.MaxPollIntervalMs = maxPollIntervalMs;
        return This;
    }

    /// <summary>
    ///     The frequency in milliseconds that the consumer offsets are committed (written) to offset storage. (0 = disable). This setting is used by the high-level consumer.
    /// </summary>
    /// <param name="autoCommitIntervalMs">
    ///     The frequency in milliseconds that the consumer offsets are committed (written) to offset storage. (0 = disable). This setting is used by the high-level consumer.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithAutoCommitIntervalMs(int? autoCommitIntervalMs)
    {
        ClientConfig.AutoCommitIntervalMs = autoCommitIntervalMs;
        return This;
    }

    /// <summary>
    ///     Minimum number of messages per topic+partition librdkafka tries to maintain in the local consumer queue.
    /// </summary>
    /// <param name="queuedMinMessages">
    ///     Minimum number of messages per topic+partition librdkafka tries to maintain in the local consumer queue.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithQueuedMinMessages(int? queuedMinMessages)
    {
        ClientConfig.QueuedMinMessages = queuedMinMessages;
        return This;
    }

    /// <summary>
    ///     Maximum number of kilobytes of queued pre-fetched messages in the local consumer queue. If using the high-level consumer this setting applies to the single consumer queue, regardless of the number of partitions. When using the legacy simple consumer or when separate partition queues are used this setting applies per partition. This value may be overshot by fetch.message.max.bytes. This property has higher priority than queued.min.messages.
    /// </summary>
    /// <param name="queuedMaxMessagesKbytes">
    ///     Maximum number of kilobytes of queued pre-fetched messages in the local consumer queue. If using the high-level consumer this setting applies to the single consumer queue, regardless of the number of partitions. When using the legacy simple consumer or when separate partition queues are used this setting applies per partition. This value may be overshot by fetch.message.max.bytes. This property has higher priority than queued.min.messages.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithQueuedMaxMessagesKbytes(int? queuedMaxMessagesKbytes)
    {
        ClientConfig.QueuedMaxMessagesKbytes = queuedMaxMessagesKbytes;
        return This;
    }

    /// <summary>
    ///     Maximum time the broker may wait to fill the Fetch response with fetch.min.bytes of messages.
    /// </summary>
    /// <param name="fetchWaitMaxMs">
    ///     Maximum time the broker may wait to fill the Fetch response with fetch.min.bytes of messages.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithFetchWaitMaxMs(int? fetchWaitMaxMs)
    {
        ClientConfig.FetchWaitMaxMs = fetchWaitMaxMs;
        return This;
    }

    /// <summary>
    ///     Initial maximum number of bytes per topic+partition to request when fetching messages from the broker. If the client encounters a message larger than this value it will gradually try to increase it until the entire message can be fetched.
    /// </summary>
    /// <param name="maxPartitionFetchBytes">
    ///     Initial maximum number of bytes per topic+partition to request when fetching messages from the broker. If the client encounters a message larger than this value it will gradually try to increase it until the entire message can be fetched.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithMaxPartitionFetchBytes(int? maxPartitionFetchBytes)
    {
        ClientConfig.MaxPartitionFetchBytes = maxPartitionFetchBytes;
        return This;
    }

    /// <summary>
    ///     Maximum amount of data the broker shall return for a Fetch request. Messages are fetched in batches by the consumer and if the first message batch in the first non-empty partition of the Fetch request is larger than this value, then the message batch will still be returned to ensure the consumer can make progress. The maximum message batch size accepted by the broker is defined via `message.max.bytes` (broker config) or `max.message.bytes` (broker topic config). `fetch.max.bytes` is automatically adjusted upwards to be at least `message.max.bytes` (consumer config).
    /// </summary>
    /// <param name="fetchMaxBytes">
    ///     Maximum amount of data the broker shall return for a Fetch request. Messages are fetched in batches by the consumer and if the first message batch in the first non-empty partition of the Fetch request is larger than this value, then the message batch will still be returned to ensure the consumer can make progress. The maximum message batch size accepted by the broker is defined via `message.max.bytes` (broker config) or `max.message.bytes` (broker topic config). `fetch.max.bytes` is automatically adjusted upwards to be at least `message.max.bytes` (consumer config).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithFetchMaxBytes(int? fetchMaxBytes)
    {
        ClientConfig.FetchMaxBytes = fetchMaxBytes;
        return This;
    }

    /// <summary>
    ///     Minimum number of bytes the broker responds with. If fetch.wait.max.ms expires the accumulated data will be sent to the client regardless of this setting.
    /// </summary>
    /// <param name="fetchMinBytes">
    ///     Minimum number of bytes the broker responds with. If fetch.wait.max.ms expires the accumulated data will be sent to the client regardless of this setting.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithFetchMinBytes(int? fetchMinBytes)
    {
        ClientConfig.FetchMinBytes = fetchMinBytes;
        return This;
    }

    /// <summary>
    ///     How long to postpone the next fetch request for a topic+partition in case of a fetch error.
    /// </summary>
    /// <param name="fetchErrorBackoffMs">
    ///     How long to postpone the next fetch request for a topic+partition in case of a fetch error.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithFetchErrorBackoffMs(int? fetchErrorBackoffMs)
    {
        ClientConfig.FetchErrorBackoffMs = fetchErrorBackoffMs;
        return This;
    }

    /// <summary>
    ///     Controls how to read messages written transactionally: `read_committed` - only return transactional messages which have been committed. `read_uncommitted` - return all messages, even transactional messages which have been aborted.
    /// </summary>
    /// <param name="isolationLevel">
    ///     Controls how to read messages written transactionally: `read_committed` - only return transactional messages which have been committed. `read_uncommitted` - return all messages, even transactional messages which have been aborted.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithIsolationLevel(IsolationLevel? isolationLevel)
    {
        ClientConfig.IsolationLevel = isolationLevel;
        return This;
    }

    /// <summary>
    ///     Verify CRC32 of consumed messages, ensuring no on-the-wire or on-disk corruption to the messages occurred. This check comes at slightly increased CPU usage.
    /// </summary>
    /// <param name="checkCrcs">
    ///     Verify CRC32 of consumed messages, ensuring no on-the-wire or on-disk corruption to the messages occurred. This check comes at slightly increased CPU usage.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaConsumerConfigurationBuilder WithCheckCrcs(bool? checkCrcs)
    {
        ClientConfig.CheckCrcs = checkCrcs;
        return This;
    }

    /// <summary>
    ///     Automatically and periodically commit offsets in the background. Note: setting this to false does not prevent the consumer from fetching previously committed start offsets. To circumvent this behaviour set specific start offsets per partition in the call to assign().
    /// </summary>
    /// <param name="enableAutoCommit">
    ///     Automatically and periodically commit offsets in the background. Note: setting this to false does not prevent the consumer from fetching previously committed start offsets. To circumvent this behaviour set specific start offsets per partition in the call to assign().
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    internal KafkaConsumerConfigurationBuilder WithEnableAutoCommit(bool? enableAutoCommit)
    {
        ClientConfig.EnableAutoCommit = enableAutoCommit;
        return This;
    }

    /// <summary>
    ///     Emit RD_KAFKA_RESP_ERR__PARTITION_EOF event whenever the consumer reaches the end of a partition.
    /// </summary>
    /// <param name="enablePartitionEof">
    ///     Emit RD_KAFKA_RESP_ERR__PARTITION_EOF event whenever the consumer reaches the end of a partition.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    internal KafkaConsumerConfigurationBuilder WithEnablePartitionEof(bool? enablePartitionEof)
    {
        ClientConfig.EnablePartitionEof = enablePartitionEof;
        return This;
    }

    /// <summary>
    ///     Allow automatic topic creation on the broker when subscribing to or assigning non-existent topics. The broker must also be configured with `auto.create.topics.enable=true` for this configuraiton to take effect. Note: The default value (false) is different from the Java consumer (true). Requires broker version &gt;= 0.11.0.0, for older broker versions only the broker configuration applies.
    /// </summary>
    /// <param name="allowAutoCreateTopics">
    ///     Allow automatic topic creation on the broker when subscribing to or assigning non-existent topics. The broker must also be configured with `auto.create.topics.enable=true` for this configuraiton to take effect. Note: The default value (false) is different from the Java consumer (true). Requires broker version &gt;= 0.11.0.0, for older broker versions only the broker configuration applies.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    internal KafkaConsumerConfigurationBuilder WithAllowAutoCreateTopics(bool? allowAutoCreateTopics)
    {
        ClientConfig.AllowAutoCreateTopics = allowAutoCreateTopics;
        return This;
    }
}

/// <content>
///     The autogenerated part of the <see cref="KafkaProducerConfigurationBuilder" /> class.
/// </content>
[SuppressMessage("", "SA1649", Justification = "Autogenerated all at once")]
[SuppressMessage("", "SA1402", Justification = "Autogenerated all at once")]
[SuppressMessage("", "CA1200", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1623", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1629", Justification = "Summary copied from wrapped class")]
[SuppressMessage("StyleCop.CSharp.DocumentationRules", "SA1625:Element documentation should not be copied and pasted", Justification = "Summary copied from wrapped class")]
public partial class KafkaProducerConfigurationBuilder
{
    /// <summary>
    ///     Specifies whether or not the producer should start a background poll
    ///     thread to receive delivery reports and event notifications. Generally,
    ///     this should be set to true. If set to false, you will need to call
    ///     the Poll function manually.
    /// </summary>
    /// <param name="enableBackgroundPoll">
    ///     Specifies whether or not the producer should start a background poll
    ///     thread to receive delivery reports and event notifications. Generally,
    ///     this should be set to true. If set to false, you will need to call
    ///     the Poll function manually.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithEnableBackgroundPoll(bool? enableBackgroundPoll)
    {
        ClientConfig.EnableBackgroundPoll = enableBackgroundPoll;
        return This;
    }

    /// <summary>
    ///     A comma separated list of fields that may be optionally set in delivery
    ///     reports. Disabling delivery report fields that you do not require will
    ///     improve maximum throughput and reduce memory usage. Allowed values:
    ///     key, value, timestamp, headers, status, all, none.
    /// </summary>
    /// <param name="deliveryReportFields">
    ///     A comma separated list of fields that may be optionally set in delivery
    ///     reports. Disabling delivery report fields that you do not require will
    ///     improve maximum throughput and reduce memory usage. Allowed values:
    ///     key, value, timestamp, headers, status, all, none.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithDeliveryReportFields(string? deliveryReportFields)
    {
        ClientConfig.DeliveryReportFields = deliveryReportFields;
        return This;
    }

    /// <summary>
    ///     The ack timeout of the producer request in milliseconds. This value is only enforced by the broker and relies on `request.required.acks` being != 0.
    /// </summary>
    /// <param name="requestTimeoutMs">
    ///     The ack timeout of the producer request in milliseconds. This value is only enforced by the broker and relies on `request.required.acks` being != 0.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithRequestTimeoutMs(int? requestTimeoutMs)
    {
        ClientConfig.RequestTimeoutMs = requestTimeoutMs;
        return This;
    }

    /// <summary>
    ///     Local message timeout. This value is only enforced locally and limits the time a produced message waits for successful delivery. A time of 0 is infinite. This is the maximum time librdkafka may use to deliver a message (including retries). Delivery error occurs when either the retry count or the message timeout are exceeded. The message timeout is automatically adjusted to `transaction.timeout.ms` if `transactional.id` is configured.
    /// </summary>
    /// <param name="messageTimeoutMs">
    ///     Local message timeout. This value is only enforced locally and limits the time a produced message waits for successful delivery. A time of 0 is infinite. This is the maximum time librdkafka may use to deliver a message (including retries). Delivery error occurs when either the retry count or the message timeout are exceeded. The message timeout is automatically adjusted to `transaction.timeout.ms` if `transactional.id` is configured.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithMessageTimeoutMs(int? messageTimeoutMs)
    {
        ClientConfig.MessageTimeoutMs = messageTimeoutMs;
        return This;
    }

    /// <summary>
    ///     Partitioner: `random` - random distribution, `consistent` - CRC32 hash of key (Empty and NULL keys are mapped to single partition), `consistent_random` - CRC32 hash of key (Empty and NULL keys are randomly partitioned), `murmur2` - Java Producer compatible Murmur2 hash of key (NULL keys are mapped to single partition), `murmur2_random` - Java Producer compatible Murmur2 hash of key (NULL keys are randomly partitioned. This is functionally equivalent to the default partitioner in the Java Producer.), `fnv1a` - FNV-1a hash of key (NULL keys are mapped to single partition), `fnv1a_random` - FNV-1a hash of key (NULL keys are randomly partitioned).
    /// </summary>
    /// <param name="partitioner">
    ///     Partitioner: `random` - random distribution, `consistent` - CRC32 hash of key (Empty and NULL keys are mapped to single partition), `consistent_random` - CRC32 hash of key (Empty and NULL keys are randomly partitioned), `murmur2` - Java Producer compatible Murmur2 hash of key (NULL keys are mapped to single partition), `murmur2_random` - Java Producer compatible Murmur2 hash of key (NULL keys are randomly partitioned. This is functionally equivalent to the default partitioner in the Java Producer.), `fnv1a` - FNV-1a hash of key (NULL keys are mapped to single partition), `fnv1a_random` - FNV-1a hash of key (NULL keys are randomly partitioned).
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithPartitioner(Partitioner? partitioner)
    {
        ClientConfig.Partitioner = partitioner;
        return This;
    }

    /// <summary>
    ///     Compression level parameter for algorithm selected by configuration property `compression.codec`. Higher values will result in better compression at the cost of more CPU usage. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level.
    /// </summary>
    /// <param name="compressionLevel">
    ///     Compression level parameter for algorithm selected by configuration property `compression.codec`. Higher values will result in better compression at the cost of more CPU usage. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithCompressionLevel(int? compressionLevel)
    {
        ClientConfig.CompressionLevel = compressionLevel;
        return This;
    }

    /// <summary>
    ///     Enables the transactional producer. The transactional.id is used to identify the same transactional producer instance across process restarts. It allows the producer to guarantee that transactions corresponding to earlier instances of the same producer have been finalized prior to starting any new transactions, and that any zombie instances are fenced off. If no transactional.id is provided, then the producer is limited to idempotent delivery (if enable.idempotence is set). Requires broker version &gt;= 0.11.0.
    /// </summary>
    /// <param name="transactionalId">
    ///     Enables the transactional producer. The transactional.id is used to identify the same transactional producer instance across process restarts. It allows the producer to guarantee that transactions corresponding to earlier instances of the same producer have been finalized prior to starting any new transactions, and that any zombie instances are fenced off. If no transactional.id is provided, then the producer is limited to idempotent delivery (if enable.idempotence is set). Requires broker version &gt;= 0.11.0.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithTransactionalId(string? transactionalId)
    {
        ClientConfig.TransactionalId = transactionalId;
        return This;
    }

    /// <summary>
    ///     The maximum amount of time in milliseconds that the transaction coordinator will wait for a transaction status update from the producer before proactively aborting the ongoing transaction. If this value is larger than the `transaction.max.timeout.ms` setting in the broker, the init_transactions() call will fail with ERR_INVALID_TRANSACTION_TIMEOUT. The transaction timeout automatically adjusts `message.timeout.ms` and `socket.timeout.ms`, unless explicitly configured in which case they must not exceed the transaction timeout (`socket.timeout.ms` must be at least 100ms lower than `transaction.timeout.ms`). This is also the default timeout value if no timeout (-1) is supplied to the transactional API methods.
    /// </summary>
    /// <param name="transactionTimeoutMs">
    ///     The maximum amount of time in milliseconds that the transaction coordinator will wait for a transaction status update from the producer before proactively aborting the ongoing transaction. If this value is larger than the `transaction.max.timeout.ms` setting in the broker, the init_transactions() call will fail with ERR_INVALID_TRANSACTION_TIMEOUT. The transaction timeout automatically adjusts `message.timeout.ms` and `socket.timeout.ms`, unless explicitly configured in which case they must not exceed the transaction timeout (`socket.timeout.ms` must be at least 100ms lower than `transaction.timeout.ms`). This is also the default timeout value if no timeout (-1) is supplied to the transactional API methods.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithTransactionTimeoutMs(int? transactionTimeoutMs)
    {
        ClientConfig.TransactionTimeoutMs = transactionTimeoutMs;
        return This;
    }

    /// <summary>
    ///     **EXPERIMENTAL**: subject to change or removal. When set to `true`, any error that could result in a gap in the produced message series when a batch of messages fails, will raise a fatal error (ERR__GAPLESS_GUARANTEE) and stop the producer. Messages failing due to `message.timeout.ms` are not covered by this guarantee. Requires `enable.idempotence=true`.
    /// </summary>
    /// <param name="enableGaplessGuarantee">
    ///     **EXPERIMENTAL**: subject to change or removal. When set to `true`, any error that could result in a gap in the produced message series when a batch of messages fails, will raise a fatal error (ERR__GAPLESS_GUARANTEE) and stop the producer. Messages failing due to `message.timeout.ms` are not covered by this guarantee. Requires `enable.idempotence=true`.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithEnableGaplessGuarantee(bool? enableGaplessGuarantee)
    {
        ClientConfig.EnableGaplessGuarantee = enableGaplessGuarantee;
        return This;
    }

    /// <summary>
    ///     Maximum number of messages allowed on the producer queue. This queue is shared by all topics and partitions.
    /// </summary>
    /// <param name="queueBufferingMaxMessages">
    ///     Maximum number of messages allowed on the producer queue. This queue is shared by all topics and partitions.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithQueueBufferingMaxMessages(int? queueBufferingMaxMessages)
    {
        ClientConfig.QueueBufferingMaxMessages = queueBufferingMaxMessages;
        return This;
    }

    /// <summary>
    ///     Maximum total message size sum allowed on the producer queue. This queue is shared by all topics and partitions. This property has higher priority than queue.buffering.max.messages.
    /// </summary>
    /// <param name="queueBufferingMaxKbytes">
    ///     Maximum total message size sum allowed on the producer queue. This queue is shared by all topics and partitions. This property has higher priority than queue.buffering.max.messages.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithQueueBufferingMaxKbytes(int? queueBufferingMaxKbytes)
    {
        ClientConfig.QueueBufferingMaxKbytes = queueBufferingMaxKbytes;
        return This;
    }

    /// <summary>
    ///     Delay in milliseconds to wait for messages in the producer queue to accumulate before constructing message batches (MessageSets) to transmit to brokers. A higher value allows larger and more effective (less overhead, improved compression) batches of messages to accumulate at the expense of increased message delivery latency.
    /// </summary>
    /// <param name="lingerMs">
    ///     Delay in milliseconds to wait for messages in the producer queue to accumulate before constructing message batches (MessageSets) to transmit to brokers. A higher value allows larger and more effective (less overhead, improved compression) batches of messages to accumulate at the expense of increased message delivery latency.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithLingerMs(double? lingerMs)
    {
        ClientConfig.LingerMs = lingerMs;
        return This;
    }

    /// <summary>
    ///     How many times to retry sending a failing Message. **Note:** retrying may cause reordering unless `enable.idempotence` is set to true.
    /// </summary>
    /// <param name="messageSendMaxRetries">
    ///     How many times to retry sending a failing Message. **Note:** retrying may cause reordering unless `enable.idempotence` is set to true.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithMessageSendMaxRetries(int? messageSendMaxRetries)
    {
        ClientConfig.MessageSendMaxRetries = messageSendMaxRetries;
        return This;
    }

    /// <summary>
    ///     The backoff time in milliseconds before retrying a protocol request.
    /// </summary>
    /// <param name="retryBackoffMs">
    ///     The backoff time in milliseconds before retrying a protocol request.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithRetryBackoffMs(int? retryBackoffMs)
    {
        ClientConfig.RetryBackoffMs = retryBackoffMs;
        return This;
    }

    /// <summary>
    ///     The threshold of outstanding not yet transmitted broker requests needed to backpressure the producer's message accumulator. If the number of not yet transmitted requests equals or exceeds this number, produce request creation that would have otherwise been triggered (for example, in accordance with linger.ms) will be delayed. A lower number yields larger and more effective batches. A higher value can improve latency when using compression on slow machines.
    /// </summary>
    /// <param name="queueBufferingBackpressureThreshold">
    ///     The threshold of outstanding not yet transmitted broker requests needed to backpressure the producer's message accumulator. If the number of not yet transmitted requests equals or exceeds this number, produce request creation that would have otherwise been triggered (for example, in accordance with linger.ms) will be delayed. A lower number yields larger and more effective batches. A higher value can improve latency when using compression on slow machines.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithQueueBufferingBackpressureThreshold(int? queueBufferingBackpressureThreshold)
    {
        ClientConfig.QueueBufferingBackpressureThreshold = queueBufferingBackpressureThreshold;
        return This;
    }

    /// <summary>
    ///     compression codec to use for compressing message sets. This is the default value for all topics, may be overridden by the topic configuration property `compression.codec`.
    /// </summary>
    /// <param name="compressionType">
    ///     compression codec to use for compressing message sets. This is the default value for all topics, may be overridden by the topic configuration property `compression.codec`.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithCompressionType(CompressionType? compressionType)
    {
        ClientConfig.CompressionType = compressionType;
        return This;
    }

    /// <summary>
    ///     Maximum number of messages batched in one MessageSet. The total MessageSet size is also limited by batch.size and message.max.bytes.
    /// </summary>
    /// <param name="batchNumMessages">
    ///     Maximum number of messages batched in one MessageSet. The total MessageSet size is also limited by batch.size and message.max.bytes.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithBatchNumMessages(int? batchNumMessages)
    {
        ClientConfig.BatchNumMessages = batchNumMessages;
        return This;
    }

    /// <summary>
    ///     Maximum size (in bytes) of all messages batched in one MessageSet, including protocol framing overhead. This limit is applied after the first message has been added to the batch, regardless of the first message's size, this is to ensure that messages that exceed batch.size are produced. The total MessageSet size is also limited by batch.num.messages and message.max.bytes.
    /// </summary>
    /// <param name="batchSize">
    ///     Maximum size (in bytes) of all messages batched in one MessageSet, including protocol framing overhead. This limit is applied after the first message has been added to the batch, regardless of the first message's size, this is to ensure that messages that exceed batch.size are produced. The total MessageSet size is also limited by batch.num.messages and message.max.bytes.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithBatchSize(int? batchSize)
    {
        ClientConfig.BatchSize = batchSize;
        return This;
    }

    /// <summary>
    ///     Delay in milliseconds to wait to assign new sticky partitions for each topic. By default, set to double the time of linger.ms. To disable sticky behavior, set to 0. This behavior affects messages with the key NULL in all cases, and messages with key lengths of zero when the consistent_random partitioner is in use. These messages would otherwise be assigned randomly. A higher value allows for more effective batching of these messages.
    /// </summary>
    /// <param name="stickyPartitioningLingerMs">
    ///     Delay in milliseconds to wait to assign new sticky partitions for each topic. By default, set to double the time of linger.ms. To disable sticky behavior, set to 0. This behavior affects messages with the key NULL in all cases, and messages with key lengths of zero when the consistent_random partitioner is in use. These messages would otherwise be assigned randomly. A higher value allows for more effective batching of these messages.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    public KafkaProducerConfigurationBuilder WithStickyPartitioningLingerMs(int? stickyPartitioningLingerMs)
    {
        ClientConfig.StickyPartitioningLingerMs = stickyPartitioningLingerMs;
        return This;
    }

    /// <summary>
    ///     Specifies whether to enable notification of delivery reports. Typically
    ///     you should set this parameter to true. Set it to false for "fire and
    ///     forget" semantics and a small boost in performance.
    /// </summary>
    /// <param name="enableDeliveryReports">
    ///     Specifies whether to enable notification of delivery reports. Typically
    ///     you should set this parameter to true. Set it to false for "fire and
    ///     forget" semantics and a small boost in performance.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    internal KafkaProducerConfigurationBuilder WithEnableDeliveryReports(bool? enableDeliveryReports)
    {
        ClientConfig.EnableDeliveryReports = enableDeliveryReports;
        return This;
    }

    /// <summary>
    ///     When set to `true`, the producer will ensure that messages are successfully produced exactly once and in the original produce order. The following configuration properties are adjusted automatically (if not modified by the user) when idempotence is enabled: `max.in.flight.requests.per.connection=5` (must be less than or equal to 5), `retries=INT32_MAX` (must be greater than 0), `acks=all`, `queuing.strategy=fifo`. Producer instantation will fail if user-supplied configuration is incompatible.
    /// </summary>
    /// <param name="enableIdempotence">
    ///     When set to `true`, the producer will ensure that messages are successfully produced exactly once and in the original produce order. The following configuration properties are adjusted automatically (if not modified by the user) when idempotence is enabled: `max.in.flight.requests.per.connection=5` (must be less than or equal to 5), `retries=INT32_MAX` (must be greater than 0), `acks=all`, `queuing.strategy=fifo`. Producer instantation will fail if user-supplied configuration is incompatible.
    /// </param>
    /// <returns>
    ///     The client configuration builder so that additional calls can be chained.
    /// </returns>
    internal KafkaProducerConfigurationBuilder WithEnableIdempotence(bool? enableIdempotence)
    {
        ClientConfig.EnableIdempotence = enableIdempotence;
        return This;
    }
}
