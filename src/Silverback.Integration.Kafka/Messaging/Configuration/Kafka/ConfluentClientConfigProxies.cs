// Copyright (c) 2020 Sergio Aquilini
// This code is licensed under MIT license (see LICENSE file for details)

/*******************************************************************************************
  Note: These proxies are generated using Silverback.Tools.KafkaConfigClassGenerator
        located under /tools/
********************************************************************************************/

using System.Diagnostics.CodeAnalysis;
using Confluent.Kafka;
using Silverback.Util;

namespace Silverback.Messaging.Configuration.Kafka;

/// <summary>
///     Wraps the <see cref="Confluent.Kafka.ClientConfig" />.
/// </summary>
[SuppressMessage("", "SA1649", Justification = "Autogenerated all at once")]
[SuppressMessage("", "SA1402", Justification = "Autogenerated all at once")]
[SuppressMessage("", "CA1200", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1623", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1629", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "CA1044", Justification = "Accessors generated according to wrapped class")]
public abstract class ConfluentClientConfigProxy : IValidatableEndpointSettings
{
    internal static readonly ConfigurationDictionaryEqualityComparer<string, string>
        ConfluentConfigEqualityComparer = new();

    /// <summary>
    ///     Initializes a new instance of the <see cref="ConfluentClientConfigProxy" /> class.
    /// </summary>
    /// <param name="confluentConfig">
    ///     The <see cref="Confluent.Kafka.ClientConfig" /> to wrap.
    /// </param>
    protected ConfluentClientConfigProxy(ClientConfig confluentConfig)
    {
        ConfluentConfig = confluentConfig;
    }

    /// <summary>
    ///     SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512.
    ///     **NOTE**: Despite the name, you may not configure more than one mechanism.
    /// </summary>
    public SaslMechanism? SaslMechanism
    {
        get => ConfluentConfig.SaslMechanism;
        set => ConfluentConfig.SaslMechanism = value;
    }

    /// <summary>
    ///     This field indicates the number of acknowledgements the leader broker must receive from ISR brokers
    ///     before responding to the request: Zero=Broker does not send any response/ack to client, One=The
    ///     leader will write the record to its local log but will respond without awaiting full acknowledgement
    ///     from all followers. All=Broker will block until message is committed by all in sync replicas (ISRs).
    ///     If there are less than min.insync.replicas (broker configuration) in the ISR set the produce request
    ///     will fail.
    /// </summary>
    public Acks? Acks
    {
        get => ConfluentConfig.Acks;
        set => ConfluentConfig.Acks = value;
    }

    /// <summary>
    ///     Client identifier.
    ///     <br /><br />default: rdkafka
    ///     <br />importance: low
    /// </summary>
    public string ClientId
    {
        get => ConfluentConfig.ClientId;
        set => ConfluentConfig.ClientId = value;
    }

    /// <summary>
    ///     Initial list of brokers as a CSV list of broker host or host:port. The application may also use
    ///     `rd_kafka_brokers_add()` to add brokers during runtime.
    ///     <br /><br />default: ''
    ///     <br />importance: high
    /// </summary>
    public string BootstrapServers
    {
        get => ConfluentConfig.BootstrapServers;
        set => ConfluentConfig.BootstrapServers = value;
    }

    /// <summary>
    ///     Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions
    ///     the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the
    ///     maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic's
    ///     `max.message.bytes` limit (see Apache Kafka documentation).
    ///     <br /><br />default: 1000000
    ///     <br />importance: medium
    /// </summary>
    public int? MessageMaxBytes
    {
        get => ConfluentConfig.MessageMaxBytes;
        set => ConfluentConfig.MessageMaxBytes = value;
    }

    /// <summary>
    ///     Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference
    ///     (zero-copy) at the expense of larger iovecs.
    ///     <br /><br />default: 65535
    ///     <br />importance: low
    /// </summary>
    public int? MessageCopyMaxBytes
    {
        get => ConfluentConfig.MessageCopyMaxBytes;
        set => ConfluentConfig.MessageCopyMaxBytes = value;
    }

    /// <summary>
    ///     Maximum Kafka protocol response message size. This serves as a safety precaution to avoid memory
    ///     exhaustion in case of protocol hickups. This value must be at least `fetch.max.bytes`  + 512 to allow for
    ///     protocol overhead; the value is adjusted automatically unless the configuration property is explicitly
    ///     set.
    ///     <br /><br />default: 100000000
    ///     <br />importance: medium
    /// </summary>
    public int? ReceiveMessageMaxBytes
    {
        get => ConfluentConfig.ReceiveMessageMaxBytes;
        set => ConfluentConfig.ReceiveMessageMaxBytes = value;
    }

    /// <summary>
    ///     Maximum number of in-flight requests per broker connection. This is a generic property applied to all
    ///     broker communication, however it is primarily relevant to produce requests. In particular, note that
    ///     other mechanisms limit the number of outstanding consumer fetch request per broker to one.
    ///     <br /><br />default: 1000000
    ///     <br />importance: low
    /// </summary>
    public int? MaxInFlight
    {
        get => ConfluentConfig.MaxInFlight;
        set => ConfluentConfig.MaxInFlight = value;
    }

    /// <summary>
    ///     Period of time in milliseconds at which topic and broker metadata is refreshed in order to proactively
    ///     discover any new brokers, topics, partitions or partition leader changes. Use -1 to disable the
    ///     intervalled refresh (not recommended). If there are no locally referenced topics (no topic objects
    ///     created, no messages produced, no subscription or no assignment) then only the broker list will be
    ///     refreshed every interval but no more often than every 10s.
    ///     <br /><br />default: 300000
    ///     <br />importance: low
    /// </summary>
    public int? TopicMetadataRefreshIntervalMs
    {
        get => ConfluentConfig.TopicMetadataRefreshIntervalMs;
        set => ConfluentConfig.TopicMetadataRefreshIntervalMs = value;
    }

    /// <summary>
    ///     Metadata cache max age. Defaults to topic.metadata.refresh.interval.ms * 3
    ///     <br /><br />default: 900000
    ///     <br />importance: low
    /// </summary>
    public int? MetadataMaxAgeMs
    {
        get => ConfluentConfig.MetadataMaxAgeMs;
        set => ConfluentConfig.MetadataMaxAgeMs = value;
    }

    /// <summary>
    ///     When a topic loses its leader a new metadata request will be enqueued with this initial interval,
    ///     exponentially increasing until the topic metadata has been refreshed. This is used to recover quickly
    ///     from transitioning leader brokers.
    ///     <br /><br />default: 250
    ///     <br />importance: low
    /// </summary>
    public int? TopicMetadataRefreshFastIntervalMs
    {
        get => ConfluentConfig.TopicMetadataRefreshFastIntervalMs;
        set => ConfluentConfig.TopicMetadataRefreshFastIntervalMs = value;
    }

    /// <summary>
    ///     Sparse metadata requests (consumes less network bandwidth)
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? TopicMetadataRefreshSparse
    {
        get => ConfluentConfig.TopicMetadataRefreshSparse;
        set => ConfluentConfig.TopicMetadataRefreshSparse = value;
    }

    /// <summary>
    ///     Apache Kafka topic creation is asynchronous and it takes some time for a new topic to propagate
    ///     throughout the cluster to all brokers. If a client requests topic metadata after manual topic creation
    ///     but before the topic has been fully propagated to the broker the client is requesting metadata from, the
    ///     topic will seem to be non-existent and the client will mark the topic as such, failing queued produced
    ///     messages with `ERR__UNKNOWN_TOPIC`. This setting delays marking a topic as non-existent until the
    ///     configured propagation max time has passed. The maximum propagation time is calculated from the time the
    ///     topic is first referenced in the client, e.g., on produce().
    ///     <br /><br />default: 30000
    ///     <br />importance: low
    /// </summary>
    public int? TopicMetadataPropagationMaxMs
    {
        get => ConfluentConfig.TopicMetadataPropagationMaxMs;
        set => ConfluentConfig.TopicMetadataPropagationMaxMs = value;
    }

    /// <summary>
    ///     Topic blacklist, a comma-separated list of regular expressions for matching topic names that should be
    ///     ignored in broker metadata information as if the topics did not exist.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string TopicBlacklist
    {
        get => ConfluentConfig.TopicBlacklist;
        set => ConfluentConfig.TopicBlacklist = value;
    }

    /// <summary>
    ///     A comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg.
    ///     Consumer: consumer,cgrp,topic,fetch
    ///     <br /><br />default: ''
    ///     <br />importance: medium
    /// </summary>
    public string Debug
    {
        get => ConfluentConfig.Debug;
        set => ConfluentConfig.Debug = value;
    }

    /// <summary>
    ///     Default timeout for network requests. Producer: ProduceRequests will use the lesser value of
    ///     `socket.timeout.ms` and remaining `message.timeout.ms` for the first message in the batch. Consumer:
    ///     FetchRequests will use `fetch.wait.max.ms` + `socket.timeout.ms`. Admin: Admin requests will use
    ///     `socket.timeout.ms` or explicitly set `rd_kafka_AdminOptions_set_operation_timeout()` value.
    ///     <br /><br />default: 60000
    ///     <br />importance: low
    /// </summary>
    public int? SocketTimeoutMs
    {
        get => ConfluentConfig.SocketTimeoutMs;
        set => ConfluentConfig.SocketTimeoutMs = value;
    }

    /// <summary>
    ///     Broker socket send buffer size. System default is used if 0.
    ///     <br /><br />default: 0
    ///     <br />importance: low
    /// </summary>
    public int? SocketSendBufferBytes
    {
        get => ConfluentConfig.SocketSendBufferBytes;
        set => ConfluentConfig.SocketSendBufferBytes = value;
    }

    /// <summary>
    ///     Broker socket receive buffer size. System default is used if 0.
    ///     <br /><br />default: 0
    ///     <br />importance: low
    /// </summary>
    public int? SocketReceiveBufferBytes
    {
        get => ConfluentConfig.SocketReceiveBufferBytes;
        set => ConfluentConfig.SocketReceiveBufferBytes = value;
    }

    /// <summary>
    ///     Enable TCP keep-alives (SO_KEEPALIVE) on broker sockets
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? SocketKeepaliveEnable
    {
        get => ConfluentConfig.SocketKeepaliveEnable;
        set => ConfluentConfig.SocketKeepaliveEnable = value;
    }

    /// <summary>
    ///     Disable the Nagle algorithm (TCP_NODELAY) on broker sockets.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? SocketNagleDisable
    {
        get => ConfluentConfig.SocketNagleDisable;
        set => ConfluentConfig.SocketNagleDisable = value;
    }

    /// <summary>
    ///     Disconnect from broker when this number of send failures (e.g., timed out requests) is reached. Disable
    ///     with 0. WARNING: It is highly recommended to leave this setting at its default value of 1 to avoid the
    ///     client and broker to become desynchronized in case of request timeouts. NOTE: The connection is
    ///     automatically re-established.
    ///     <br /><br />default: 1
    ///     <br />importance: low
    /// </summary>
    public int? SocketMaxFails
    {
        get => ConfluentConfig.SocketMaxFails;
        set => ConfluentConfig.SocketMaxFails = value;
    }

    /// <summary>
    ///     How long to cache the broker address resolving results (milliseconds).
    ///     <br /><br />default: 1000
    ///     <br />importance: low
    /// </summary>
    public int? BrokerAddressTtl
    {
        get => ConfluentConfig.BrokerAddressTtl;
        set => ConfluentConfig.BrokerAddressTtl = value;
    }

    /// <summary>
    ///     Allowed broker IP address families: any, v4, v6
    ///     <br /><br />default: any
    ///     <br />importance: low
    /// </summary>
    public BrokerAddressFamily? BrokerAddressFamily
    {
        get => ConfluentConfig.BrokerAddressFamily;
        set => ConfluentConfig.BrokerAddressFamily = value;
    }

    /// <summary>
    ///     Close broker connections after the specified time of inactivity. Disable with 0. If this property is left
    ///     at its default value some heuristics are performed to determine a suitable default value, this is
    ///     currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info).
    ///     <br /><br />default: 0
    ///     <br />importance: medium
    /// </summary>
    public int? ConnectionsMaxIdleMs
    {
        get => ConfluentConfig.ConnectionsMaxIdleMs;
        set => ConfluentConfig.ConnectionsMaxIdleMs = value;
    }

    /// <summary>
    ///     The initial time to wait before reconnecting to a broker after the connection has been closed. The time
    ///     is increased exponentially until `reconnect.backoff.max.ms` is reached. -25% to +50% jitter is applied to
    ///     each reconnect backoff. A value of 0 disables the backoff and reconnects immediately.
    ///     <br /><br />default: 100
    ///     <br />importance: medium
    /// </summary>
    public int? ReconnectBackoffMs
    {
        get => ConfluentConfig.ReconnectBackoffMs;
        set => ConfluentConfig.ReconnectBackoffMs = value;
    }

    /// <summary>
    ///     The maximum time to wait before reconnecting to a broker after the connection has been closed.
    ///     <br /><br />default: 10000
    ///     <br />importance: medium
    /// </summary>
    public int? ReconnectBackoffMaxMs
    {
        get => ConfluentConfig.ReconnectBackoffMaxMs;
        set => ConfluentConfig.ReconnectBackoffMaxMs = value;
    }

    /// <summary>
    ///     librdkafka statistics emit interval. The application also needs to register a stats callback using
    ///     `rd_kafka_conf_set_stats_cb()`. The granularity is 1000ms. A value of 0 disables statistics.
    ///     <br /><br />default: 0
    ///     <br />importance: high
    /// </summary>
    public int? StatisticsIntervalMs
    {
        get => ConfluentConfig.StatisticsIntervalMs;
        set => ConfluentConfig.StatisticsIntervalMs = value;
    }

    /// <summary>
    ///     Disable spontaneous log_cb from internal librdkafka threads, instead enqueue log messages on queue set
    ///     with `rd_kafka_set_log_queue()` and serve log callbacks or events through the standard poll APIs.
    ///     **NOTE**: Log messages will linger in a temporary queue until the log queue has been set.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? LogQueue
    {
        get => ConfluentConfig.LogQueue;
        set => ConfluentConfig.LogQueue = value;
    }

    /// <summary>
    ///     Print internal thread name in log messages (useful for debugging librdkafka internals)
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? LogThreadName
    {
        get => ConfluentConfig.LogThreadName;
        set => ConfluentConfig.LogThreadName = value;
    }

    /// <summary>
    ///     If enabled librdkafka will initialize the PRNG with srand(current_time.milliseconds) on the first
    ///     invocation of rd_kafka_new() (required only if rand_r() is not available on your platform). If disabled
    ///     the application must call srand() prior to calling rd_kafka_new().
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? EnableRandomSeed
    {
        get => ConfluentConfig.EnableRandomSeed;
        set => ConfluentConfig.EnableRandomSeed = value;
    }

    /// <summary>
    ///     Log broker disconnects. It might be useful to turn this off when interacting with 0.9 brokers with an
    ///     aggressive `connection.max.idle.ms` value.
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? LogConnectionClose
    {
        get => ConfluentConfig.LogConnectionClose;
        set => ConfluentConfig.LogConnectionClose = value;
    }

    /// <summary>
    ///     Signal that librdkafka will use to quickly terminate on rd_kafka_destroy(). If this signal is not set
    ///     then there will be a delay before rd_kafka_wait_destroyed() returns true as internal threads are timing
    ///     out their system calls. If this signal is set however the delay will be minimal. The application should
    ///     mask this signal as an internal signal handler is installed.
    ///     <br /><br />default: 0
    ///     <br />importance: low
    /// </summary>
    public int? InternalTerminationSignal
    {
        get => ConfluentConfig.InternalTerminationSignal;
        set => ConfluentConfig.InternalTerminationSignal = value;
    }

    /// <summary>
    ///     Request broker's supported API versions to adjust functionality to available protocol features. If set to
    ///     false, or the ApiVersionRequest fails, the fallback version `broker.version.fallback` will be used.
    ///     **NOTE**: Depends on broker version &gt;=0.10.0. If the request is not supported by (an older) broker the
    ///     `broker.version.fallback` fallback is used.
    ///     <br /><br />default: true
    ///     <br />importance: high
    /// </summary>
    public bool? ApiVersionRequest
    {
        get => ConfluentConfig.ApiVersionRequest;
        set => ConfluentConfig.ApiVersionRequest = value;
    }

    /// <summary>
    ///     Timeout for broker API version requests.
    ///     <br /><br />default: 10000
    ///     <br />importance: low
    /// </summary>
    public int? ApiVersionRequestTimeoutMs
    {
        get => ConfluentConfig.ApiVersionRequestTimeoutMs;
        set => ConfluentConfig.ApiVersionRequestTimeoutMs = value;
    }

    /// <summary>
    ///     Dictates how long the `broker.version.fallback` fallback is used in the case the ApiVersionRequest fails.
    ///     **NOTE**: The ApiVersionRequest is only issued when a new connection to the broker is made (such as after
    ///     an upgrade).
    ///     <br /><br />default: 0
    ///     <br />importance: medium
    /// </summary>
    public int? ApiVersionFallbackMs
    {
        get => ConfluentConfig.ApiVersionFallbackMs;
        set => ConfluentConfig.ApiVersionFallbackMs = value;
    }

    /// <summary>
    ///     Older broker versions (before 0.10.0) provide no way for a client to query for supported protocol
    ///     features (ApiVersionRequest, see `api.version.request`) making it impossible for the client to know what
    ///     features it may use. As a workaround a user may set this property to the expected broker version and the
    ///     client will automatically adjust its feature set accordingly if the ApiVersionRequest fails (or is
    ///     disabled). The fallback broker version will be used for `api.version.fallback.ms`. Valid values are:
    ///     0.9.0, 0.8.2, 0.8.1, 0.8.0. Any other value &gt;= 0.10, such as 0.10.2.1, enables ApiVersionRequests.
    ///     <br /><br />default: 0.10.0
    ///     <br />importance: medium
    /// </summary>
    public string BrokerVersionFallback
    {
        get => ConfluentConfig.BrokerVersionFallback;
        set => ConfluentConfig.BrokerVersionFallback = value;
    }

    /// <summary>
    ///     Protocol used to communicate with brokers.
    ///     <br /><br />default: plaintext
    ///     <br />importance: high
    /// </summary>
    public SecurityProtocol? SecurityProtocol
    {
        get => ConfluentConfig.SecurityProtocol;
        set => ConfluentConfig.SecurityProtocol = value;
    }

    /// <summary>
    ///     A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used
    ///     to negotiate the security settings for a network connection using TLS or SSL network protocol. See manual
    ///     page for `ciphers(1)` and `SSL_CTX_set_cipher_list(3).
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslCipherSuites
    {
        get => ConfluentConfig.SslCipherSuites;
        set => ConfluentConfig.SslCipherSuites = value;
    }

    /// <summary>
    ///     The supported-curves extension in the TLS ClientHello message specifies the curves (standard/named, or
    ///     'explicit' GF(2^k) or GF(p)) the client is willing to have the server use. See manual page for
    ///     `SSL_CTX_set1_curves_list(3)`. OpenSSL &gt;= 1.0.2 required.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslCurvesList
    {
        get => ConfluentConfig.SslCurvesList;
        set => ConfluentConfig.SslCurvesList = value;
    }

    /// <summary>
    ///     The client uses the TLS ClientHello signature_algorithms extension to indicate to the server which
    ///     signature/hash algorithm pairs may be used in digital signatures. See manual page for
    ///     `SSL_CTX_set1_sigalgs_list(3)`. OpenSSL &gt;= 1.0.2 required.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslSigalgsList
    {
        get => ConfluentConfig.SslSigalgsList;
        set => ConfluentConfig.SslSigalgsList = value;
    }

    /// <summary>
    ///     Path to client's private key (PEM) used for authentication.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslKeyLocation
    {
        get => ConfluentConfig.SslKeyLocation;
        set => ConfluentConfig.SslKeyLocation = value;
    }

    /// <summary>
    ///     Private key passphrase (for use with `ssl.key.location` and `set_ssl_cert()`)
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslKeyPassword
    {
        get => ConfluentConfig.SslKeyPassword;
        set => ConfluentConfig.SslKeyPassword = value;
    }

    /// <summary>
    ///     Client's private key string (PEM format) used for authentication.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslKeyPem
    {
        get => ConfluentConfig.SslKeyPem;
        set => ConfluentConfig.SslKeyPem = value;
    }

    /// <summary>
    ///     Path to client's public key (PEM) used for authentication.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslCertificateLocation
    {
        get => ConfluentConfig.SslCertificateLocation;
        set => ConfluentConfig.SslCertificateLocation = value;
    }

    /// <summary>
    ///     Client's public key string (PEM format) used for authentication.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslCertificatePem
    {
        get => ConfluentConfig.SslCertificatePem;
        set => ConfluentConfig.SslCertificatePem = value;
    }

    /// <summary>
    ///     File or directory path to CA certificate(s) for verifying the broker's key. Defaults: On Windows the
    ///     system's CA certificates are automatically looked up in the Windows Root certificate store. On Mac OSX
    ///     this configuration defaults to `probe`. It is recommended to install openssl using Homebrew, to provide
    ///     CA certificates. On Linux install the distribution's ca-certificates package. If OpenSSL is statically
    ///     linked or `ssl.ca.location` is set to `probe` a list of standard paths will be probed and the first one
    ///     found will be used as the default CA certificate location path. If OpenSSL is dynamically linked the
    ///     OpenSSL library's default path will be used (see `OPENSSLDIR` in `openssl version -a`).
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslCaLocation
    {
        get => ConfluentConfig.SslCaLocation;
        set => ConfluentConfig.SslCaLocation = value;
    }

    /// <summary>
    ///     Comma-separated list of Windows Certificate stores to load CA certificates from. Certificates will be
    ///     loaded in the same order as stores are specified. If no certificates can be loaded from any of the
    ///     specified stores an error is logged and the OpenSSL library's default CA location is used instead. Store
    ///     names are typically one or more of: MY, Root, Trust, CA.
    ///     <br /><br />default: Root
    ///     <br />importance: low
    /// </summary>
    public string SslCaCertificateStores
    {
        get => ConfluentConfig.SslCaCertificateStores;
        set => ConfluentConfig.SslCaCertificateStores = value;
    }

    /// <summary>
    ///     Path to CRL for verifying broker's certificate validity.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslCrlLocation
    {
        get => ConfluentConfig.SslCrlLocation;
        set => ConfluentConfig.SslCrlLocation = value;
    }

    /// <summary>
    ///     Path to client's keystore (PKCS#12) used for authentication.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslKeystoreLocation
    {
        get => ConfluentConfig.SslKeystoreLocation;
        set => ConfluentConfig.SslKeystoreLocation = value;
    }

    /// <summary>
    ///     Client's keystore (PKCS#12) password.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslKeystorePassword
    {
        get => ConfluentConfig.SslKeystorePassword;
        set => ConfluentConfig.SslKeystorePassword = value;
    }

    /// <summary>
    ///     Path to OpenSSL engine library. OpenSSL &gt;= 1.1.0 required.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SslEngineLocation
    {
        get => ConfluentConfig.SslEngineLocation;
        set => ConfluentConfig.SslEngineLocation = value;
    }

    /// <summary>
    ///     OpenSSL engine id is the name used for loading engine.
    ///     <br /><br />default: dynamic
    ///     <br />importance: low
    /// </summary>
    public string SslEngineId
    {
        get => ConfluentConfig.SslEngineId;
        set => ConfluentConfig.SslEngineId = value;
    }

    /// <summary>
    ///     Enable OpenSSL's builtin broker (server) certificate verification. This verification can be extended by
    ///     the application by implementing a certificate_verify_cb.
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? EnableSslCertificateVerification
    {
        get => ConfluentConfig.EnableSslCertificateVerification;
        set => ConfluentConfig.EnableSslCertificateVerification = value;
    }

    /// <summary>
    ///     Endpoint identification algorithm to validate broker hostname using broker certificate. https - Server
    ///     (broker) hostname verification as specified in RFC2818. none - No endpoint verification. OpenSSL &gt;=
    ///     1.0.2 required.
    ///     <br /><br />default: none
    ///     <br />importance: low
    /// </summary>
    public SslEndpointIdentificationAlgorithm? SslEndpointIdentificationAlgorithm
    {
        get => ConfluentConfig.SslEndpointIdentificationAlgorithm;
        set => ConfluentConfig.SslEndpointIdentificationAlgorithm = value;
    }

    /// <summary>
    ///     Kerberos principal name that Kafka runs as, not including /hostname@REALM
    ///     <br /><br />default: kafka
    ///     <br />importance: low
    /// </summary>
    public string SaslKerberosServiceName
    {
        get => ConfluentConfig.SaslKerberosServiceName;
        set => ConfluentConfig.SaslKerberosServiceName = value;
    }

    /// <summary>
    ///     This client's Kerberos principal name. (Not supported on Windows, will use the logon user's principal).
    ///     <br /><br />default: kafkaclient
    ///     <br />importance: low
    /// </summary>
    public string SaslKerberosPrincipal
    {
        get => ConfluentConfig.SaslKerberosPrincipal;
        set => ConfluentConfig.SaslKerberosPrincipal = value;
    }

    /// <summary>
    ///     Shell command to refresh or acquire the client's Kerberos ticket. This command is executed on client
    ///     creation and every sasl.kerberos.min.time.before.relogin (0=disable). %{config.prop.name} is replaced by
    ///     corresponding config object value.
    ///     <br /><br />default: kinit -R -t "%{sasl.kerberos.keytab}" -k %{sasl.kerberos.principal} || kinit -t
    ///     "%{sasl.kerberos.keytab}" -k %{sasl.kerberos.principal}
    ///     <br />importance: low
    /// </summary>
    public string SaslKerberosKinitCmd
    {
        get => ConfluentConfig.SaslKerberosKinitCmd;
        set => ConfluentConfig.SaslKerberosKinitCmd = value;
    }

    /// <summary>
    ///     Path to Kerberos keytab file. This configuration property is only used as a variable in
    ///     `sasl.kerberos.kinit.cmd` as ` ... -t "%{sasl.kerberos.keytab}"`.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SaslKerberosKeytab
    {
        get => ConfluentConfig.SaslKerberosKeytab;
        set => ConfluentConfig.SaslKerberosKeytab = value;
    }

    /// <summary>
    ///     Minimum time in milliseconds between key refresh attempts. Disable automatic key refresh by setting this
    ///     property to 0.
    ///     <br /><br />default: 60000
    ///     <br />importance: low
    /// </summary>
    public int? SaslKerberosMinTimeBeforeRelogin
    {
        get => ConfluentConfig.SaslKerberosMinTimeBeforeRelogin;
        set => ConfluentConfig.SaslKerberosMinTimeBeforeRelogin = value;
    }

    /// <summary>
    ///     SASL username for use with the PLAIN and SASL-SCRAM-.. mechanisms
    ///     <br /><br />default: ''
    ///     <br />importance: high
    /// </summary>
    public string SaslUsername
    {
        get => ConfluentConfig.SaslUsername;
        set => ConfluentConfig.SaslUsername = value;
    }

    /// <summary>
    ///     SASL password for use with the PLAIN and SASL-SCRAM-.. mechanism
    ///     <br /><br />default: ''
    ///     <br />importance: high
    /// </summary>
    public string SaslPassword
    {
        get => ConfluentConfig.SaslPassword;
        set => ConfluentConfig.SaslPassword = value;
    }

    /// <summary>
    ///     SASL/OAUTHBEARER configuration. The format is implementation-dependent and must be parsed accordingly.
    ///     The default unsecured token implementation (see https://tools.ietf.org/html/rfc7515#appendix-A.5)
    ///     recognizes space-separated name=value pairs with valid names including principalClaimName, principal,
    ///     scopeClaimName, scope, and lifeSeconds. The default value for principalClaimName is "sub", the default
    ///     value for scopeClaimName is "scope", and the default value for lifeSeconds is 3600. The scope value is
    ///     CSV format with the default value being no/empty scope. For example: `principalClaimName=azp
    ///     principal=admin scopeClaimName=roles scope=role1,role2 lifeSeconds=600`. In addition, SASL extensions can
    ///     be communicated to the broker via `extension_NAME=value`. For example: `principal=admin
    ///     extension_traceId=123`
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string SaslOauthbearerConfig
    {
        get => ConfluentConfig.SaslOauthbearerConfig;
        set => ConfluentConfig.SaslOauthbearerConfig = value;
    }

    /// <summary>
    ///     Enable the builtin unsecure JWT OAUTHBEARER token handler if no oauthbearer_refresh_cb has been set. This
    ///     builtin handler should only be used for development or testing, and not in production.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? EnableSaslOauthbearerUnsecureJwt
    {
        get => ConfluentConfig.EnableSaslOauthbearerUnsecureJwt;
        set => ConfluentConfig.EnableSaslOauthbearerUnsecureJwt = value;
    }

    /// <summary>
    ///     List of plugin libraries to load (; separated). The library search path is platform dependent (see
    ///     dlopen(3) for Unix and LoadLibrary() for Windows). If no filename extension is specified the
    ///     platform-specific extension (such as .dll or .so) will be appended automatically.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string PluginLibraryPaths
    {
        get => ConfluentConfig.PluginLibraryPaths;
        set => ConfluentConfig.PluginLibraryPaths = value;
    }

    /// <summary>
    ///     A rack identifier for this client. This can be any string value which indicates where this client is
    ///     physically located. It corresponds with the broker config `broker.rack`.
    ///     <br /><br />default: ''
    ///     <br />importance: low
    /// </summary>
    public string ClientRack
    {
        get => ConfluentConfig.ClientRack;
        set => ConfluentConfig.ClientRack = value;
    }

    /// <summary>
    ///     The maximum length of time (in milliseconds) before a cancellation request
    ///     is acted on. Low values may result in measurably higher CPU usage.
    ///     <br /><br />default: 100
    ///     range: 1 &lt;= dotnet.cancellation.delay.max.ms &lt;= 10000
    ///     <br />importance: low
    /// </summary>
    public int CancellationDelayMaxMs
    {
        set => ConfluentConfig.CancellationDelayMaxMs = value;
    }

    /// <summary>
    ///     Gets the <see cref="Confluent.Kafka.ClientConfig" /> instance being wrapped.
    /// </summary>
    protected ClientConfig ConfluentConfig { get; }

    /// <inheritdoc cref="IValidatableEndpointSettings.Validate" />
    public abstract void Validate();

    internal ClientConfig GetConfluentConfig() => ConfluentConfig;
}

/// <summary>
///     Wraps the <see cref="Confluent.Kafka.ConsumerConfig" />.
/// </summary>
[SuppressMessage("", "SA1649", Justification = "Autogenerated all at once")]
[SuppressMessage("", "SA1402", Justification = "Autogenerated all at once")]
[SuppressMessage("", "CA1200", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1623", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1629", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "CA1044", Justification = "Accessors generated according to wrapped class")]
public abstract class ConfluentConsumerConfigProxy : ConfluentClientConfigProxy
{
    /// <summary>
    ///     Initializes a new instance of the <see cref="ConfluentConsumerConfigProxy" /> class.
    /// </summary>
    /// <param name="clientConfig">
    ///     The <see cref="Confluent.Kafka.ClientConfig" /> to be used to initialize the
    ///     <see cref="Confluent.Kafka.ConsumerConfig" />.
    /// </param>
    protected ConfluentConsumerConfigProxy(ClientConfig? clientConfig = null)
        : base(clientConfig != null ? new ConsumerConfig(clientConfig.Clone()) : new ConsumerConfig())
    {
    }

    /// <summary>
    ///     A comma separated list of fields that may be optionally set
    ///     in <see cref="T:Confluent.Kafka.ConsumeResult`2" />
    ///     objects returned by the
    ///     <see cref="M:Confluent.Kafka.Consumer`2.Consume(System.TimeSpan)" />
    ///     method. Disabling fields that you do not require will improve
    ///     throughput and reduce memory consumption. Allowed values:
    ///     headers, timestamp, topic, all, none
    ///     <br /><br />default: all
    ///     <br />importance: low
    /// </summary>
    public string ConsumeResultFields
    {
        set => ConfluentConfig.ConsumeResultFields = value;
    }

    /// <summary>
    ///     Action to take when there is no initial offset in offset store or the desired offset is out of range:
    ///     'smallest','earliest' - automatically reset the offset to the smallest offset, 'largest','latest' -
    ///     automatically reset the offset to the largest offset, 'error' - trigger an error (ERR__AUTO_OFFSET_RESET)
    ///     which is retrieved by consuming messages and checking 'message-&gt;err'.
    ///     <br /><br />default: largest
    ///     <br />importance: high
    /// </summary>
    public AutoOffsetReset? AutoOffsetReset
    {
        get => ConfluentConfig.AutoOffsetReset;
        set => ConfluentConfig.AutoOffsetReset = value;
    }

    /// <summary>
    ///     Client group id string. All clients sharing the same group.id belong to the same group.
    ///     <br /><br />default: ''
    ///     <br />importance: high
    /// </summary>
    public string GroupId
    {
        get => ConfluentConfig.GroupId;
        set => ConfluentConfig.GroupId = value;
    }

    /// <summary>
    ///     Enable static group membership. Static group members are able to leave and rejoin a group within the
    ///     configured `session.timeout.ms` without prompting a group rebalance. This should be used in combination
    ///     with a larger `session.timeout.ms` to avoid group rebalances caused by transient unavailability (e.g.
    ///     process restarts). Requires broker version &gt;= 2.3.0.
    ///     <br /><br />default: ''
    ///     <br />importance: medium
    /// </summary>
    public string GroupInstanceId
    {
        get => ConfluentConfig.GroupInstanceId;
        set => ConfluentConfig.GroupInstanceId = value;
    }

    /// <summary>
    ///     The name of one or more partition assignment strategies. The elected group leader will use a strategy
    ///     supported by all members of the group to assign partitions to group members. If there is more than one
    ///     eligible strategy, preference is determined by the order of this list (strategies earlier in the list
    ///     have higher priority). Cooperative and non-cooperative (eager) strategies must not be mixed. Available
    ///     strategies: range, roundrobin, cooperative-sticky.
    ///     <br /><br />default: range,roundrobin
    ///     <br />importance: medium
    /// </summary>
    public PartitionAssignmentStrategy? PartitionAssignmentStrategy
    {
        get => ConfluentConfig.PartitionAssignmentStrategy;
        set => ConfluentConfig.PartitionAssignmentStrategy = value;
    }

    /// <summary>
    ///     Client group session and failure detection timeout. The consumer sends periodic heartbeats
    ///     (heartbeat.interval.ms) to indicate its liveness to the broker. If no hearts are received by the broker
    ///     for a group member within the session timeout, the broker will remove the consumer from the group and
    ///     trigger a rebalance. The allowed range is configured with the **broker** configuration properties
    ///     `group.min.session.timeout.ms` and `group.max.session.timeout.ms`. Also see `max.poll.interval.ms`.
    ///     <br /><br />default: 45000
    ///     <br />importance: high
    /// </summary>
    public int? SessionTimeoutMs
    {
        get => ConfluentConfig.SessionTimeoutMs;
        set => ConfluentConfig.SessionTimeoutMs = value;
    }

    /// <summary>
    ///     Group session keepalive heartbeat interval.
    ///     <br /><br />default: 3000
    ///     <br />importance: low
    /// </summary>
    public int? HeartbeatIntervalMs
    {
        get => ConfluentConfig.HeartbeatIntervalMs;
        set => ConfluentConfig.HeartbeatIntervalMs = value;
    }

    /// <summary>
    ///     Group protocol type. NOTE: Currently, the only supported group protocol type is `consumer`.
    ///     <br /><br />default: consumer
    ///     <br />importance: low
    /// </summary>
    public string GroupProtocolType
    {
        get => ConfluentConfig.GroupProtocolType;
        set => ConfluentConfig.GroupProtocolType = value;
    }

    /// <summary>
    ///     How often to query for the current client group coordinator. If the currently assigned coordinator is
    ///     down the configured query interval will be divided by ten to more quickly recover in case of coordinator
    ///     reassignment.
    ///     <br /><br />default: 600000
    ///     <br />importance: low
    /// </summary>
    public int? CoordinatorQueryIntervalMs
    {
        get => ConfluentConfig.CoordinatorQueryIntervalMs;
        set => ConfluentConfig.CoordinatorQueryIntervalMs = value;
    }

    /// <summary>
    ///     Maximum allowed time between calls to consume messages (e.g., rd_kafka_consumer_poll()) for high-level
    ///     consumers. If this interval is exceeded the consumer is considered failed and the group will rebalance in
    ///     order to reassign the partitions to another consumer group member. Warning: Offset commits may be not
    ///     possible at this point. Note: It is recommended to set `enable.auto.offset.store=false` for long-time
    ///     processing applications and then explicitly store offsets (using offsets_store()) *after* message
    ///     processing, to make sure offsets are not auto-committed prior to processing has finished. The interval is
    ///     checked two times per second. See KIP-62 for more information.
    ///     <br /><br />default: 300000
    ///     <br />importance: high
    /// </summary>
    public int? MaxPollIntervalMs
    {
        get => ConfluentConfig.MaxPollIntervalMs;
        set => ConfluentConfig.MaxPollIntervalMs = value;
    }

    /// <summary>
    ///     Automatically and periodically commit offsets in the background. Note: setting this to false does not
    ///     prevent the consumer from fetching previously committed start offsets. To circumvent this behaviour set
    ///     specific start offsets per partition in the call to assign().
    ///     <br /><br />default: true
    ///     <br />importance: high
    /// </summary>
    public bool? EnableAutoCommit
    {
        get => ConfluentConfig.EnableAutoCommit;
        set => ConfluentConfig.EnableAutoCommit = value;
    }

    /// <summary>
    ///     The frequency in milliseconds that the consumer offsets are committed (written) to offset storage. (0 =
    ///     disable). This setting is used by the high-level consumer.
    ///     <br /><br />default: 5000
    ///     <br />importance: medium
    /// </summary>
    public int? AutoCommitIntervalMs
    {
        get => ConfluentConfig.AutoCommitIntervalMs;
        set => ConfluentConfig.AutoCommitIntervalMs = value;
    }

    /// <summary>
    ///     Automatically store offset of last message provided to application. The offset store is an in-memory
    ///     store of the next offset to (auto-)commit for each partition.
    ///     <br /><br />default: true
    ///     <br />importance: high
    /// </summary>
    public bool? EnableAutoOffsetStore
    {
        get => ConfluentConfig.EnableAutoOffsetStore;
        set => ConfluentConfig.EnableAutoOffsetStore = value;
    }

    /// <summary>
    ///     Minimum number of messages per topic+partition librdkafka tries to maintain in the local consumer queue.
    ///     <br /><br />default: 100000
    ///     <br />importance: medium
    /// </summary>
    public int? QueuedMinMessages
    {
        get => ConfluentConfig.QueuedMinMessages;
        set => ConfluentConfig.QueuedMinMessages = value;
    }

    /// <summary>
    ///     Maximum number of kilobytes of queued pre-fetched messages in the local consumer queue. If using the
    ///     high-level consumer this setting applies to the single consumer queue, regardless of the number of
    ///     partitions. When using the legacy simple consumer or when separate partition queues are used this setting
    ///     applies per partition. This value may be overshot by fetch.message.max.bytes. This property has higher
    ///     priority than queued.min.messages.
    ///     <br /><br />default: 65536
    ///     <br />importance: medium
    /// </summary>
    public int? QueuedMaxMessagesKbytes
    {
        get => ConfluentConfig.QueuedMaxMessagesKbytes;
        set => ConfluentConfig.QueuedMaxMessagesKbytes = value;
    }

    /// <summary>
    ///     Maximum time the broker may wait to fill the Fetch response with fetch.min.bytes of messages.
    ///     <br /><br />default: 500
    ///     <br />importance: low
    /// </summary>
    public int? FetchWaitMaxMs
    {
        get => ConfluentConfig.FetchWaitMaxMs;
        set => ConfluentConfig.FetchWaitMaxMs = value;
    }

    /// <summary>
    ///     Initial maximum number of bytes per topic+partition to request when fetching messages from the broker. If
    ///     the client encounters a message larger than this value it will gradually try to increase it until the
    ///     entire message can be fetched.
    ///     <br /><br />default: 1048576
    ///     <br />importance: medium
    /// </summary>
    public int? MaxPartitionFetchBytes
    {
        get => ConfluentConfig.MaxPartitionFetchBytes;
        set => ConfluentConfig.MaxPartitionFetchBytes = value;
    }

    /// <summary>
    ///     Maximum amount of data the broker shall return for a Fetch request. Messages are fetched in batches by
    ///     the consumer and if the first message batch in the first non-empty partition of the Fetch request is
    ///     larger than this value, then the message batch will still be returned to ensure the consumer can make
    ///     progress. The maximum message batch size accepted by the broker is defined via `message.max.bytes`
    ///     (broker config) or `max.message.bytes` (broker topic config). `fetch.max.bytes` is automatically adjusted
    ///     upwards to be at least `message.max.bytes` (consumer config).
    ///     <br /><br />default: 52428800
    ///     <br />importance: medium
    /// </summary>
    public int? FetchMaxBytes
    {
        get => ConfluentConfig.FetchMaxBytes;
        set => ConfluentConfig.FetchMaxBytes = value;
    }

    /// <summary>
    ///     Minimum number of bytes the broker responds with. If fetch.wait.max.ms expires the accumulated data will
    ///     be sent to the client regardless of this setting.
    ///     <br /><br />default: 1
    ///     <br />importance: low
    /// </summary>
    public int? FetchMinBytes
    {
        get => ConfluentConfig.FetchMinBytes;
        set => ConfluentConfig.FetchMinBytes = value;
    }

    /// <summary>
    ///     How long to postpone the next fetch request for a topic+partition in case of a fetch error.
    ///     <br /><br />default: 500
    ///     <br />importance: medium
    /// </summary>
    public int? FetchErrorBackoffMs
    {
        get => ConfluentConfig.FetchErrorBackoffMs;
        set => ConfluentConfig.FetchErrorBackoffMs = value;
    }

    /// <summary>
    ///     Controls how to read messages written transactionally: `read_committed` - only return transactional
    ///     messages which have been committed. `read_uncommitted` - return all messages, even transactional messages
    ///     which have been aborted.
    ///     <br /><br />default: read_committed
    ///     <br />importance: high
    /// </summary>
    public IsolationLevel? IsolationLevel
    {
        get => ConfluentConfig.IsolationLevel;
        set => ConfluentConfig.IsolationLevel = value;
    }

    /// <summary>
    ///     Emit RD_KAFKA_RESP_ERR__PARTITION_EOF event whenever the consumer reaches the end of a partition.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? EnablePartitionEof
    {
        get => ConfluentConfig.EnablePartitionEof;
        set => ConfluentConfig.EnablePartitionEof = value;
    }

    /// <summary>
    ///     Verify CRC32 of consumed messages, ensuring no on-the-wire or on-disk corruption to the messages
    ///     occurred. This check comes at slightly increased CPU usage.
    ///     <br /><br />default: false
    ///     <br />importance: medium
    /// </summary>
    public bool? CheckCrcs
    {
        get => ConfluentConfig.CheckCrcs;
        set => ConfluentConfig.CheckCrcs = value;
    }

    /// <summary>
    ///     Allow automatic topic creation on the broker when subscribing to or assigning non-existent topics. The
    ///     broker must also be configured with `auto.create.topics.enable=true` for this configuraiton to take
    ///     effect. Note: The default value (false) is different from the Java consumer (true). Requires broker
    ///     version &gt;= 0.11.0.0, for older broker versions only the broker configuration applies.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? AllowAutoCreateTopics
    {
        get => ConfluentConfig.AllowAutoCreateTopics;
        set => ConfluentConfig.AllowAutoCreateTopics = value;
    }

    /// <summary>
    ///     Gets the <see cref="Confluent.Kafka.ClientConfig" /> instance being wrapped.
    /// </summary>
    protected new ConsumerConfig ConfluentConfig => (ConsumerConfig)base.ConfluentConfig;

    internal new ConsumerConfig GetConfluentConfig() => ConfluentConfig;
}

/// <summary>
///     Wraps the <see cref="Confluent.Kafka.ProducerConfig" />.
/// </summary>
[SuppressMessage("", "SA1649", Justification = "Autogenerated all at once")]
[SuppressMessage("", "SA1402", Justification = "Autogenerated all at once")]
[SuppressMessage("", "CA1200", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1623", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "SA1629", Justification = "Summary copied from wrapped class")]
[SuppressMessage("", "CA1044", Justification = "Accessors generated according to wrapped class")]
public abstract class ConfluentProducerConfigProxy : ConfluentClientConfigProxy
{
    /// <summary>
    ///     Initializes a new instance of the <see cref="ConfluentProducerConfigProxy" /> class.
    /// </summary>
    /// <param name="clientConfig">
    ///     The <see cref="Confluent.Kafka.ClientConfig" /> to be used to initialize the
    ///     <see cref="Confluent.Kafka.ProducerConfig" />.
    /// </param>
    protected ConfluentProducerConfigProxy(ClientConfig? clientConfig = null)
        : base(clientConfig != null ? new ProducerConfig(clientConfig.Clone()) : new ProducerConfig())
    {
    }

    /// <summary>
    ///     Specifies whether or not the producer should start a background poll
    ///     thread to receive delivery reports and event notifications. Generally,
    ///     this should be set to true. If set to false, you will need to call
    ///     the Poll function manually.
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? EnableBackgroundPoll
    {
        get => ConfluentConfig.EnableBackgroundPoll;
        set => ConfluentConfig.EnableBackgroundPoll = value;
    }

    /// <summary>
    ///     Specifies whether to enable notification of delivery reports. Typically
    ///     you should set this parameter to true. Set it to false for "fire and
    ///     forget" semantics and a small boost in performance.
    ///     <br /><br />default: true
    ///     <br />importance: low
    /// </summary>
    public bool? EnableDeliveryReports
    {
        get => ConfluentConfig.EnableDeliveryReports;
        set => ConfluentConfig.EnableDeliveryReports = value;
    }

    /// <summary>
    ///     A comma separated list of fields that may be optionally set in delivery
    ///     reports. Disabling delivery report fields that you do not require will
    ///     improve maximum throughput and reduce memory usage. Allowed values:
    ///     key, value, timestamp, headers, status, all, none.
    ///     <br /><br />default: all
    ///     <br />importance: low
    /// </summary>
    /// <remarks>
    ///     Silverback overrides this value by default setting it to &quot;key,status&quot; as an optimization,
    ///     since the other fields aren't used.
    /// </remarks>
    public string DeliveryReportFields
    {
        get => ConfluentConfig.DeliveryReportFields;
        set
        {
            if (value != null)
                ConfluentConfig.DeliveryReportFields = value;
        }
    }

    /// <summary>
    ///     The ack timeout of the producer request in milliseconds. This value is only enforced by the broker and
    ///     relies on `request.required.acks` being != 0.
    ///     <br /><br />default: 30000
    ///     <br />importance: medium
    /// </summary>
    public int? RequestTimeoutMs
    {
        get => ConfluentConfig.RequestTimeoutMs;
        set => ConfluentConfig.RequestTimeoutMs = value;
    }

    /// <summary>
    ///     Local message timeout. This value is only enforced locally and limits the time a produced message waits
    ///     for successful delivery. A time of 0 is infinite. This is the maximum time librdkafka may use to deliver
    ///     a message (including retries). Delivery error occurs when either the retry count or the message timeout
    ///     are exceeded. The message timeout is automatically adjusted to `transaction.timeout.ms` if
    ///     `transactional.id` is configured.
    ///     <br /><br />default: 300000
    ///     <br />importance: high
    /// </summary>
    public int? MessageTimeoutMs
    {
        get => ConfluentConfig.MessageTimeoutMs;
        set => ConfluentConfig.MessageTimeoutMs = value;
    }

    /// <summary>
    ///     Partitioner: `random` - random distribution, `consistent` - CRC32 hash of key (Empty and NULL keys are
    ///     mapped to single partition), `consistent_random` - CRC32 hash of key (Empty and NULL keys are randomly
    ///     partitioned), `murmur2` - Java Producer compatible Murmur2 hash of key (NULL keys are mapped to single
    ///     partition), `murmur2_random` - Java Producer compatible Murmur2 hash of key (NULL keys are randomly
    ///     partitioned. This is functionally equivalent to the default partitioner in the Java Producer.), `fnv1a` -
    ///     FNV-1a hash of key (NULL keys are mapped to single partition), `fnv1a_random` - FNV-1a hash of key (NULL
    ///     keys are randomly partitioned).
    ///     <br /><br />default: consistent_random
    ///     <br />importance: high
    /// </summary>
    public Partitioner? Partitioner
    {
        get => ConfluentConfig.Partitioner;
        set => ConfluentConfig.Partitioner = value;
    }

    /// <summary>
    ///     Compression level parameter for algorithm selected by configuration property `compression.codec`. Higher
    ///     values will result in better compression at the cost of more CPU usage. Usable range is
    ///     algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default
    ///     compression level.
    ///     <br /><br />default: -1
    ///     <br />importance: medium
    /// </summary>
    public int? CompressionLevel
    {
        get => ConfluentConfig.CompressionLevel;
        set => ConfluentConfig.CompressionLevel = value;
    }

    /// <summary>
    ///     Enables the transactional producer. The transactional.id is used to identify the same transactional
    ///     producer instance across process restarts. It allows the producer to guarantee that transactions
    ///     corresponding to earlier instances of the same producer have been finalized prior to starting any new
    ///     transactions, and that any zombie instances are fenced off. If no transactional.id is provided, then the
    ///     producer is limited to idempotent delivery (if enable.idempotence is set). Requires broker version &gt;=
    ///     0.11.0.
    ///     <br /><br />default: ''
    ///     <br />importance: high
    /// </summary>
    public string TransactionalId
    {
        get => ConfluentConfig.TransactionalId;
        set => ConfluentConfig.TransactionalId = value;
    }

    /// <summary>
    ///     The maximum amount of time in milliseconds that the transaction coordinator will wait for a transaction
    ///     status update from the producer before proactively aborting the ongoing transaction. If this value is
    ///     larger than the `transaction.max.timeout.ms` setting in the broker, the init_transactions() call will
    ///     fail with ERR_INVALID_TRANSACTION_TIMEOUT. The transaction timeout automatically adjusts
    ///     `message.timeout.ms` and `socket.timeout.ms`, unless explicitly configured in which case they must not
    ///     exceed the transaction timeout (`socket.timeout.ms` must be at least 100ms lower than
    ///     `transaction.timeout.ms`). This is also the default timeout value if no timeout (-1) is supplied to the
    ///     transactional API methods.
    ///     <br /><br />default: 60000
    ///     <br />importance: medium
    /// </summary>
    public int? TransactionTimeoutMs
    {
        get => ConfluentConfig.TransactionTimeoutMs;
        set => ConfluentConfig.TransactionTimeoutMs = value;
    }

    /// <summary>
    ///     When set to `true`, the producer will ensure that messages are successfully produced exactly once and in
    ///     the original produce order. The following configuration properties are adjusted automatically (if not
    ///     modified by the user) when idempotence is enabled: `max.in.flight.requests.per.connection=5` (must be
    ///     less than or equal to 5), `retries=INT32_MAX` (must be greater than 0), `acks=all`,
    ///     `queuing.strategy=fifo`. Producer instantation will fail if user-supplied configuration is incompatible.
    ///     <br /><br />default: false
    ///     <br />importance: high
    /// </summary>
    public bool? EnableIdempotence
    {
        get => ConfluentConfig.EnableIdempotence;
        set => ConfluentConfig.EnableIdempotence = value;
    }

    /// <summary>
    ///     **EXPERIMENTAL**: subject to change or removal. When set to `true`, any error that could result in a gap
    ///     in the produced message series when a batch of messages fails, will raise a fatal error
    ///     (ERR__GAPLESS_GUARANTEE) and stop the producer. Messages failing due to `message.timeout.ms` are not
    ///     covered by this guarantee. Requires `enable.idempotence=true`.
    ///     <br /><br />default: false
    ///     <br />importance: low
    /// </summary>
    public bool? EnableGaplessGuarantee
    {
        get => ConfluentConfig.EnableGaplessGuarantee;
        set => ConfluentConfig.EnableGaplessGuarantee = value;
    }

    /// <summary>
    ///     Maximum number of messages allowed on the producer queue. This queue is shared by all topics and
    ///     partitions.
    ///     <br /><br />default: 100000
    ///     <br />importance: high
    /// </summary>
    public int? QueueBufferingMaxMessages
    {
        get => ConfluentConfig.QueueBufferingMaxMessages;
        set => ConfluentConfig.QueueBufferingMaxMessages = value;
    }

    /// <summary>
    ///     Maximum total message size sum allowed on the producer queue. This queue is shared by all topics and
    ///     partitions. This property has higher priority than queue.buffering.max.messages.
    ///     <br /><br />default: 1048576
    ///     <br />importance: high
    /// </summary>
    public int? QueueBufferingMaxKbytes
    {
        get => ConfluentConfig.QueueBufferingMaxKbytes;
        set => ConfluentConfig.QueueBufferingMaxKbytes = value;
    }

    /// <summary>
    ///     Delay in milliseconds to wait for messages in the producer queue to accumulate before constructing
    ///     message batches (MessageSets) to transmit to brokers. A higher value allows larger and more effective
    ///     (less overhead, improved compression) batches of messages to accumulate at the expense of increased
    ///     message delivery latency.
    ///     <br /><br />default: 5
    ///     <br />importance: high
    /// </summary>
    public double? LingerMs
    {
        get => ConfluentConfig.LingerMs;
        set => ConfluentConfig.LingerMs = value;
    }

    /// <summary>
    ///     How many times to retry sending a failing Message. **Note:** retrying may cause reordering unless
    ///     `enable.idempotence` is set to true.
    ///     <br /><br />default: 2147483647
    ///     <br />importance: high
    /// </summary>
    public int? MessageSendMaxRetries
    {
        get => ConfluentConfig.MessageSendMaxRetries;
        set => ConfluentConfig.MessageSendMaxRetries = value;
    }

    /// <summary>
    ///     The backoff time in milliseconds before retrying a protocol request.
    ///     <br /><br />default: 100
    ///     <br />importance: medium
    /// </summary>
    public int? RetryBackoffMs
    {
        get => ConfluentConfig.RetryBackoffMs;
        set => ConfluentConfig.RetryBackoffMs = value;
    }

    /// <summary>
    ///     The threshold of outstanding not yet transmitted broker requests needed to backpressure the producer's
    ///     message accumulator. If the number of not yet transmitted requests equals or exceeds this number, produce
    ///     request creation that would have otherwise been triggered (for example, in accordance with linger.ms)
    ///     will be delayed. A lower number yields larger and more effective batches. A higher value can improve
    ///     latency when using compression on slow machines.
    ///     <br /><br />default: 1
    ///     <br />importance: low
    /// </summary>
    public int? QueueBufferingBackpressureThreshold
    {
        get => ConfluentConfig.QueueBufferingBackpressureThreshold;
        set => ConfluentConfig.QueueBufferingBackpressureThreshold = value;
    }

    /// <summary>
    ///     compression codec to use for compressing message sets. This is the default value for all topics, may be
    ///     overridden by the topic configuration property `compression.codec`.
    ///     <br /><br />default: none
    ///     <br />importance: medium
    /// </summary>
    public CompressionType? CompressionType
    {
        get => ConfluentConfig.CompressionType;
        set => ConfluentConfig.CompressionType = value;
    }

    /// <summary>
    ///     Maximum number of messages batched in one MessageSet. The total MessageSet size is also limited by
    ///     batch.size and message.max.bytes.
    ///     <br /><br />default: 10000
    ///     <br />importance: medium
    /// </summary>
    public int? BatchNumMessages
    {
        get => ConfluentConfig.BatchNumMessages;
        set => ConfluentConfig.BatchNumMessages = value;
    }

    /// <summary>
    ///     Maximum size (in bytes) of all messages batched in one MessageSet, including protocol framing overhead.
    ///     This limit is applied after the first message has been added to the batch, regardless of the first
    ///     message's size, this is to ensure that messages that exceed batch.size are produced. The total MessageSet
    ///     size is also limited by batch.num.messages and message.max.bytes.
    ///     <br /><br />default: 1000000
    ///     <br />importance: medium
    /// </summary>
    public int? BatchSize
    {
        get => ConfluentConfig.BatchSize;
        set => ConfluentConfig.BatchSize = value;
    }

    /// <summary>
    ///     Delay in milliseconds to wait to assign new sticky partitions for each topic. By default, set to double
    ///     the time of linger.ms. To disable sticky behavior, set to 0. This behavior affects messages with the key
    ///     NULL in all cases, and messages with key lengths of zero when the consistent_random partitioner is in
    ///     use. These messages would otherwise be assigned randomly. A higher value allows for more effective
    ///     batching of these messages.
    ///     <br /><br />default: 10
    ///     <br />importance: low
    /// </summary>
    public int? StickyPartitioningLingerMs
    {
        get => ConfluentConfig.StickyPartitioningLingerMs;
        set => ConfluentConfig.StickyPartitioningLingerMs = value;
    }

    /// <summary>
    ///     Gets the <see cref="Confluent.Kafka.ClientConfig" /> instance being wrapped.
    /// </summary>
    protected new ProducerConfig ConfluentConfig => (ProducerConfig)base.ConfluentConfig;

    internal new ProducerConfig GetConfluentConfig() => ConfluentConfig;
}
